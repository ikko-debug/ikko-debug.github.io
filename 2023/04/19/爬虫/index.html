<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Ikko">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    
        
            <link rel="preconnect" href="https://npm.elemecdn.com" crossorigin>
        
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/2023/04/19/爬虫/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="初始代码copyright from https:&#x2F;&#x2F;blog.csdn.net&#x2F;INTEGRATOR_37&#x2F;article&#x2F;details&#x2F;113386649  1234567891011121314151617181920212223242526272829303132333435363738import requestsimport refrom bs4 import BeautifulSo">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫">
<meta property="og:url" content="http://example.com/2023/04/19/%E7%88%AC%E8%99%AB/index.html">
<meta property="og:site_name" content="IKKO">
<meta property="og:description" content="初始代码copyright from https:&#x2F;&#x2F;blog.csdn.net&#x2F;INTEGRATOR_37&#x2F;article&#x2F;details&#x2F;113386649  1234567891011121314151617181920212223242526272829303132333435363738import requestsimport refrom bs4 import BeautifulSo">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-04-19T04:58:02.000Z">
<meta property="article:modified_time" content="2023-05-22T06:16:01.623Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/idea.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/idea.svg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/idea.svg">
    <!--- Page Info-->
    
    <title>
        
            爬虫 -
        
        Ikko&#39;s Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/assets/fonts.css">
    <!--- Font Part-->
    
    
    
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"example.com","root":"/","language":"en"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":true,"list":[""]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":true,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"busuanzi_counter":{"enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"pjax":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"static","image":{"light":"/images/light.jpg","dark":"/images/dark.jpg"},"title":"若有恒，何必三更眠五更起 最无益，只怕一日曝十日寒","subtitle":{"text":[],"hitokoto":{"enable":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"1.5rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":false,"links":{"github":null,"instagram":null,"zhihu":null,"twitter":null,"email":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.1.4","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"FriendLinks":{"icon":"fa-solid fa-link","submenus":{"yebao":"https://danmoliuhen.github.io/","wu-22":"https://wu-22.github.io/","jjb":"https://byjiangjb.github.io/"}}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"links":null},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}}};
    Global.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    Global.data_config = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/fontawesome/fontawesome.min.css">
    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/fontawesome/brands.min.css">
    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/fontawesome/solid.min.css">
    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/fontawesome/regular.min.css">
    
    
    
    
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="main-content-container">

        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
                <a class="logo-image" href="/">
                    <img src="/images/idea.svg">
                </a>
            
            <a class="logo-title" href="/">
                
                Ikko&#39;s Blog
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        HOME
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/archives"  >
                                    
                                        
                                            <i class="fa-regular fa-archive"></i>
                                        
                                        ARCHIVES
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-solid fa-link"></i>
                                        
                                        FRIENDLINKS&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://danmoliuhen.github.io/">YEBAO
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://wu-22.github.io/">WU-22
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://byjiangjb.github.io/">JJB
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer">
        <ul class="drawer-navbar-list">
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                HOME
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/archives"  >
                             
                                
                                    <i class="fa-regular fa-archive"></i>
                                
                                ARCHIVES
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-solid fa-link"></i>
                                
                                FRIENDLINKS&nbsp;<i class="fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://danmoliuhen.github.io/">YEBAO</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://wu-22.github.io/">WU-22</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://byjiangjb.github.io/">JJB</a>
                            </li>
                        
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            
            
                <div class="article-title">
                    <h1 class="article-title-regular">爬虫</h1>
                </div>
            
                
            

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/56D9D2F8A1A04F1D59D48CAD0011E450.jpg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">Ikko</span>
                            
                                <span class="author-label">Lv2</span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2023-04-19 12:58:02</span>
        <span class="mobile">2023-04-19 12:58</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2023-05-22 14:16:01</span>
            <span class="mobile">2023-05-22 14:16</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fa-regular fa-typewriter"></i>&nbsp;<span>3.6k Words</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fa-regular fa-clock"></i>&nbsp;<span>17 Mins</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h2 id="初始代码"><a href="#初始代码" class="headerlink" title="初始代码"></a>初始代码</h2><p>copyright from <a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/INTEGRATOR_37/article/details/113386649" >https://blog.csdn.net/INTEGRATOR_37/article/details/113386649 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.gov.cn/zhengce/zuixin.htm&#x27;</span></span><br><span class="line">UA = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36&#x27;</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User_Agent&#x27;</span>: UA&#125;</span><br><span class="line"></span><br><span class="line">r = requests.get(url, headers=headers)</span><br><span class="line">r.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(r.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">attrs = &#123;<span class="string">&#x27;class&#x27;</span>: <span class="string">&#x27;date&#x27;</span>&#125;</span><br><span class="line">links = soup.find_all(href=re.<span class="built_in">compile</span>(<span class="string">&#x27;content&#x27;</span>))</span><br><span class="line">dates = soup.find_all(name=<span class="string">&#x27;span&#x27;</span>, attrs=attrs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get titles and links</span></span><br><span class="line">titles = []</span><br><span class="line">urls = []</span><br><span class="line"><span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">    titles.append(<span class="built_in">str</span>(link.string))</span><br><span class="line">    url = link.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">    urls.append(<span class="built_in">str</span>(url))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get days</span></span><br><span class="line">days = []</span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;(\d+)\-(\d+)\-(\d+)&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> date <span class="keyword">in</span> dates:</span><br><span class="line">    s = date.string</span><br><span class="line">    day = re.search(pattern, s)</span><br><span class="line">    days.append(<span class="built_in">str</span>(day.group()))</span><br><span class="line"></span><br><span class="line">data = &#123;<span class="string">&#x27;date&#x27;</span>: days,</span><br><span class="line">        <span class="string">&#x27;title&#x27;</span>: titles,</span><br><span class="line">        <span class="string">&#x27;url&#x27;</span>: urls&#125;</span><br><span class="line">frame = DataFrame(data)</span><br><span class="line">frame.to_csv(<span class="string">&#x27;test.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure></div>
<h2 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解"></a>个人理解</h2><p>上面这段代码主要的目的是抓取中国政府网（<a class="link"   target="_blank" rel="noopener" href="http://www.gov.cn)中最新的法规政策,并将其日期、标题和链接保存到csv文件中./" >www.gov.cn）中最新的法规政策，并将其日期、标题和链接保存到CSV文件中。 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<ul>
<li><code>import requests</code>: 引入requests库，用于网络请求操作。</li>
<li><code>import re</code>: 引入re库，用于正则表达式匹配。</li>
<li><code>from bs4 import BeautifulSoup</code>: 引入BeautifulSoup库，用于解析HTML页面。</li>
<li><code>from pandas import DataFrame</code>: 引入DataFrame库，用于构建数据表格。</li>
</ul>
<p>接下来是变量定义：</p>
<ul>
<li><code>url = &#39;http://www.gov.cn/zhengce/zuixin.htm&#39;</code>：指定要爬取数据的网站URL。</li>
<li><code>UA = &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36&#39;</code>：设置User-Agent，模拟浏览器进行访问。</li>
<li><code>headers = &#123;&#39;User_Agent&#39;: UA&#125;</code>: 设置请求头部信息，包括User-Agent。</li>
<li><code>r = requests.get(url, headers=headers)</code>：发送HTTP GET请求获取网站页面源码。</li>
<li><code>r.encoding = &#39;utf-8&#39;</code>：设置网页内容的编码格式为UTF-8。</li>
<li><code>soup = BeautifulSoup(r.text, &#39;lxml&#39;)</code>：使用BeautifulSoup库解析HTML页面。</li>
<li><code>attrs = &#123;&#39;class&#39;: &#39;date&#39;&#125;</code>：设定属性字典{‘class’: ‘date’}，以查找所有class为’date’的span标签。</li>
<li><code>links = soup.find_all(href=re.compile(&#39;content&#39;))</code>：使用正则表达式查找href属性中包含’content’字符串的所有a标签。</li>
<li><code>dates = soup.find_all(name=&#39;span&#39;, attrs=attrs)</code>：查找所有class为’date’的span标签，并通过attrs参数和name参数指定进一步的筛选条件。</li>
</ul>
<p>接下来，代码获取标题、链接和日期数据：</p>
<ul>
<li>使用for循环遍历所有查找到的links元素，将其中的标题保存到变量titles列表中，将链接地址存储到urls列表中。</li>
<li>使用正则表达式从日期的字符串中提取出日期信息，并将其追加到days列表中。</li>
</ul>
<p>最后，该程序创建了一个数据字典data，并使用DataFrame类构建了一个数据表格frame。然后，将这个表格以CSV格式写入test.csv文件中，其中index&#x3D;False指定不要写入索引值（即第一列）到文件中。<br>爬出数据是这样的：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">date,title,url</span><br><span class="line">2023-04-18,中共中央印发《中央党内法规制定工作规划纲要（2023－2027年）》,/zhengce/2023-04/18/content_5752088.htm</span><br><span class="line">2023-04-18,国务院办公厅关于调整第19届亚运会和第4届亚残运会工作领导小组组成人员等有关事项的通知,http://www.gov.cn/zhengce/content/2023-04/18/content_5752017.htm</span><br><span class="line">2023-04-14,国务院办公厅关于上市公司独立董事制度改革的意见,http://www.gov.cn/zhengce/content/2023-04/14/content_5751463.htm</span><br><span class="line">2023-04-12,征兵工作条例,http://www.gov.cn/zhengce/content/2023-04/12/content_5750986.htm</span><br><span class="line">2023-04-10,中共中央发出关于学习《习近平著作选读》第一卷、第二卷的通知,/zhengce/2023-04/10/content_5750697.htm</span><br><span class="line">2023-04-07,国务院办公厅关于成立第五次全国经济普查领导小组的通知,http://www.gov.cn/zhengce/content/2023-04/07/content_5750375.htm</span><br></pre></td></tr></table></figure></div>
<h2 id="代码修改"><a href="#代码修改" class="headerlink" title="代码修改"></a>代码修改</h2><h3 id="url修改"><a href="#url修改" class="headerlink" title="url修改"></a>url修改</h3><p>首先可以看到爬取的url是不规范的，有的是绝对url，有的是相对url，这样就需要对url进行处理，使其都是绝对url。对于不规范的URL，我们可以使用Python中的urllib.parse.urljoin()函数将相对URL转化为绝对URL。该函数可以将基础URL和相对URL合并成一个完整的URL，并返回结果。<br>通过调用urljoin()函数，将上述示例中的URL处理为绝对URL：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"></span><br><span class="line">base_url = <span class="string">&#x27;http://www.gov.cn/&#x27;</span></span><br><span class="line">data = [</span><br><span class="line">    (<span class="string">&#x27;2023-04-18&#x27;</span>, <span class="string">&#x27;中共中央印发《中央党内法规制定工作规划纲要（2023－2027年）》&#x27;</span>, <span class="string">&#x27;/zhengce/2023-04/18/content_5752088.htm&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;2023-04-18&#x27;</span>, <span class="string">&#x27;国务院办公厅关于调整第19届亚运会和第4届亚残运会工作领导小组组成人员等有关事项的通知&#x27;</span>, <span class="string">&#x27;http://www.gov.cn/zhengce/content/2023-04/18/content_5752017.htm&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;2023-04-14&#x27;</span>, <span class="string">&#x27;国务院办公厅关于上市公司独立董事制度改革的意见&#x27;</span>, <span class="string">&#x27;http://www.gov.cn/zhengce/content/2023-04/14/content_5751463.htm&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;2023-04-12&#x27;</span>, <span class="string">&#x27;征兵工作条例&#x27;</span>, <span class="string">&#x27;http://www.gov.cn/zhengce/content/2023-04/12/content_5750986.htm&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;2023-04-10&#x27;</span>, <span class="string">&#x27;中共中央发出关于学习《习近平著作选读》第一卷、第二卷的通知&#x27;</span>, <span class="string">&#x27;/zhengce/2023-04/10/content_5750697.htm&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;2023-04-07&#x27;</span>, <span class="string">&#x27;国务院办公厅关于成立第五次全国经济普查领导小组的通知&#x27;</span>, <span class="string">&#x27;http://www.gov.cn/zhengce/content/2023-04/07/content_5750375.htm&#x27;</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> date, title, url <span class="keyword">in</span> data:</span><br><span class="line">    full_url = urljoin(base_url, url)</span><br><span class="line">    <span class="built_in">print</span>(date, title, full_url)</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>
<p>对于相对URL，urljoin()函数会自动补全为绝对URL，而对于绝对URL，则不会进行任何更改。</p>
<h3 id="修改成功后发现，绝对url和相对url对应的页面结构是不同的"><a href="#修改成功后发现，绝对url和相对url对应的页面结构是不同的" class="headerlink" title="修改成功后发现，绝对url和相对url对应的页面结构是不同的"></a>修改成功后发现，绝对url和相对url对应的页面结构是不同的</h3><h4 id="添加判断url为相对url还是绝对url"><a href="#添加判断url为相对url还是绝对url" class="headerlink" title="添加判断url为相对url还是绝对url"></a>添加判断url为相对url还是绝对url</h4><p>可以使用Python的urllib.parse.urlparse()函数将URL解析成6个部分（scheme，netloc，path，params，query和fragment），然后检查其中是否包含netloc字段，如果netloc为空，就是相对URL，否则就是绝对URL。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_relative_url</span>(<span class="params">url</span>):</span><br><span class="line">    parsed_url = urlparse(url)</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">not</span> <span class="built_in">bool</span>(parsed_url.netloc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="built_in">print</span>(is_relative_url(<span class="string">&#x27;/zhengce/2023-04/18/content_5752088.htm&#x27;</span>))     <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(is_relative_url(<span class="string">&#x27;http://www.gov.cn/zhengce/content/2023-04/14/content_5751463.htm&#x27;</span>))     <span class="comment"># False</span></span><br></pre></td></tr></table></figure></div>
<h4 id="先处理相对url"><a href="#先处理相对url" class="headerlink" title="先处理相对url"></a>先处理相对url</h4><p>相对url有四个字段：标题，时间，来源，正文</p>
<h5 id="提取标题"><a href="#提取标题" class="headerlink" title="提取标题"></a>提取标题</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line">url = <span class="string">&#x27;http://www.gov.cn/zhengce/2023-04/18/content_5752088.htm&#x27;</span></span><br><span class="line">UA = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36&#x27;</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User_Agent&#x27;</span>: UA&#125;</span><br><span class="line"></span><br><span class="line">r = requests.get(url, headers=headers)</span><br><span class="line">r.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(r.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"></span><br><span class="line">title_element = soup.find(<span class="string">&#x27;h1&#x27;</span>)</span><br><span class="line">title = title_element.get_text().strip()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(title)  <span class="comment"># 输出 &quot;中共中央印发《中央党内法规制定工作规划纲要（2023－2027年）》&quot;</span></span><br></pre></td></tr></table></figure></div>
<h5 id="提取发布时间和来源"><a href="#提取发布时间和来源" class="headerlink" title="提取发布时间和来源"></a>提取发布时间和来源</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用class属性查找包含发布时间和来源的div元素</span></span><br><span class="line">pages_date = soup.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;pages-date&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取发布时间和来源的文本内容</span></span><br><span class="line">publish_time = pages_date.contents[<span class="number">0</span>].strip()</span><br><span class="line"><span class="comment"># 将日期字符串转换为datetime类型的数据</span></span><br><span class="line">publish_time = datetime.strptime(publish_time, <span class="string">&#x27;%Y-%m-%d %H:%M&#x27;</span>)</span><br><span class="line">source = pages_date.find(<span class="string">&#x27;span&#x27;</span>, class_=<span class="string">&#x27;font&#x27;</span>).text.strip().replace(<span class="string">&#x27;来源：&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;发布时间：&quot;</span>, publish_time)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;来源：&quot;</span>, source)</span><br></pre></td></tr></table></figure></div>
<h5 id="提取正文"><a href="#提取正文" class="headerlink" title="提取正文"></a>提取正文</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用id属性查找包含正文内容的div元素</span></span><br><span class="line">pages_content = soup.find(<span class="string">&#x27;div&#x27;</span>, <span class="built_in">id</span>=<span class="string">&#x27;UCAP-CONTENT&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找正文内容中的所有p元素</span></span><br><span class="line">p_tags = pages_content.find_all(<span class="string">&#x27;p&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历所有的p标签，并将它们的文本内容连接起来</span></span><br><span class="line">text = <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> p_tag <span class="keyword">in</span> p_tags:</span><br><span class="line">    <span class="comment"># 判断该p标签内是否包含&lt;span&gt;标签</span></span><br><span class="line">    <span class="keyword">if</span> p_tag.find(<span class="string">&#x27;span&#x27;</span>):</span><br><span class="line">        <span class="comment"># 使用extract()方法将该&lt;span&gt;标签从文档中去除</span></span><br><span class="line">        p_tag.find(<span class="string">&#x27;span&#x27;</span>).extract()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将该p标签的文本内容连接到text变量中</span></span><br><span class="line">    text += p_tag.text.strip() + <span class="string">&#x27;\n&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(text)</span><br></pre></td></tr></table></figure></div>
<h4 id="处理绝对url"><a href="#处理绝对url" class="headerlink" title="处理绝对url"></a>处理绝对url</h4><p>绝对url有主题分类，发文机关，标题，发文字号，成文日期，发布日期,正文</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line">url = <span class="string">&#x27;http://www.gov.cn/zhengce/content/2023-04/12/content_5750986.htm&#x27;</span></span><br><span class="line">UA = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36&#x27;</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User_Agent&#x27;</span>: UA&#125;</span><br><span class="line"></span><br><span class="line">r = requests.get(url, headers=headers)</span><br><span class="line">r.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(r.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 找到包含信息的表格标签</span></span><br><span class="line">info_table = soup.find(<span class="string">&#x27;table&#x27;</span>, &#123;<span class="string">&#x27;class&#x27;</span>: <span class="string">&#x27;bd1&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找到每个信息所在的表格行，并提取数据</span></span><br><span class="line">rows = info_table.find_all(<span class="string">&#x27;tr&#x27;</span>)</span><br><span class="line"></span><br><span class="line">topic_category = rows[<span class="number">0</span>].find_all(<span class="string">&#x27;td&#x27;</span>)[<span class="number">3</span>].text.strip()</span><br><span class="line">publishing_organization = rows[<span class="number">1</span>].find_all(<span class="string">&#x27;td&#x27;</span>)[<span class="number">1</span>].text.strip()</span><br><span class="line">title = rows[<span class="number">2</span>].find_all(<span class="string">&#x27;td&#x27;</span>)[<span class="number">1</span>].text.strip()</span><br><span class="line">document_number = rows[<span class="number">3</span>].find_all(<span class="string">&#x27;td&#x27;</span>)[<span class="number">1</span>].text.strip()</span><br><span class="line">written_date = rows[<span class="number">1</span>].find_all(<span class="string">&#x27;td&#x27;</span>)[<span class="number">3</span>].text.strip()</span><br><span class="line">release_date = rows[<span class="number">3</span>].find_all(<span class="string">&#x27;td&#x27;</span>)[<span class="number">3</span>].text.strip()</span><br><span class="line">written_date = datetime.strptime(written_date, <span class="string">&#x27;%Y年%m月%d日&#x27;</span>)</span><br><span class="line">release_date = datetime.strptime(release_date, <span class="string">&#x27;%Y年%m月%d日&#x27;</span>)</span><br><span class="line"><span class="comment"># 打印提取到的数据</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;主题分类：&#x27;</span>, topic_category)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;发文机关：&#x27;</span>, publishing_organization)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;标题：&#x27;</span>, title)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;发文字号：&#x27;</span>, document_number)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;成文日期：&#x27;</span>, written_date)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;发布日期：&#x27;</span>, release_date)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用id属性查找包含正文内容的div元素</span></span><br><span class="line">pages_content = soup.find(<span class="string">&#x27;td&#x27;</span>, <span class="built_in">id</span>=<span class="string">&#x27;UCAP-CONTENT&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找正文内容中的所有p元素</span></span><br><span class="line">p_tags = pages_content.find_all(<span class="string">&#x27;p&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历所有的p标签，并将它们的文本内容连接起来</span></span><br><span class="line">text = <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> p_tag <span class="keyword">in</span> p_tags:</span><br><span class="line">    <span class="comment"># 判断该p标签内是否包含&lt;span&gt;标签</span></span><br><span class="line">    <span class="keyword">if</span> p_tag.find(<span class="string">&#x27;span&#x27;</span>):</span><br><span class="line">        <span class="comment"># 使用extract()方法将该&lt;span&gt;标签从文档中去除</span></span><br><span class="line">        p_tag.find(<span class="string">&#x27;span&#x27;</span>).extract()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将该p标签的文本内容连接到text变量中</span></span><br><span class="line">    text += p_tag.text.strip() + <span class="string">&#x27;\n&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(text)</span><br></pre></td></tr></table></figure></div>
<h3 id="确保数据不重复"><a href="#确保数据不重复" class="headerlink" title="确保数据不重复"></a>确保数据不重复</h3><p>使用pickle模块将已经爬取过的url保存到本地，下次爬取时，先从本地读取已经爬取过的url，然后再进行爬取，这样就可以确保数据不重复了。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data.pickle&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    visited_urls = pickle.load(f)</span><br><span class="line">    <span class="built_in">print</span>(visited_urls)</span><br></pre></td></tr></table></figure></div>
<h2 id="最终代码"><a href="#最终代码" class="headerlink" title="最终代码"></a>最终代码</h2><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> pymysql <span class="keyword">import</span> Error</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">crawl_and_process</span>(<span class="params">urls_to_crawl</span>):</span><br><span class="line">    conn = pymysql.connect(host=<span class="string">&#x27;&#x27;</span>, user=<span class="string">&#x27;root&#x27;</span>, password=<span class="string">&#x27;&#x27;</span>, database=<span class="string">&#x27;search&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建游标对象</span></span><br><span class="line">    cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查询数据库中最大的主键值</span></span><br><span class="line">    cursor.execute(<span class="string">&#x27;SELECT MAX(policy_id) FROM search_policy&#x27;</span>)</span><br><span class="line">    result = cursor.fetchone()</span><br><span class="line">    max_id = result[<span class="number">0</span>] <span class="keyword">if</span> result[<span class="number">0</span>] <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将新数据的主键值设置为查询到的最大主键值加1</span></span><br><span class="line">    new_id = <span class="built_in">int</span>(max_id) + <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="number">23</span>,new_id)</span><br><span class="line">    new_id = <span class="built_in">str</span>(new_id)</span><br><span class="line">    <span class="comment"># 读取已经爬取的URL</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data.pickle&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            visited_urls = pickle.load(f)</span><br><span class="line">            <span class="built_in">print</span>(visited_urls)</span><br><span class="line">    <span class="keyword">except</span> FileNotFoundError:</span><br><span class="line">        visited_urls = <span class="built_in">set</span>() <span class="comment">#实际我生成的是列表</span></span><br><span class="line">    </span><br><span class="line">    data1 =[]</span><br><span class="line">    count1 = <span class="number">0</span></span><br><span class="line">    count2 = <span class="number">0</span></span><br><span class="line">    data2=[]</span><br><span class="line">    sql1 = <span class="string">&#x27;INSERT INTO search_policy (policy_id, policy_title, pub_time, pub_agency, policy_body,policy_grade) VALUES (%s, %s,  %s, %s, %s, %s)&#x27;</span></span><br><span class="line">    sql2 = <span class="string">&#x27;INSERT INTO search_policy (policy_id, policy_title, pub_agency,pub_time,UPDATE_DATE,pub_number, policy_body,policy_grade) VALUES (%s, %s,  %s, %s,  %s, %s, %s, %s)&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> urls_to_crawl:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 判断URL是否已经被爬取过</span></span><br><span class="line">        <span class="keyword">if</span> url <span class="keyword">in</span> visited_urls:</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 爬取URL并进行数据处理</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;/content/&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> url:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;正在爬取：&#x27;</span>, url)</span><br><span class="line">            data1.append(relativeurl(url,new_id))</span><br><span class="line">            new_id = <span class="built_in">int</span>(new_id) + <span class="number">1</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="number">52</span>,new_id)</span><br><span class="line">            new_id = <span class="built_in">str</span>(new_id)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;正在爬取：&#x27;</span>, url)</span><br><span class="line">            data2.append(absoluteurl(url,new_id))</span><br><span class="line">            new_id = <span class="built_in">int</span>(new_id) + <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            new_id = <span class="built_in">str</span>(new_id)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 将已经爬取的URL添加到visited_urls中</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="number">64</span>)    </span><br><span class="line">        visited_urls.append(url)</span><br><span class="line">        <span class="built_in">print</span>(<span class="number">66</span>,)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data1) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;enter&#x27;</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            cursor.executemany(sql1, data1)</span><br><span class="line">            conn.commit()</span><br><span class="line">        <span class="keyword">except</span> Error <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(e)</span><br><span class="line">            conn.rollback()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;ok&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data2) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;enter&#x27;</span>)</span><br><span class="line">        cursor.executemany(sql2, data2)</span><br><span class="line">        conn.commit()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;ok&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="number">74</span>)</span><br><span class="line">    <span class="comment"># 保存visited_urls到pickle文件中</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data.pickle&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        pickle.dump(visited_urls, f)</span><br><span class="line">    cursor.close()</span><br><span class="line">    conn.close() </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_relative_url</span>(<span class="params">url</span>):<span class="comment">#判断是否为相对路径</span></span><br><span class="line">    parsed_url = urlparse(url)</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">not</span> <span class="built_in">bool</span>(parsed_url.netloc)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relativeurl</span>(<span class="params">url,new_id</span>):</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    UA = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36&#x27;</span></span><br><span class="line">    headers = &#123;<span class="string">&#x27;User_Agent&#x27;</span>: UA&#125;</span><br><span class="line"></span><br><span class="line">    r = requests.get(url, headers=headers)</span><br><span class="line">    r.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line"></span><br><span class="line">    soup = BeautifulSoup(r.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    title_element = soup.find(<span class="string">&#x27;h1&#x27;</span>)</span><br><span class="line">    title = title_element.get_text().strip()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用class属性查找包含发布时间和来源的div元素</span></span><br><span class="line">    pages_date = soup.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;pages-date&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取发布时间和来源的文本内容</span></span><br><span class="line">    publish_time = pages_date.contents[<span class="number">0</span>].strip()</span><br><span class="line">    <span class="comment"># 将日期字符串转换为datetime类型的数据</span></span><br><span class="line">    publish_time = datetime.strptime(publish_time, <span class="string">&#x27;%Y-%m-%d %H:%M&#x27;</span>)</span><br><span class="line">    source = pages_date.find(<span class="string">&#x27;span&#x27;</span>, class_=<span class="string">&#x27;font&#x27;</span>).text.strip().replace(<span class="string">&#x27;来源：&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用id属性查找包含正文内容的div元素</span></span><br><span class="line">    pages_content = soup.find(<span class="string">&#x27;div&#x27;</span>, <span class="built_in">id</span>=<span class="string">&#x27;UCAP-CONTENT&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 查找正文内容中的所有p元素</span></span><br><span class="line">    p_tags = pages_content.find_all(<span class="string">&#x27;p&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 遍历所有的p标签，并将它们的文本内容连接起来</span></span><br><span class="line">    text = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> p_tag <span class="keyword">in</span> p_tags:</span><br><span class="line">        <span class="comment"># 判断该p标签内是否包含&lt;span&gt;标签</span></span><br><span class="line">        <span class="keyword">if</span> p_tag.find(<span class="string">&#x27;span&#x27;</span>):</span><br><span class="line">            <span class="comment"># 使用extract()方法将该&lt;span&gt;标签从文档中去除</span></span><br><span class="line">            p_tag.find(<span class="string">&#x27;span&#x27;</span>).extract()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将该p标签的文本内容连接到text变量中</span></span><br><span class="line">        text += p_tag.text.strip() + <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    data = (new_id,title, publish_time, source, text,<span class="string">&#x27;国家级&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="number">125</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">absoluteurl</span>(<span class="params">url,new_id</span>):</span><br><span class="line">    UA = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36&#x27;</span></span><br><span class="line">    headers = &#123;<span class="string">&#x27;User_Agent&#x27;</span>: UA&#125;</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;正在爬取：&#x27;</span>, url)</span><br><span class="line">    r = requests.get(url, headers=headers)</span><br><span class="line">    r.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    soup = BeautifulSoup(r.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    <span class="comment"># 找到包含信息的表格标签</span></span><br><span class="line">    info_table = soup.find(<span class="string">&#x27;table&#x27;</span>, &#123;<span class="string">&#x27;class&#x27;</span>: <span class="string">&#x27;bd1&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 找到每个信息所在的表格行，并提取数据</span></span><br><span class="line">    rows = info_table.find_all(<span class="string">&#x27;tr&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    topic_category = rows[<span class="number">0</span>].find_all(<span class="string">&#x27;td&#x27;</span>)[<span class="number">3</span>].text.strip()</span><br><span class="line">    publishing_organization = rows[<span class="number">1</span>].find_all(<span class="string">&#x27;td&#x27;</span>)[<span class="number">1</span>].text.strip()</span><br><span class="line">    title = rows[<span class="number">2</span>].find_all(<span class="string">&#x27;td&#x27;</span>)[<span class="number">1</span>].text.strip()</span><br><span class="line">    document_number = rows[<span class="number">3</span>].find_all(<span class="string">&#x27;td&#x27;</span>)[<span class="number">1</span>].text.strip()</span><br><span class="line">    written_date = rows[<span class="number">1</span>].find_all(<span class="string">&#x27;td&#x27;</span>)[<span class="number">3</span>].text.strip()</span><br><span class="line">    release_date = rows[<span class="number">3</span>].find_all(<span class="string">&#x27;td&#x27;</span>)[<span class="number">3</span>].text.strip()</span><br><span class="line">    written_date = datetime.strptime(written_date, <span class="string">&#x27;%Y年%m月%d日&#x27;</span>)</span><br><span class="line">    release_date = datetime.strptime(release_date, <span class="string">&#x27;%Y年%m月%d日&#x27;</span>)</span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用id属性查找包含正文内容的div元素</span></span><br><span class="line">    pages_content = soup.find(<span class="string">&#x27;td&#x27;</span>, <span class="built_in">id</span>=<span class="string">&#x27;UCAP-CONTENT&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查找正文内容中的所有p元素</span></span><br><span class="line">    p_tags = pages_content.find_all(<span class="string">&#x27;p&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历所有的p标签，并将它们的文本内容连接起来</span></span><br><span class="line">    text = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> p_tag <span class="keyword">in</span> p_tags:</span><br><span class="line">        <span class="comment"># 判断该p标签内是否包含&lt;span&gt;标签</span></span><br><span class="line">        <span class="keyword">if</span> p_tag.find(<span class="string">&#x27;span&#x27;</span>):</span><br><span class="line">            <span class="comment"># 使用extract()方法将该&lt;span&gt;标签从文档中去除</span></span><br><span class="line">            p_tag.find(<span class="string">&#x27;span&#x27;</span>).extract()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将该p标签的文本内容连接到text变量中</span></span><br><span class="line">        text += p_tag.text.strip() + <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">    </span><br><span class="line">    data = (new_id,title,publishing_organization,written_date,release_date,document_number,text,<span class="string">&#x27;国家级&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">UA = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36&#x27;</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User_Agent&#x27;</span>: UA&#125;</span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">83</span>):  <span class="comment"># 假设要爬取前100页</span></span><br><span class="line">    cn = <span class="string">f&#x27;http://sousuo.gov.cn/column/30469/<span class="subst">&#123;page&#125;</span>.htm&#x27;</span></span><br><span class="line">    <span class="comment"># 爬取该页的数据</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(cn, headers=headers)</span><br><span class="line">        r.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line"></span><br><span class="line">        soup = BeautifulSoup(r.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">        <span class="comment">#获取该页的所有政策链接</span></span><br><span class="line">        links = soup.find_all(href=re.<span class="built_in">compile</span>(<span class="string">&#x27;content&#x27;</span>))</span><br><span class="line">        urls = []</span><br><span class="line">        <span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">            </span><br><span class="line">            url = link.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">            urls.append(<span class="built_in">str</span>(url))</span><br><span class="line">        <span class="built_in">print</span>(urls)</span><br><span class="line">        crawl_and_process(urls)</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 关闭游标和连接</span></span><br><span class="line">           </span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure></div>
            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li><strong>Title:</strong> 爬虫</li>
        <li><strong>Author:</strong> Ikko</li>
        <li><strong>Created at:</strong> 2023-04-19 12:58:02</li>
        
            <li>
                <strong>Updated at:</strong> 2023-05-22 14:16:01
            </li>
        
        <li>
            <strong>Link:</strong> https://redefine.ohevan.com/2023/04/19/爬虫/
        </li>
        <li>
            <strong>License:</strong> This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a>.
        </li>
    </ul>
</div>

                </div>
            

            

            

            
                <div class="article-nav">
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="/2023/04/14/django%E9%83%A8%E7%BD%B2%E4%B8%8A%E4%BA%91/"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">django部署上云</span>
                                    <span class="post-nav-item">Next posts</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            


            
                <div class="comment-container">
                    <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fa-solid fa-comments"></i>&nbsp;Comments
    </div>
    

        
            
 
    <div id="waline"></div>
    <script type="module"  data-pjax>
        import { init } from 'https://evan.beee.top/js/waline.mjs';

        function loadWaline() {
            init({
                el: '#waline',
                serverURL: 'https://waline-nine-kohl.vercel.app/',
                lang: 'zh-CN',
                dark: 'body[class~="dark-mode"]',
                requiredMeta: ['nick','mail'], // cannot customize by theme config, change it yourself
            });
        }

        if ('true') {
            const loadWalineTimeout = setTimeout(() => {
                loadWaline();
                clearTimeout(loadWalineTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadWaline);
        }
        
    </script>



        
    
</div>

                </div>
            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">On this page</div>
        <div class="page-title">爬虫</div>
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E4%BB%A3%E7%A0%81"><span class="nav-text">初始代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%AA%E4%BA%BA%E7%90%86%E8%A7%A3"><span class="nav-text">个人理解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E4%BF%AE%E6%94%B9"><span class="nav-text">代码修改</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#url%E4%BF%AE%E6%94%B9"><span class="nav-text">url修改</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9%E6%88%90%E5%8A%9F%E5%90%8E%E5%8F%91%E7%8E%B0%EF%BC%8C%E7%BB%9D%E5%AF%B9url%E5%92%8C%E7%9B%B8%E5%AF%B9url%E5%AF%B9%E5%BA%94%E7%9A%84%E9%A1%B5%E9%9D%A2%E7%BB%93%E6%9E%84%E6%98%AF%E4%B8%8D%E5%90%8C%E7%9A%84"><span class="nav-text">修改成功后发现，绝对url和相对url对应的页面结构是不同的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A1%AE%E4%BF%9D%E6%95%B0%E6%8D%AE%E4%B8%8D%E9%87%8D%E5%A4%8D"><span class="nav-text">确保数据不重复</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E7%BB%88%E4%BB%A3%E7%A0%81"><span class="nav-text">最终代码</span></a></li></ol>

    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>
            
            

        </div>

        <div class="main-content-footer">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info">
            &copy;
            
              <span>2022</span>
              -
            
            2023&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Ikko</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv" class="busuanzi_container_site_uv">
                        VISITOR COUNT&nbsp;<span id="busuanzi_value_site_uv" class="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="busuanzi_container_site_pv">
                        TOTAL PAGE VIEWS&nbsp;<span id="busuanzi_value_site_pv" class="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            <span class="powered-by-container">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" href="https://hexo.io">Hexo</a></span>
                <br>
            <span class="theme-version-container">THEME&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.1.4</a>
        </div>
        
        
        
            <div id="start_div" style="display:none">
                2022/12/17 11:45:14
            </div>
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
        
            <script async data-pjax>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    


</main>



<script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/utils.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/main.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/layouts/navbarShrink.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/tools/scrollTopBottom.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/tools/lightDarkSwitch.js"></script>




    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/tools/codeBlock.js"></script>



    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/layouts/lazyload.js"></script>



    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/tools/runtime.js"></script>
    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/layouts/odometer.min.js"></script>
    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/assets/odometer-theme-minimal.css">



  <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/libs/Typed.min.js"></script>
  <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/plugins/typed.js"></script>






<div class="post-scripts pjax">
    
        <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/tools/tocToggle.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/libs/anime.min.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/layouts/toc.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/plugins/tabs.js"></script>
    
</div>


    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.4/source/js/libs/pjax.min.js"></script>
<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax',
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            Global.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            Global.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            Global.refresh();
        });
    });
</script>




</body>
</html>
