<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Ikko">
    
    <!-- Completely eliminate flash of wrong theme -->
    <script>
        (function() {
            const THEME_KEY = "REDEFINE-THEME-STATUS";
            const DARK = "dark", LIGHT = "light";
            
            // Get preferred theme
            function getTheme() {
                try {
                    const saved = localStorage.getItem(THEME_KEY);
                    if (saved) {
                        const { isDark } = JSON.parse(saved);
                        return isDark ? DARK : LIGHT;
                    }
                } catch (e) {}
                
                return matchMedia("(prefers-color-scheme: dark)").matches ? DARK : LIGHT;
            }
            
            // Apply theme to document
            function applyTheme(theme) {
                const isDark = theme === DARK;
                const root = document.documentElement;
                
                // Set data attribute for CSS variables
                root.setAttribute("data-theme", theme);
                
                // Set classes for compatibility
                root.classList.add(theme);
                root.classList.remove(isDark ? LIGHT : DARK);
                root.style.colorScheme = theme;
            }
            
            // Initial application
            const theme = getTheme();
            applyTheme(theme);
            
            // Listen for system preference changes
            matchMedia("(prefers-color-scheme: dark)").addEventListener("change", ({ matches }) => {
                // Only update if using system preference (no localStorage entry)
                if (!localStorage.getItem(THEME_KEY)) {
                    applyTheme(matches ? DARK : LIGHT);
                }
            });
            
            // Set body classes once DOM is ready
            if (document.readyState !== "loading") {
                document.body.classList.add(theme + "-mode");
            } else {
                document.addEventListener("DOMContentLoaded", () => {
                    document.body.classList.add(theme + "-mode");
                    document.body.classList.remove((theme === DARK ? LIGHT : DARK) + "-mode");
                });
            }
        })();
    </script>
    
    <!-- Critical CSS to prevent flash -->
    <style>
        :root[data-theme="dark"] {
            --background-color: #202124;
            --background-color-transparent: rgba(32, 33, 36, 0.6);
            --second-background-color: #2d2e32;
            --third-background-color: #34353a;
            --third-background-color-transparent: rgba(32, 33, 36, 0.6);
            --primary-color: #0066CC;
            --first-text-color: #ffffff;
            --second-text-color: #eeeeee;
            --third-text-color: #bebec6;
            --fourth-text-color: #999999;
            --default-text-color: #bebec6;
            --invert-text-color: #373D3F;
            --border-color: rgba(255, 255, 255, 0.08);
            --selection-color: #0066CC;
            --shadow-color-1: rgba(255, 255, 255, 0.08);
            --shadow-color-2: rgba(255, 255, 255, 0.05);
        }
        
        :root[data-theme="light"] {
            --background-color: #fff;
            --background-color-transparent: rgba(255, 255, 255, 0.6);
            --second-background-color: #f8f8f8;
            --third-background-color: #f2f2f2;
            --third-background-color-transparent: rgba(241, 241, 241, 0.6);
            --primary-color: #0066CC;
            --first-text-color: #16171a;
            --second-text-color: #2f3037;
            --third-text-color: #5e5e5e;
            --fourth-text-color: #eeeeee;
            --default-text-color: #373D3F;
            --invert-text-color: #bebec6;
            --border-color: rgba(0, 0, 0, 0.08);
            --selection-color: #0066CC;
            --shadow-color-1: rgba(0, 0, 0, 0.08);
            --shadow-color-2: rgba(0, 0, 0, 0.05);
        }
        
        body {
            background-color: var(--background-color);
            color: var(--default-text-color);
        }
        
        /* Apply body classes as soon as DOM is ready */
        :root[data-theme="dark"] body {
            background-color: var(--background-color);
            color: var(--default-text-color);
        }
    </style>
    
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    
        
        
        
            <link rel="preconnect" href="https://registry.npmmirror.com" crossorigin>
        
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://ikko-debug.github.io/2026/02/05/suanzi/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
    
    
        
        <meta name="description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:type" content="article">
<meta property="og:title" content="算子实例">
<meta property="og:url" content="http://ikko-debug.github.io/2026/02/05/suanzi/index.html">
<meta property="og:site_name" content="IKKO">
<meta property="og:description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://ikko-debug.github.io/images/redefine-og.webp">
<meta property="article:published_time" content="2026-02-05T05:56:54.000Z">
<meta property="article:modified_time" content="2026-02-22T12:26:10.315Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://ikko-debug.github.io/images/redefine-og.webp">
    
    
        <!-- Google tag (gtag.js) -->
        <script src="https://www.googletagmanager.com/gtag/js?id=G-P5152X2PGJ"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-P5152X2PGJ');
        </script>
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/idea.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/idea.svg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/idea.svg">
    <!--- Page Info-->
    
    <title>
        
            算子实例 | IKKO
        
    </title>

    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fonts/Chillax/chillax.css">

    <!--- Inject Part-->
    

    
<link rel="stylesheet" href="/css/style.css">


    
        <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/css/build/tailwind.css">
    

    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fonts/GeistMono/geist-mono.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fonts/Geist/geist.css">
    <!--- Font Part-->
    
    
    
    
    
    

    <script id="hexo-configurations">
    window.config = {"hostname":"ikko-debug.github.io","root":"/","language":"en","path":"search.xml"};
    window.theme = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true,"delete_mask":false,"title_alignment":"left","headings_top_spacing":{"h1":"3.2rem","h2":"2.4rem","h3":"1.9rem","h4":"1.6rem","h5":"1.4rem","h6":"1.3rem"}},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":true,"list":[""]},"code_block":{"copy":true,"style":"mac","highlight_theme":{"light":"github","dark":"vs2015"},"font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":{"enable":true,"default":"cc_by_nc_sa"},"lazyload":true,"pangu_js":false,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null,"default_mode":"dark"},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null},"title":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"website_counter":{"url":"https://cn.vercount.one/js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"preloader":{"enable":false,"custom_message":null},"open_graph":{"enable":true,"image":"/images/redefine-og.webp","description":"Hexo Theme Redefine, Redefine Your Hexo Journey."},"google_analytics":{"enable":true,"id":"G-P5152X2PGJ"}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/light.jpg","dark":"/images/dark.jpg"},"title":"若有恒，何必三更眠五更起 最无益，只怕一日曝十日寒","subtitle":{"text":[],"hitokoto":{"enable":false,"show_author":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"1.5rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":false,"style":"default","links":{"github":"https://github.com/ikko-debug","instagram":null,"zhihu":null,"twitter":null,"email":null},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null,"lrc":null}]},"mermaid":{"enable":false,"version":"11.4.1"}},"version":"2.8.5","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"width":{"home":"1200px","pages":"1000px"},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"FriendLinks":{"icon":"fa-solid fa-link","submenus":{"yebao":"https://danmoliuhen.github.io/","wu-22":"https://wu-22.github.io/","jjb":"https://byjiangjb.github.io/"}},"album":{"icon":"fa-solid fa-image","path":"/masonry/"},"Bookmarks":{"icon":"fa-solid fa-bookmark","path":"/bookmarks/"}},"search":{"enable":true,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":"never","show_on_mobile":true,"links":null},"article_date_format":"auto","excerpt_length":200,"categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2022/8/17 11:45:14"};
    window.lang_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    window.data = {"masonry":true};
  </script>
    
    <!--- Fontawesome Part-->
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/fontawesome.min.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/brands.min.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/solid.min.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/regular.min.css">
    
    
    
    
<meta name="generator" content="Hexo 6.3.0"></head>



<body>
	<div class="progress-bar-container">
	

	
	<span class="pjax-progress-bar"></span>
	<!--        <span class="swup-progress-icon">-->
	<!--            <i class="fa-solid fa-circle-notch fa-spin"></i>-->
	<!--        </span>-->
	
</div>

<main class="page-container" id="swup">

	

	<div class="main-content-container flex flex-col justify-between min-h-dvh">
		<div class="main-content-header">
			<header class="navbar-container px-6 md:px-12">
    <div class="navbar-content transition-navbar ">
        <div class="left">
            
                <a class="logo-image h-8 w-8 sm:w-10 sm:h-10 mr-3" href="/">
                    <img src="/images/idea.svg" class="w-full h-full rounded-xs">
                </a>
            
            <a class="logo-title" href="/">
                
                IKKO
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/"
                                        >
                                    <i class="fa-regular fa-house fa-fw"></i>
                                    HOME
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/archives"
                                        >
                                    <i class="fa-regular fa-archive fa-fw"></i>
                                    ARCHIVES
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown"
                                   href="#"
                                        onClick=&#34;return false;&#34;>
                                    <i class="fa-solid fa-link fa-fw"></i>
                                    FRIENDLINKS
                                    <i class="fa-solid fa-chevron-down fa-fw"></i>
                                </a>

                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                        
                                            <li>
                                                <a target="_blank" rel="noopener" href="https://danmoliuhen.github.io/">
                                                    YEBAO
                                                </a>
                                            </li>
                                        
                                            <li>
                                                <a target="_blank" rel="noopener" href="https://wu-22.github.io/">
                                                    WU-22
                                                </a>
                                            </li>
                                        
                                            <li>
                                                <a target="_blank" rel="noopener" href="https://byjiangjb.github.io/">
                                                    JJB
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/masonry/"
                                        >
                                    <i class="fa-solid fa-image fa-fw"></i>
                                    ALBUM
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/bookmarks/"
                                        >
                                    <i class="fa-solid fa-bookmark fa-fw"></i>
                                    BOOKMARKS
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                    
                        <li class="navbar-item search search-popup-trigger">
                            <i class="fa-solid fa-magnifying-glass"></i>
                        </li>
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i>
                    </div>
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile sheet -->
    <div class="navbar-drawer h-dvh w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between">
        <ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start">
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/"
                        >
                            <span>
                                HOME
                            </span>
                            
                                <i class="fa-regular fa-house fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/archives"
                        >
                            <span>
                                ARCHIVES
                            </span>
                            
                                <i class="fa-regular fa-archive fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item-sub text-base my-1.5 flex flex-col w-full">
                        
                        <div class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary cursor-pointer text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                             navbar-data-toggle="submenu-FriendLinks"
                        >
                            <span>
                                FRIENDLINKS
                            </span>
                            
                                <i class="fa-solid fa-chevron-right fa-sm fa-fw transition-all"></i>
                            
                        </div>
                        

                        
                            <div class="flex-col items-start px-2 py-2 hidden" data-target="submenu-FriendLinks">
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           target="_blank" rel="noopener" href="https://danmoliuhen.github.io/">YEBAO</a>
                                    </div>
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           target="_blank" rel="noopener" href="https://wu-22.github.io/">WU-22</a>
                                    </div>
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           target="_blank" rel="noopener" href="https://byjiangjb.github.io/">JJB</a>
                                    </div>
                                
                            </div>
                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/masonry/"
                        >
                            <span>
                                ALBUM
                            </span>
                            
                                <i class="fa-solid fa-image fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/bookmarks/"
                        >
                            <span>
                                BOOKMARKS
                            </span>
                            
                                <i class="fa-solid fa-bookmark fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            

            
            
        </ul>

        <div class="statistics flex justify-around my-2.5">
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/tags">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">7</div>
        <div class="label text-third-text-color text-sm">Tags</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/categories">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">0</div>
        <div class="label text-third-text-color text-sm">Categories</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/archives">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">34</div>
        <div class="label text-third-text-color text-sm">Posts</div>
    </a>
</div>
    </div>

    <div class="window-mask"></div>

</header>


		</div>

		<div class="main-content-body transition-fade-up">
			

			<div class="main-content">
				<div class="post-page-container flex relative justify-between box-border w-full h-full">
	<div class="article-content-container">

		<div class="article-title relative w-full">
			
			<div class="w-full flex items-center pt-6 justify-start">
				<h1 class="article-title-regular text-second-text-color tracking-tight text-4xl md:text-6xl font-semibold px-2 sm:px-6 md:px-8 py-3">算子实例</h1>
			</div>
			
		</div>

		
		<div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8">
			<div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]">
				<img src="/images/56D9D2F8A1A04F1D59D48CAD0011E450.jpg">
			</div>
			<div class="info flex flex-col justify-between">
				<div class="author flex items-center">
					<span class="name text-default-text-color text-lg font-semibold">Ikko</span>
					
					<span class="author-label ml-1.5 text-xs px-2 py-0.5 rounded-small text-third-text-color border border-shadow-color-1">Lv4</span>
					
				</div>
				<div class="meta-info">
					<div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2026-02-05 13:56:54</span>
        <span class="mobile">2026-02-05 13:56:54</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2026-02-22 20:26:10</span>
            <span class="mobile">2026-02-22 20:26:10</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fa-regular fa-typewriter"></i>&nbsp;<span>7.9k Words</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fa-regular fa-clock"></i>&nbsp;<span>36 Mins</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

				</div>
			</div>
		</div>
		

		


		<div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8">
			<p>做了一个简单的cuda项目，故记录下来。</p>
<h2 id="头文件和宏定义"><a href="#头文件和宏定义" class="headerlink" title="头文件和宏定义"></a>头文件和宏定义</h2><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_fp16.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cassert&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;../tester/utils.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> BLOCK_SIZE 256</span></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> Br = <span class="number">16</span>;  <span class="comment">// Query tile 的行数</span></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> Bc = <span class="number">16</span>;  <span class="comment">// Key/Value tile 的列数</span></span><br></pre></td></tr></table></figure></div>

<h2 id="1-Warp-级归约操作"><a href="#1-Warp-级归约操作" class="headerlink" title="1. Warp 级归约操作"></a>1. Warp 级归约操作</h2><h3 id="Warp-Reduce-Sum"><a href="#Warp-Reduce-Sum" class="headerlink" title="Warp Reduce Sum"></a>Warp Reduce Sum</h3><p>在 32 个线程内快速求和（不需要 Shared Memory）</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ __forceinline__ T <span class="title">warpReduceSum</span><span class="params">(T val)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 0xffffffff 表示 Warp 里所有 32 个线程都参与 0x 16进制</span></span><br><span class="line">    <span class="comment">// 每次折叠一半: 16 -&gt; 8 -&gt; 4 -&gt; 2 -&gt; 1</span></span><br><span class="line">    <span class="comment">// 除2等同于右移1位</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = warpSize / <span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="comment">// &quot;当前值&quot; + &quot;offset个偏移量位置&quot;</span></span><br><span class="line">        val += __shfl_down_sync(<span class="number">0xffffffff</span>, val, offset);</span><br><span class="line">        <span class="comment">//返回来自 同一 warp 内、laneId + delta 的线程所持有的 val</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> val;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="Warp-Reduce-Max"><a href="#Warp-Reduce-Max" class="headerlink" title="Warp Reduce Max"></a>Warp Reduce Max</h3><p>辅助函数：Warp 内求最大值</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ __forceinline__ T <span class="title">warpReduceMax</span><span class="params">(T val)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>) &#123;</span><br><span class="line">        T temp = __shfl_down_sync(<span class="number">0xffffffff</span>, val, offset);</span><br><span class="line">        <span class="keyword">if</span> (temp &gt; val) val = temp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> val;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h2 id="2-Block-级归约操作"><a href="#2-Block-级归约操作" class="headerlink" title="2. Block 级归约操作"></a>2. Block 级归约操作</h2><p>blockDim.x定义在dim3 block(BLOCK_SIZE);</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ __forceinline__ T <span class="title">blockReduceSum</span><span class="params">(T val)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//blockDim.x ≤ 32（单 Warp block）</span></span><br><span class="line">  <span class="keyword">if</span> (blockDim.x &lt;= warpSize) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">warpReduceSum</span>(val);</span><br><span class="line">  &#125;</span><br><span class="line">    <span class="comment">//blockDim.x &gt; 32（多 Warp block）</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 静态分配共享内存，用来存放每个 Warp 的总和</span></span><br><span class="line">    <span class="comment">// 一个 Block 最多 32 个 Warps</span></span><br><span class="line">    <span class="type">static</span> __shared__ T shared[<span class="number">32</span>]; </span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> lane = threadIdx.x % warpSize;  <span class="comment">// Warp内排第几 (0-31)</span></span><br><span class="line">    <span class="type">int</span> wid  = threadIdx.x / warpSize;  <span class="comment">// 第几个 Warp</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 每个 Warp 内部先归并</span></span><br><span class="line">    val = <span class="built_in">warpReduceSum</span>(val);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Warp 0把结果写到 Shared Memory</span></span><br><span class="line">    <span class="keyword">if</span> (lane == <span class="number">0</span>) &#123;</span><br><span class="line">        shared[wid] = val;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 等待所有 Warp 写完</span></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 由第一个 Warp (warp 0) 负责把 Shared Memory 里的数加起来</span></span><br><span class="line">    <span class="comment">// 只有前 (blockDim.x / 32) 个线程需要读取数据</span></span><br><span class="line">    val = (threadIdx.x &lt; blockDim.x / warpSize) ? shared[lane] : <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 第一个 Warp 再次做 Warp Reduce </span></span><br><span class="line">    <span class="keyword">if</span> (wid == <span class="number">0</span>) &#123;</span><br><span class="line">        val = <span class="built_in">warpReduceSum</span>(val);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> val;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h2 id="3-矩阵迹的计算"><a href="#3-矩阵迹的计算" class="headerlink" title="3. 矩阵迹的计算"></a>3. 矩阵迹的计算</h2><h3 id="迹的核函数"><a href="#迹的核函数" class="headerlink" title="迹的核函数"></a>迹的核函数</h3><p>注意核函数均定义为void</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">traceKernel</span><span class="params">(<span class="type">const</span> T* __restrict__ input, <span class="type">size_t</span> rows, <span class="type">size_t</span> cols, <span class="type">size_t</span> n, T* out)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 这里的 stride 是整个 Grid 一次能处理的数据量</span></span><br><span class="line">    <span class="type">size_t</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">size_t</span> stride = blockDim.x * gridDim.x;  <span class="comment">// 一个 Grid 的总线程数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// local sum for each thread in register</span></span><br><span class="line">    T local_sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 防止一个 Grid 处理不完</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = idx; i &lt; n; i += stride) &#123;</span><br><span class="line">        <span class="comment">// i * cols + i 是对角线元素的物理索引</span></span><br><span class="line">        local_sum += input[i * cols + i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Block 内部归并</span></span><br><span class="line">    local_sum = <span class="built_in">blockReduceSum</span>(local_sum);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 只有 Block 的 Thread 0 负责写回 Global Memory</span></span><br><span class="line">    <span class="comment">// 极大减少 atomicAdd 的竞争</span></span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="built_in">atomicAdd</span>(out, local_sum);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="Host-端迹的计算函数"><a href="#Host-端迹的计算函数" class="headerlink" title="Host 端迹的计算函数"></a>Host 端迹的计算函数</h3><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">T <span class="title">trace</span><span class="params">(<span class="type">const</span> std::vector&lt;T&gt;&amp; h_input, <span class="type">size_t</span> rows, <span class="type">size_t</span> cols)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (rows == <span class="number">0</span> || cols == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">T</span>(<span class="number">0</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">size_t</span> n = std::<span class="built_in">min</span>(rows, cols);</span><br><span class="line">  <span class="type">size_t</span> total_elems = rows * cols;</span><br><span class="line"></span><br><span class="line">  T* d_input = <span class="literal">nullptr</span>;</span><br><span class="line">  T* d_out = <span class="literal">nullptr</span>;</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;d_input, total_elems * <span class="built_in">sizeof</span>(T)));</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;d_out, <span class="built_in">sizeof</span>(T)));</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMemcpy</span>(d_input, h_input.<span class="built_in">data</span>(), total_elems * <span class="built_in">sizeof</span>(T), cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMemset</span>(d_out, <span class="number">0</span>, <span class="built_in">sizeof</span>(T)));</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(BLOCK_SIZE)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">((n + BLOCK_SIZE - <span class="number">1</span>) / BLOCK_SIZE)</span></span>;</span><br><span class="line">  traceKernel&lt;T&gt;&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_input, rows, cols, n, d_out);<span class="comment">//这里传了d_out地址</span></span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaGetLastError</span>());</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaDeviceSynchronize</span>());</span><br><span class="line"></span><br><span class="line">  T h_out = <span class="built_in">T</span>(<span class="number">0</span>);</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMemcpy</span>(&amp;h_out, d_out, <span class="built_in">sizeof</span>(T), cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaFree</span>(d_input));</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaFree</span>(d_out));</span><br><span class="line">  <span class="keyword">return</span> h_out;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h2 id="4-注意力机制-类型转换"><a href="#4-注意力机制-类型转换" class="headerlink" title="4. 注意力机制 - 类型转换"></a>4. 注意力机制 - 类型转换</h2><h3 id="float-类型转换"><a href="#float-类型转换" class="headerlink" title="float 类型转换"></a>float 类型转换</h3><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ __forceinline__ <span class="type">float</span> <span class="title">to_float</span><span class="params">(T v)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line">__device__ __forceinline__ <span class="type">float</span> <span class="built_in">to_float</span>&lt;<span class="type">float</span>&gt;(<span class="type">float</span> v) &#123;</span><br><span class="line">  <span class="keyword">return</span> v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line">__device__ __forceinline__ <span class="type">float</span> <span class="built_in">to_float</span>&lt;half&gt;(half v) &#123;</span><br><span class="line">  <span class="keyword">return</span> __half2float(v);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="反向类型转换"><a href="#反向类型转换" class="headerlink" title="反向类型转换"></a>反向类型转换</h3><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ __forceinline__ T <span class="title">from_float</span><span class="params">(<span class="type">float</span> v)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;&gt; <span class="comment">//这是一个模板特化，不再引入新的模板参数</span></span><br><span class="line"><span class="comment">//&lt;float&gt;指定了特化的类型</span></span><br><span class="line">__device__ __forceinline__ <span class="type">float</span> <span class="built_in">from_float</span>&lt;<span class="type">float</span>&gt;(<span class="type">float</span> v) &#123;</span><br><span class="line">  <span class="keyword">return</span> v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line">__device__ __forceinline__ half <span class="built_in">from_float</span>&lt;half&gt;(<span class="type">float</span> v) &#123;</span><br><span class="line">  <span class="keyword">return</span> __float2half(v);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p><strong>forceinline</strong></p>
<p>强制内联：<br>    •	减少函数调用开销<br>    •	对 warp-level 操作重要<br>  ifelse逻辑会runtime branching，性能差</p>
<h2 id="5-朴素注意力实现"><a href="#5-朴素注意力实现" class="headerlink" title="5. 朴素注意力实现"></a>5. 朴素注意力实现</h2><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">native_attention_kernel</span><span class="params">(<span class="type">const</span> T* q, <span class="type">const</span> T* k, <span class="type">const</span> T* v, T* o,</span></span></span><br><span class="line"><span class="params"><span class="function">                                        <span class="type">int</span> batch_size, <span class="type">int</span> target_seq_len, <span class="type">int</span> src_seq_len,</span></span></span><br><span class="line"><span class="params"><span class="function">                                        <span class="type">int</span> query_heads, <span class="type">int</span> kv_heads, <span class="type">int</span> head_dim, <span class="type">bool</span> is_causal)</span> </span>&#123;</span><br><span class="line">  <span class="type">size_t</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="type">const</span> <span class="type">size_t</span> o_elems = batch_size * target_seq_len * query_heads * head_dim;  <span class="comment">//输出元素总数</span></span><br><span class="line">  <span class="keyword">if</span> (idx &gt;= o_elems) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 线性索引 -&gt; (b, t, qh, d)</span></span><br><span class="line">  <span class="comment">//static_cast 用于在不同类型之间进行显式转换</span></span><br><span class="line">  <span class="type">int</span> d = <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(idx % head_dim);</span><br><span class="line">  <span class="type">size_t</span> tmp = idx / head_dim;</span><br><span class="line">  <span class="type">int</span> qh = <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(tmp % query_heads);</span><br><span class="line">  tmp /= query_heads;</span><br><span class="line">  <span class="type">int</span> t = <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(tmp % target_seq_len);</span><br><span class="line">  <span class="type">int</span> b = <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(tmp / target_seq_len);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// GQA: query head -&gt; kv head</span></span><br><span class="line">  <span class="comment">//多个 Q head 共享一个 KV head</span></span><br><span class="line">  <span class="type">int</span> kv_h = (qh * kv_heads) / query_heads;</span><br><span class="line"></span><br><span class="line">  <span class="type">const</span> <span class="type">float</span> scale = <span class="number">1.0f</span> / <span class="built_in">sqrtf</span>(<span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(head_dim));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Step 1: 计算 max score (数值稳定)</span></span><br><span class="line">  <span class="type">float</span> max_score = -INFINITY;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> sk = <span class="number">0</span>; sk &lt; src_seq_len; ++sk) &#123;</span><br><span class="line">    <span class="keyword">if</span> (is_causal &amp;&amp; sk &gt; t) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">const</span> T* q_ptr = q + (((b * target_seq_len + t) * query_heads + qh) * head_dim);</span><br><span class="line">    <span class="type">const</span> T* k_ptr = k + (((b * src_seq_len + sk) * kv_heads + kv_h) * head_dim);</span><br><span class="line">    <span class="type">float</span> dot = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; head_dim; ++i) &#123;</span><br><span class="line">      dot += <span class="built_in">to_float</span>(q_ptr[i]) * <span class="built_in">to_float</span>(k_ptr[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">float</span> score = dot * scale;</span><br><span class="line">    <span class="keyword">if</span> (score &gt; max_score) &#123;</span><br><span class="line">      max_score = score;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Step 2: 计算 softmax 分母</span></span><br><span class="line">  <span class="type">float</span> denom = <span class="number">0.0f</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> sk = <span class="number">0</span>; sk &lt; src_seq_len; ++sk) &#123;</span><br><span class="line">    <span class="keyword">if</span> (is_causal &amp;&amp; sk &gt; t) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">const</span> T* q_ptr = q + (((b * target_seq_len + t) * query_heads + qh) * head_dim);</span><br><span class="line">    <span class="type">const</span> T* k_ptr = k + (((b * src_seq_len + sk) * kv_heads + kv_h) * head_dim);</span><br><span class="line">    <span class="type">float</span> dot = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; head_dim; ++i) &#123;</span><br><span class="line">      dot += <span class="built_in">to_float</span>(q_ptr[i]) * <span class="built_in">to_float</span>(k_ptr[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">float</span> score = dot * scale;</span><br><span class="line">    denom += <span class="built_in">expf</span>(score - max_score);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (denom == <span class="number">0.0f</span>) &#123;</span><br><span class="line">    o[idx] = <span class="built_in">from_float</span>&lt;T&gt;(<span class="number">0.0f</span>);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Step 3: 计算加权和 (输出元素)</span></span><br><span class="line">  <span class="type">float</span> out_val = <span class="number">0.0f</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> sk = <span class="number">0</span>; sk &lt; src_seq_len; ++sk) &#123;</span><br><span class="line">    <span class="keyword">if</span> (is_causal &amp;&amp; sk &gt; t) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">const</span> T* q_ptr = q + (((b * target_seq_len + t) * query_heads + qh) * head_dim);</span><br><span class="line">    <span class="type">const</span> T* k_ptr = k + (((b * src_seq_len + sk) * kv_heads + kv_h) * head_dim);</span><br><span class="line">    <span class="type">const</span> T* v_ptr = v + (((b * src_seq_len + sk) * kv_heads + kv_h) * head_dim);</span><br><span class="line">    <span class="type">float</span> dot = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; head_dim; ++i) &#123;</span><br><span class="line">      dot += <span class="built_in">to_float</span>(q_ptr[i]) * <span class="built_in">to_float</span>(k_ptr[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">float</span> score = dot * scale;</span><br><span class="line">    <span class="type">float</span> w = <span class="built_in">expf</span>(score - max_score) / denom;</span><br><span class="line">    out_val += w * <span class="built_in">to_float</span>(v_ptr[d]);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  o[idx] = <span class="built_in">from_float</span>&lt;T&gt;(out_val);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h2 id="6-Flash-Attention-V1-优化版本"><a href="#6-Flash-Attention-V1-优化版本" class="headerlink" title="6. Flash Attention V1 - 优化版本"></a>6. Flash Attention V1 - 优化版本</h2><h3 id="核心改进说明"><a href="#核心改进说明" class="headerlink" title="核心改进说明"></a>核心改进说明</h3><p>Flash Attention V2升级到支持<strong>分块(Tiling)计算</strong>：</p>
<ul>
<li><strong>2D线程块</strong>：从1D改为<code>(Bc, Br)</code>，充分利用硬件</li>
<li><strong>Tile操作</strong>：分别载入和计算Q、K、V的tile，减少全局内存访问</li>
<li><strong>Bank Conflict避免</strong>：使用<code>smem_stride</code>参数在共享内存中填充，避免冲突</li>
<li><strong>Online Softmax</strong>：在计算过程中动态更新max和denominator，数值稳定</li>
<li><strong>Warp级规约</strong>：使用shuffle指令进行高效的跨线程规约</li>
</ul>
<h3 id="优化后的核函数"><a href="#优化后的核函数" class="headerlink" title="优化后的核函数"></a>优化后的核函数</h3><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">flash_attention_v1_kernel</span><span class="params">(<span class="type">const</span> T* __restrict__ Q,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          <span class="type">const</span> T* __restrict__ K,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          <span class="type">const</span> T* __restrict__ V,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          T* __restrict__ O,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          <span class="type">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          <span class="type">int</span> target_seq_len,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          <span class="type">int</span> src_seq_len,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          <span class="type">int</span> query_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          <span class="type">int</span> kv_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          <span class="type">int</span> head_dim,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          <span class="type">int</span> smem_stride,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          <span class="type">bool</span> is_causal,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          <span class="type">float</span> scale)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 获取当前线程在 block 内的 x 坐标（对应 K/V 的列索引，范围 0-Bc）</span></span><br><span class="line">  <span class="type">int</span> tx = threadIdx.x; <span class="comment">//Bc 维度</span></span><br><span class="line">  <span class="comment">// 获取当前线程在 block 内的 y 坐标（对应 Q 的行索引，范围 0-Br）</span></span><br><span class="line">  <span class="type">int</span> ty = threadIdx.y; <span class="comment">//Br 维度</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 获取当前 block 所属的 batch 索引</span></span><br><span class="line">  <span class="type">int</span> batch_idx = blockIdx.z;</span><br><span class="line">  <span class="comment">// 获取当前 block 所属的 attention head 索引</span></span><br><span class="line">  <span class="type">int</span> head_idx = blockIdx.y;</span><br><span class="line">  <span class="comment">// 获取当前 block 在 query 行方向上的 block 索引</span></span><br><span class="line">  <span class="type">int</span> q_block_idx = blockIdx.x;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算当前 block 负责的 query 行的起始位置（0-indexed）</span></span><br><span class="line">  <span class="type">int</span> q_start_idx = q_block_idx * Br;</span><br><span class="line">  <span class="comment">// 计算当前 block 实际处理的 query 行数（边界处理，可能小于 Br）</span></span><br><span class="line">  <span class="type">int</span> q_len_local = <span class="built_in">min</span>(Br, target_seq_len - q_start_idx);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// GQA机制：将 query head 映射到 kv head（多个 Q head 共享一个 KV head）</span></span><br><span class="line">  <span class="type">int</span> kv_head_idx = (head_idx * kv_heads) / query_heads;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 声明 block 内所有线程共享的内存指针</span></span><br><span class="line">  <span class="keyword">extern</span> __shared__ <span class="type">float</span> smem[];</span><br><span class="line">  <span class="comment">// 指向 Q tile 的共享内存起始地址（大小：Br * smem_stride * sizeof(float)）</span></span><br><span class="line">  <span class="type">float</span>* s_Q = smem;                                  <span class="comment">// Br * smem_stride</span></span><br><span class="line">  <span class="comment">// 指向 K tile 的共享内存起始地址（大小：Bc * smem_stride * sizeof(float)）</span></span><br><span class="line">  <span class="type">float</span>* s_K = s_Q + Br * smem_stride;                <span class="comment">// Bc * smem_stride</span></span><br><span class="line">  <span class="comment">// 指向 V tile 的共享内存起始地址（大小：Bc * smem_stride * sizeof(float)）</span></span><br><span class="line">  <span class="type">float</span>* s_V = s_K + Bc * smem_stride;                <span class="comment">// Bc * smem_stride</span></span><br><span class="line">  <span class="comment">// 指向输出累积值 O tile 的共享内存起始地址（大小：Br * smem_stride * sizeof(float)）</span></span><br><span class="line">  <span class="type">float</span>* s_O = s_V + Bc * smem_stride;                <span class="comment">// Br * smem_stride</span></span><br><span class="line">  <span class="comment">// 指向每个 query 行的最大得分 max 值数组（大小：Br * sizeof(float)）</span></span><br><span class="line">  <span class="type">float</span>* s_m = s_O + Br * smem_stride;                <span class="comment">// Br (max scores)</span></span><br><span class="line">  <span class="comment">// 指向每个 query 行的 exp 和 sum 值数组（大小：Br * sizeof(float)）</span></span><br><span class="line">  <span class="type">float</span>* s_l = s_m + Br;                              <span class="comment">// Br (sum of exp)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将 2D 线程坐标转换为全局线程 ID（用于合并加载）</span></span><br><span class="line">  <span class="type">int</span> tid = threadIdx.y * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="comment">// 计算当前 block 内的总线程数</span></span><br><span class="line">  <span class="type">int</span> total_threads = blockDim.x * blockDim.y;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ============ 第1阶段：载入 Q tile 到共享内存 ============</span></span><br><span class="line">  <span class="comment">// 使用 grid-stride loop 让所有线程协作加载 Br * head_dim 个元素</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = tid; i &lt; Br * head_dim; i += total_threads) &#123;</span><br><span class="line">      <span class="comment">// 将线性索引 i 转换为二维坐标 (r, c)</span></span><br><span class="line">      <span class="type">int</span> r = i / head_dim;</span><br><span class="line">      <span class="comment">// 列索引 c（对应 head_dim 维度）</span></span><br><span class="line">      <span class="type">int</span> c = i % head_dim;</span><br><span class="line">      <span class="comment">// 计算全局的 query 行索引</span></span><br><span class="line">      <span class="type">int</span> global_q = q_start_idx + r;</span><br><span class="line">      <span class="comment">// 检查行和列是否在有效范围内，有效则从全局内存加载，否则填 0</span></span><br><span class="line">      <span class="keyword">if</span> (r &lt; q_len_local &amp;&amp; global_q &lt; target_seq_len) &#123;</span><br><span class="line">          <span class="comment">// 计算 Q 在全局内存中的线性索引：(batch, seq_pos, head, dim)</span></span><br><span class="line">          <span class="type">size_t</span> q_index = ((<span class="built_in">static_cast</span>&lt;<span class="type">size_t</span>&gt;(batch_idx) * target_seq_len + global_q) * query_heads + head_idx) * head_dim + c;</span><br><span class="line">          <span class="comment">// 从全局内存加载 Q 元素到共享内存（类型转换为 float）</span></span><br><span class="line">          s_Q[r * smem_stride + c] = <span class="built_in">to_float</span>(Q[q_index]);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 超出范围则填 0</span></span><br><span class="line">          s_Q[r * smem_stride + c] = <span class="number">0.0f</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 同时初始化输出累积值 O 为 0</span></span><br><span class="line">      s_O[r * smem_stride + c] = <span class="number">0.0f</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 只有 tx == 0 的线程初始化每个 query 行的 max 值和 sum 值</span></span><br><span class="line">  <span class="keyword">if</span> (tx == <span class="number">0</span> &amp;&amp; ty &lt; Br) &#123;</span><br><span class="line">    <span class="comment">// 初始化最大得分为极小值（用于第一次比较）</span></span><br><span class="line">    s_m[ty] = <span class="number">-1e20</span>f;</span><br><span class="line">    <span class="comment">// 初始化 exp 求和为 0</span></span><br><span class="line">    s_l[ty] = <span class="number">0.0f</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 等待所有线程完成 Q tile 加载和初始化</span></span><br><span class="line">  __syncthreads();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ============ 第2阶段：逐块（Tile）处理 K/V，实现 Online Softmax ============</span></span><br><span class="line">  <span class="comment">// 外层循环：每次处理一个 K/V tile（Bc 大小）</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> j_base = <span class="number">0</span>; j_base &lt; src_seq_len; j_base += Bc) &#123;</span><br><span class="line">    <span class="comment">// 计算当前 K/V tile 的实际长度（边界处理）</span></span><br><span class="line">    <span class="type">int</span> kv_len_local = <span class="built_in">min</span>(Bc, src_seq_len - j_base);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用 grid-stride loop 让所有线程协作加载 Bc * head_dim 个 K/V 元素</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = tid; i &lt; Bc * head_dim; i += total_threads) &#123;</span><br><span class="line">        <span class="comment">// 将线性索引转换为二维坐标</span></span><br><span class="line">        <span class="type">int</span> r = i / head_dim;</span><br><span class="line">        <span class="comment">// 列索引</span></span><br><span class="line">        <span class="type">int</span> c = i % head_dim;</span><br><span class="line">        <span class="comment">// 计算全局的 key 行索引</span></span><br><span class="line">        <span class="type">int</span> global_k = j_base + r;</span><br><span class="line">        <span class="comment">// 检查行和列是否在有效范围内</span></span><br><span class="line">        <span class="keyword">if</span> (r &lt; kv_len_local &amp;&amp; global_k &lt; src_seq_len) &#123;</span><br><span class="line">            <span class="comment">// 计算 K/V 在全局内存中的线性索引</span></span><br><span class="line">            <span class="type">size_t</span> k_index = ((<span class="built_in">static_cast</span>&lt;<span class="type">size_t</span>&gt;(batch_idx) * src_seq_len + global_k) * kv_heads + kv_head_idx) * head_dim + c;</span><br><span class="line">            <span class="comment">// 从全局内存加载 K 元素到共享内存</span></span><br><span class="line">            s_K[r * smem_stride + c] = <span class="built_in">to_float</span>(K[k_index]);</span><br><span class="line">            <span class="comment">// 从全局内存加载 V 元素到共享内存</span></span><br><span class="line">            s_V[r * smem_stride + c] = <span class="built_in">to_float</span>(V[k_index]);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 超出范围则填 0</span></span><br><span class="line">            s_K[r * smem_stride + c] = <span class="number">0.0f</span>;</span><br><span class="line">            <span class="comment">// V 也填 0</span></span><br><span class="line">            s_V[r * smem_stride + c] = <span class="number">0.0f</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 等待所有线程完成 K/V tile 加载</span></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ============ 第3阶段：对每个 query 行计算 attention 并更新 online softmax ============</span></span><br><span class="line">    <span class="comment">// 只有处理有效 query 行的线程执行此部分</span></span><br><span class="line">    <span class="keyword">if</span> (ty &lt; q_len_local) &#123;</span><br><span class="line">      <span class="comment">// 初始化当前线程对应的 Q-K 点积得分</span></span><br><span class="line">      <span class="type">float</span> score = <span class="number">0.0f</span>;</span><br><span class="line">      <span class="comment">// 检查当前线程对应的 K 列是否有效</span></span><br><span class="line">      <span class="type">bool</span> valid_k = (tx &lt; kv_len_local);</span><br><span class="line">      <span class="comment">// 计算全局 query 行索引（用于 causal mask 检查）</span></span><br><span class="line">      <span class="type">int</span> global_q_idx = q_start_idx + ty;</span><br><span class="line">      <span class="comment">// 计算全局 key 行索引（用于 causal mask 检查）</span></span><br><span class="line">      <span class="type">int</span> global_k_idx = j_base + tx;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Causal 掩码：只允许 query 关注当前及之前的 key</span></span><br><span class="line">      <span class="keyword">if</span> (is_causal &amp;&amp; global_k_idx &gt; global_q_idx) &#123;</span><br><span class="line">        <span class="comment">// 如果 key 行 &gt; query 行，则屏蔽此位置</span></span><br><span class="line">        valid_k = <span class="literal">false</span>;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 计算 Q[query_row] 与 K[key_row] 的缩放点积得分</span></span><br><span class="line">      <span class="keyword">if</span> (valid_k) &#123;</span><br><span class="line">        <span class="comment">// 逐维度计算点积</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> d = <span class="number">0</span>; d &lt; head_dim; ++d) &#123;</span><br><span class="line">          <span class="comment">// Q 行向量与 K 列向量的对应元素相乘并累加</span></span><br><span class="line">          score += s_Q[ty * smem_stride + d] * s_K[tx * smem_stride + d];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 应用缩放因子 1/sqrt(d_k)</span></span><br><span class="line">        score *= scale;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 如果不有效，设置为负无穷，softmax 后为 0</span></span><br><span class="line">        score = -INFINITY;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// ============ Warp 级规约：求 Warp 内的最大得分 ============</span></span><br><span class="line">      <span class="comment">// 获取当前 Warp 的活跃线程掩码</span></span><br><span class="line">      <span class="type">unsigned</span> mask = __activemask();</span><br><span class="line">      <span class="comment">// 初始化本地最大值为当前线程的得分</span></span><br><span class="line">      <span class="type">float</span> m_local = score;</span><br><span class="line">      <span class="comment">// 展开的循环：Warp 内规约（16 -&gt; 8 -&gt; 4 -&gt; 2 -&gt; 1）</span></span><br><span class="line">      <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">8</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="comment">// Warp 内规约：与偏移量为 offset 的线程进行 max 操作</span></span><br><span class="line">        m_local = <span class="built_in">fmaxf</span>(m_local, __shfl_xor_sync(mask, m_local, offset));</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 计算 p = exp(score - m_local)（数值稳定的 softmax ）</span></span><br><span class="line">      <span class="type">float</span> p = (score == -INFINITY) ? <span class="number">0.0f</span> : <span class="built_in">expf</span>(score - m_local);</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// ============ Warp 级规约：求 Warp 内 exp 的和 ============</span></span><br><span class="line">      <span class="comment">// 初始化本地 exp 和为 p</span></span><br><span class="line">      <span class="type">float</span> l_local = p;</span><br><span class="line">      <span class="comment">// 展开的循环：Warp 内规约求和</span></span><br><span class="line">      <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">8</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="comment">// Warp 内规约：与偏移量为 offset 的线程进行加法操作</span></span><br><span class="line">        l_local += __shfl_xor_sync(mask, l_local, offset);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// ============ Online Softmax 更新：结合前一个 K/V tile 的结果 ============</span></span><br><span class="line">      <span class="comment">// 从共享内存读取前一个 tile 的最大得分（该 query 行的全局最大值）</span></span><br><span class="line">      <span class="type">float</span> m_prev = s_m[ty];</span><br><span class="line">      <span class="comment">// 从共享内存读取前一个 tile 的 exp 求和（该 query 行的分母）</span></span><br><span class="line">      <span class="type">float</span> l_prev = s_l[ty];</span><br><span class="line">      <span class="comment">// 计算新的全局最大值（当前 tile 的最大值与全局最大值的 max）</span></span><br><span class="line">      <span class="type">float</span> m_new = <span class="built_in">fmaxf</span>(m_prev, m_local);</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// 计算前一个 tile 的贡献因子（归一化系数）</span></span><br><span class="line">      <span class="type">float</span> scale_prev = <span class="built_in">expf</span>(m_prev - m_new);</span><br><span class="line">      <span class="comment">// 计算当前 tile 的贡献因子（归一化系数）</span></span><br><span class="line">      <span class="type">float</span> scale_curr = <span class="built_in">expf</span>(m_local - m_new);</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// 更新累积的 exp 求和：l_new = l_prev * e^(m_prev-m_new) + l_local * e^(m_local-m_new)</span></span><br><span class="line">      <span class="type">float</span> l_new = l_prev * scale_prev + l_local * scale_curr;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 只有 tx == 0 的线程更新共享内存中的全局最大值和分母</span></span><br><span class="line">      <span class="keyword">if</span> (tx == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// 更新该 query 行的全局最大得分</span></span><br><span class="line">        s_m[ty] = m_new;</span><br><span class="line">        <span class="comment">// 更新该 query 行的累积 exp 求和</span></span><br><span class="line">        s_l[ty] = l_new;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// ============ 输出累积值更新，使用带 Padding 的共享内存步幅 ============</span></span><br><span class="line">      <span class="comment">// 逐维度更新输出向量 O</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> d = <span class="number">0</span>; d &lt; head_dim; ++d) &#123;</span><br><span class="line">        <span class="comment">// 获取当前线程对应的 V 元素（如果不有效则为 0）</span></span><br><span class="line">        <span class="type">float</span> v_val = valid_k ? s_V[tx * smem_stride + d] : <span class="number">0.0f</span>;</span><br><span class="line">        <span class="comment">// 计算加权值：p * V[key_row, d]</span></span><br><span class="line">        <span class="type">float</span> pd = p * v_val;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 展开的循环：Warp 内规约求和</span></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">8</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>) &#123;</span><br><span class="line">          <span class="comment">// Warp 内规约：将所有 lane 的 pd 相加</span></span><br><span class="line">          pd += __shfl_xor_sync(mask, pd, offset);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 只有 tx == 0 的线程更新共享内存中的输出累积值</span></span><br><span class="line">        <span class="keyword">if</span> (tx == <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="comment">// 读取该 query 行、维度 d 的之前累积值</span></span><br><span class="line">          <span class="type">float</span> o_val = s_O[ty * smem_stride + d];</span><br><span class="line">          <span class="comment">// 更新输出值：o_new = o_prev * scale_prev + pd * scale_curr（归一化处理）</span></span><br><span class="line">          s_O[ty * smem_stride + d] = o_val * scale_prev + pd * scale_curr;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 等待所有线程完成当前 K/V tile 的计算</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ============ 第4阶段：归一化并写回全局内存 ============</span></span><br><span class="line">  <span class="comment">// 只有处理有效 query 行的线程执行此部分</span></span><br><span class="line">  <span class="keyword">if</span> (ty &lt; q_len_local) &#123;</span><br><span class="line">    <span class="comment">// 读取该 query 行的累积 exp 求和（softmax 分母）</span></span><br><span class="line">    <span class="type">float</span> denom = s_l[ty];</span><br><span class="line">    <span class="comment">// 计算分母的倒数，防止除 0</span></span><br><span class="line">    <span class="type">float</span> inv_l = (denom &gt; <span class="number">0.0f</span>) ? (<span class="number">1.0f</span> / denom) : <span class="number">0.0f</span>;</span><br><span class="line">    <span class="comment">// 使用 grid-stride loop 由多个线程写回输出</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> d = tx; d &lt; head_dim; d += Bc) &#123;</span><br><span class="line">      <span class="comment">// 读取共享内存中的累积输出值（带 Padding 的步幅）</span></span><br><span class="line">      <span class="type">float</span> val = s_O[ty * smem_stride + d] * inv_l;</span><br><span class="line">      <span class="comment">// 计算全局 query 行索引</span></span><br><span class="line">      <span class="type">int</span> global_q = q_start_idx + ty;</span><br><span class="line">      <span class="comment">// 计算输出在全局内存中的线性索引：(batch, seq_pos, head, dim)</span></span><br><span class="line">      <span class="type">size_t</span> o_index = ((<span class="built_in">static_cast</span>&lt;<span class="type">size_t</span>&gt;(batch_idx) * target_seq_len + global_q) * query_heads + head_idx) * head_dim + d;</span><br><span class="line">      <span class="comment">// 将归一化后的输出写回全局内存（类型转换回原类型 T）</span></span><br><span class="line">      O[o_index] = <span class="built_in">from_float</span>&lt;T&gt;(val);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="Host-端-Flash-Attention-函数"><a href="#Host-端-Flash-Attention-函数" class="headerlink" title="Host 端 Flash Attention 函数"></a>Host 端 Flash Attention 函数</h3><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">flashAttention</span><span class="params">(<span class="type">const</span> std::vector&lt;T&gt;&amp; h_q, <span class="type">const</span> std::vector&lt;T&gt;&amp; h_k,</span></span></span><br><span class="line"><span class="params"><span class="function">                    <span class="type">const</span> std::vector&lt;T&gt;&amp; h_v, std::vector&lt;T&gt;&amp; h_o,</span></span></span><br><span class="line"><span class="params"><span class="function">                    <span class="type">int</span> batch_size, <span class="type">int</span> target_seq_len, <span class="type">int</span> src_seq_len, </span></span></span><br><span class="line"><span class="params"><span class="function">                    <span class="type">int</span> query_heads, <span class="type">int</span> kv_heads, <span class="type">int</span> head_dim, <span class="type">bool</span> is_causal)</span> </span>&#123;       </span><br><span class="line">  <span class="keyword">if</span> (batch_size &lt;= <span class="number">0</span> || target_seq_len &lt;= <span class="number">0</span> || src_seq_len &lt;= <span class="number">0</span> || </span><br><span class="line">      query_heads &lt;= <span class="number">0</span> || kv_heads &lt;= <span class="number">0</span> || head_dim &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">    h_o.<span class="built_in">clear</span>();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">const</span> <span class="type">size_t</span> q_elems = batch_size * target_seq_len * query_heads * head_dim;</span><br><span class="line">  <span class="type">const</span> <span class="type">size_t</span> k_elems = batch_size * src_seq_len * kv_heads * head_dim;</span><br><span class="line">  <span class="type">const</span> <span class="type">size_t</span> v_elems = k_elems;</span><br><span class="line">  <span class="type">const</span> <span class="type">size_t</span> o_elems = q_elems;</span><br><span class="line"></span><br><span class="line">  h_o.<span class="built_in">resize</span>(o_elems);</span><br><span class="line"></span><br><span class="line">  T* d_q = <span class="literal">nullptr</span>;</span><br><span class="line">  T* d_k = <span class="literal">nullptr</span>;</span><br><span class="line">  T* d_v = <span class="literal">nullptr</span>;</span><br><span class="line">  T* d_o = <span class="literal">nullptr</span>;</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;d_q, q_elems * <span class="built_in">sizeof</span>(T)));</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;d_k, k_elems * <span class="built_in">sizeof</span>(T)));</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;d_v, v_elems * <span class="built_in">sizeof</span>(T)));</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;d_o, o_elems * <span class="built_in">sizeof</span>(T)));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Host -&gt; Device: 拷贝 Q/K/V 到 GPU，并清空输出</span></span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMemcpy</span>(d_q, h_q.<span class="built_in">data</span>(), q_elems * <span class="built_in">sizeof</span>(T), cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMemcpy</span>(d_k, h_k.<span class="built_in">data</span>(), k_elems * <span class="built_in">sizeof</span>(T), cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMemcpy</span>(d_v, h_v.<span class="built_in">data</span>(), v_elems * <span class="built_in">sizeof</span>(T), cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMemset</span>(d_o, <span class="number">0</span>, o_elems * <span class="built_in">sizeof</span>(T)));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 线程块: (Bc, Br) 形成 [列, 行] 的 tile 计算</span></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(Bc, Br)</span></span>;</span><br><span class="line">  <span class="type">int</span> grid_x = (target_seq_len + Br - <span class="number">1</span>) / Br;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(grid_x, query_heads, batch_size)</span></span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// SMEM Padding: +4 (16字节) 以彻底消除 Bank Conflict</span></span><br><span class="line">  <span class="type">int</span> smem_stride = head_dim + <span class="number">4</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 共享内存大小与 kernel 中的 smem 布局保持一致</span></span><br><span class="line">  <span class="comment">// 布局: s_Q (Br*stride) + s_K (Bc*stride) + s_V (Bc*stride) + s_O (Br*stride) + s_m (Br) + s_l (Br)</span></span><br><span class="line">  <span class="type">size_t</span> shared_bytes = (Br * smem_stride + Bc * smem_stride + Bc * smem_stride + </span><br><span class="line">                         Br * smem_stride + Br + Br) * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">  <span class="type">float</span> scale = <span class="number">1.0f</span> / <span class="built_in">sqrtf</span>(<span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(head_dim));</span><br><span class="line">  </span><br><span class="line">  flash_attention_v1_kernel&lt;T&gt;&lt;&lt;&lt;grid, block, shared_bytes&gt;&gt;&gt;(</span><br><span class="line">    d_q, d_k, d_v, d_o,</span><br><span class="line">    batch_size, target_seq_len, src_seq_len,</span><br><span class="line">    query_heads, kv_heads, head_dim, smem_stride, is_causal, scale);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaGetLastError</span>());</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaDeviceSynchronize</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Device -&gt; Host: 拷回输出结果</span></span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMemcpy</span>(h_o.<span class="built_in">data</span>(), d_o, o_elems * <span class="built_in">sizeof</span>(T), cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaFree</span>(d_q));</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaFree</span>(d_k));</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaFree</span>(d_v));</span><br><span class="line">  <span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaFree</span>(d_o));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<hr>
<h2 id="更新日志"><a href="#更新日志" class="headerlink" title="更新日志"></a>更新日志</h2><h3 id="Flash-Attention-优化升级"><a href="#Flash-Attention-优化升级" class="headerlink" title="Flash Attention 优化升级"></a>Flash Attention 优化升级</h3><p><strong>主要改进</strong>：</p>
<ol>
<li><p><strong>Tiling 策略</strong></p>
<ul>
<li>Query 按 Br×head_dim 分块（Br&#x3D;16）</li>
<li>Key&#x2F;Value 按 Bc×head_dim 分块（Bc&#x3D;16）</li>
<li>减少全局内存访问，增加局部数据重用</li>
</ul>
</li>
<li><p><strong>线程块布局</strong></p>
<ul>
<li>从 1D 改为 2D：<code>(Bc, Br)</code> 形状</li>
<li>x 维处理 K&#x2F;V 的列（head_dim）</li>
<li>y 维处理 Q 的行（Br）</li>
</ul>
</li>
<li><p><strong>Bank Conflict 消除</strong></p>
<ul>
<li>共享内存使用 <code>smem_stride = head_dim + 4</code></li>
<li>避免多个线程同时访问同一 bank</li>
</ul>
</li>
<li><p><strong>Online Softmax 实现</strong></p>
<ul>
<li>每个 tile 进行一次 softmax 更新</li>
<li>使用归一化公式：$m_{new} &#x3D; \max(m_{prev}, m_{local})$</li>
<li>$l_{new} &#x3D; l_{prev} \cdot e^{m_{prev} - m_{new}} + l_{local} \cdot e^{m_{local} - m_{new}}$</li>
</ul>
</li>
<li><p><strong>Warp 级规约</strong></p>
<ul>
<li>使用 <code>__shfl_xor_sync</code> 替代 shared memory 规约</li>
<li>更高效的跨 lane 通信</li>
</ul>
</li>
<li><p><strong>Grid 配置</strong></p>
<ul>
<li>Grid: <code>(grid_x, query_heads, batch_size)</code></li>
<li>blockIdx.x 对应 Q block，blockIdx.y 对应 head，blockIdx.z 对应 batch</li>
</ul>
</li>
</ol>
<h2 id="7-显式模板实例化"><a href="#7-显式模板实例化" class="headerlink" title="7. 显式模板实例化"></a>7. 显式模板实例化</h2><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// REQUIRED FOR LINKING WITH TESTER.O</span></span><br><span class="line"><span class="comment">// DO NOT MODIFY THIS SECTION</span></span><br><span class="line"><span class="keyword">template</span> <span class="type">int</span> <span class="built_in">trace</span>&lt;<span class="type">int</span>&gt;(<span class="type">const</span> std::vector&lt;<span class="type">int</span>&gt;&amp;, <span class="type">size_t</span>, <span class="type">size_t</span>);</span><br><span class="line"><span class="keyword">template</span> <span class="type">float</span> <span class="built_in">trace</span>&lt;<span class="type">float</span>&gt;(<span class="type">const</span> std::vector&lt;<span class="type">float</span>&gt;&amp;, <span class="type">size_t</span>, <span class="type">size_t</span>);</span><br><span class="line"><span class="keyword">template</span> <span class="type">void</span> <span class="built_in">flashAttention</span>&lt;<span class="type">float</span>&gt;(<span class="type">const</span> std::vector&lt;<span class="type">float</span>&gt;&amp;, <span class="type">const</span> std::vector&lt;<span class="type">float</span>&gt;&amp;,</span><br><span class="line">  <span class="type">const</span> std::vector&lt;<span class="type">float</span>&gt;&amp;, std::vector&lt;<span class="type">float</span>&gt;&amp;,</span><br><span class="line">  <span class="type">int</span>, <span class="type">int</span>, <span class="type">int</span>, <span class="type">int</span>, <span class="type">int</span>, <span class="type">int</span>, <span class="type">bool</span>);</span><br><span class="line"><span class="keyword">template</span> <span class="type">void</span> <span class="built_in">flashAttention</span>&lt;half&gt;(<span class="type">const</span> std::vector&lt;half&gt;&amp;, <span class="type">const</span> std::vector&lt;half&gt;&amp;,</span><br><span class="line">  <span class="type">const</span> std::vector&lt;half&gt;&amp;, std::vector&lt;half&gt;&amp;,</span><br><span class="line">  <span class="type">int</span>, <span class="type">int</span>, <span class="type">int</span>, <span class="type">int</span>, <span class="type">int</span>, <span class="type">int</span>, <span class="type">bool</span>);</span><br></pre></td></tr></table></figure></div>

<h2 id="8-极致性能优化：与第一对比"><a href="#8-极致性能优化：与第一对比" class="headerlink" title="8. 极致性能优化：与第一对比"></a>8. 极致性能优化：与第一对比</h2><p>在参考了其他优秀的开源实现（如 <code>forlearn/Learning-CUDA/src/kernels.cu</code>）后，我发现我的 V1 版本虽然实现了基本的分块和 Shared Memory 优化，但在工程细节和极致性能压榨上还有很大的提升空间。以下是对该高性能版本中 5 个核心优化点的深度剖析，通过“我的写法”与“人家的写法”的直观对比，总结其性能差异。</p>
<h3 id="8-1-内存分配与释放的极致优化：合并与异步"><a href="#8-1-内存分配与释放的极致优化：合并与异步" class="headerlink" title="8.1 内存分配与释放的极致优化：合并与异步"></a>8.1 内存分配与释放的极致优化：合并与异步</h3><p>在原版实现中，我们为 Q、K、V、O 分别调用了四次 <code>cudaMalloc</code> 和 <code>cudaFree</code>。由于 GPU 内存分配是同步且开销极大的操作，这会严重拖慢整体执行速度。</p>
<p><strong>我的写法（多次同步分配与释放）</strong>：</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">T* d_q, *d_k, *d_v, *d_o;</span><br><span class="line"><span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;d_q, q_elems * <span class="built_in">sizeof</span>(T)));</span><br><span class="line"><span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;d_k, k_elems * <span class="built_in">sizeof</span>(T)));</span><br><span class="line"><span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;d_v, v_elems * <span class="built_in">sizeof</span>(T)));</span><br><span class="line"><span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;d_o, o_elems * <span class="built_in">sizeof</span>(T)));</span><br><span class="line"><span class="comment">// ... 同步拷贝与计算 ...</span></span><br><span class="line"><span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaFree</span>(d_q));</span><br><span class="line"><span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaFree</span>(d_k));</span><br><span class="line"><span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaFree</span>(d_v));</span><br><span class="line"><span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaFree</span>(d_o));</span><br></pre></td></tr></table></figure></div>

<p><strong>人家的写法（单次异步分配，切片使用）</strong>：<br>计算出总共需要的内存大小，一次性分配一整块连续的 Device 内存，然后通过指针偏移（切片）给 Q、K、V、O 使用。同时使用 <code>cudaMallocAsync</code> 和 <code>cudaFreeAsync</code>，并绑定到具有高优先级的独立非阻塞 Stream 上，减少与默认 Stream 的同步开销。</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 创建高优先级非阻塞 Stream</span></span><br><span class="line"><span class="built_in">cudaStreamCreateWithPriority</span>(&amp;stream2, cudaStreamNonBlocking, <span class="number">-1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 计算总字节数</span></span><br><span class="line"><span class="type">const</span> <span class="type">size_t</span> total_bytes = size_bytes_q + size_bytes_k + size_bytes_v + size_bytes_o;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 一次性异步分配</span></span><br><span class="line">T* d_all = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMallocAsync</span>(&amp;d_all, total_bytes, stream2));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. 指针偏移切片</span></span><br><span class="line">T *d_q = d_all;</span><br><span class="line">T *d_k = d_q + h_q.<span class="built_in">size</span>();</span><br><span class="line">T *d_v = d_k + h_k.<span class="built_in">size</span>();</span><br><span class="line">T *d_o = d_v + h_v.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5. 异步拷贝与释放</span></span><br><span class="line"><span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(d_q, h_q.<span class="built_in">data</span>(), size_bytes_q, cudaMemcpyHostToDevice, stream2));</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaFreeAsync</span>(d_all, stream2));</span><br></pre></td></tr></table></figure></div>

<h3 id="8-2-算法层面的-I-x2F-O-优化：仅加载有效数据"><a href="#8-2-算法层面的-I-x2F-O-优化：仅加载有效数据" class="headerlink" title="8.2 算法层面的 I&#x2F;O 优化：仅加载有效数据"></a>8.2 算法层面的 I&#x2F;O 优化：仅加载有效数据</h3><p>在计算矩阵的迹（Trace）时，原版代码将整个 $N \times N$ 的矩阵拷贝到了 GPU，然后在 Kernel 中通过 <code>i * cols + i</code> 提取对角线元素。这导致了 $O(N^2)$ 的数据传输，而实际参与计算的只有 $O(N)$ 的数据。</p>
<p><strong>我的写法（全量拷贝，跨步访问）</strong>：</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Host 端：全量拷贝 N x N 矩阵</span></span><br><span class="line"><span class="built_in">RUNTIME_CHECK</span>(<span class="built_in">cudaMemcpy</span>(d_input, h_input.<span class="built_in">data</span>(), total_elems * <span class="built_in">sizeof</span>(T), cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Kernel 端：跨步访问，访存效率极低</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">size_t</span> i = idx; i &lt; n; i += stride) &#123;</span><br><span class="line">    local_sum += input[i * cols + i]; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><strong>人家的写法（按需拷贝，连续访问）</strong>：<br>在 Host 端预先提取对角线元素，仅将有效数据传输至 Device 端。这不仅大幅降低了 PCIe 带宽压力，还使得 Kernel 中的内存访问变成了完全连续的合并访问（Coalesced Memory Access）。</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Host 端：提取对角元素，仅拷贝 O(N) 数据</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">std::vector&lt;T&gt; <span class="title">extract_diag</span><span class="params">(<span class="type">const</span> std::vector&lt;T&gt; &amp; h_input, <span class="type">size_t</span> rows, <span class="type">size_t</span> cols)</span></span>&#123;</span><br><span class="line">  <span class="type">size_t</span> n = std::<span class="built_in">min</span>(rows, cols);</span><br><span class="line">  <span class="function">std::vector&lt;T&gt; <span class="title">diag</span><span class="params">(n)</span></span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; n; ++i)&#123;</span><br><span class="line">    diag[i] = h_input[i * cols + i];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> diag;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Kernel 端：变为连续访问，极致的访存效率</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">trace_calc</span><span class="params">(T* d_trace, <span class="type">const</span> T* d_diag, <span class="type">size_t</span> n)</span></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">size_t</span> i = idx; i &lt; n; i += stride)&#123;</span><br><span class="line">    sum += d_diag[i]; </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="8-3-循环展开与分支预测优化"><a href="#8-3-循环展开与分支预测优化" class="headerlink" title="8.3 循环展开与分支预测优化"></a>8.3 循环展开与分支预测优化</h3><p>在 GPU 编程中，分支（Branching）和循环控制开销会破坏指令流水线。</p>
<p><strong>我的写法（常规循环与内层条件判断）</strong>：</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 常规循环，存在循环变量更新开销</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> offset = warpSize / <span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>) &#123;</span><br><span class="line">    val += __shfl_down_sync(<span class="number">0xffffffff</span>, val, offset);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Kernel 内层循环中频繁进行复杂的 Causal Mask 判断</span></span><br><span class="line"><span class="keyword">if</span> (is_causal &amp;&amp; global_k_idx &gt; global_q_idx) &#123;</span><br><span class="line">    valid_k = <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><strong>人家的写法（强制展开与分支前置）</strong>：<br>对已知迭代次数的短循环进行强制展开；<br>#pragam unroll 指令告诉编译器将循环完全展开，消除循环控制开销和分支预测的影响。<br>编译后直接变成：<br>val +&#x3D; __shfl_down_sync(…, 16);<br>val +&#x3D; __shfl_down_sync(…, 8);<br>val +&#x3D; __shfl_down_sync(…, 4);<br>val +&#x3D; __shfl_down_sync(…, 2);<br>val +&#x3D; __shfl_down_sync(…, 1);</p>
<p>将复杂的 Causal Mask 判断逻辑提前计算并转换为布尔标志，避免在内层循环中反复进行复杂的条件判断。</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ T <span class="title">warp_reduce_sum</span><span class="params">(T val)</span></span>&#123;</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll <span class="comment">// 短循环自动展开，省去分支预测，提升效率</span></span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>)&#123;</span><br><span class="line">        val += __shfl_down_sync(<span class="number">0xffffffff</span>, val, offset);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> val;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Flash Attention 中的分支优化：提前计算当前 Tile 是否需要被 Mask 掉</span></span><br><span class="line"><span class="type">bool</span> is_compute = <span class="literal">true</span>; <span class="comment">// 分支处理，加速 branch-resolving</span></span><br><span class="line"><span class="keyword">if</span> (is_causal) &#123;</span><br><span class="line">    <span class="comment">// 提前在循环外处理 branch-resolving 逻辑</span></span><br><span class="line">    <span class="comment">// 是否causal对一个tile的所有线程都是一样的，所以只需要计算一次</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="8-4-Shared-Memory-资源复用"><a href="#8-4-Shared-Memory-资源复用" class="headerlink" title="8.4 Shared Memory 资源复用"></a>8.4 Shared Memory 资源复用</h3><p>Shared Memory（SMEM）是 SM 中极其宝贵且有限的资源。在 Flash Attention 中，我们需要存储 $S &#x3D; Q \times K^T$ 的结果，随后计算 $P &#x3D; \text{softmax}(S)$。</p>
<p><strong>我的写法（独立分配）</strong>：<br>为不同阶段的变量分配独立的 Shared Memory，如果中间矩阵较多，极易导致 SMEM 耗尽，降低 Thread Block 的 Occupancy（占用率）。</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span>* s_Q = smem;                                  </span><br><span class="line"><span class="type">float</span>* s_K = s_Q + Br * smem_stride;                </span><br><span class="line"><span class="type">float</span>* s_V = s_K + Bc * smem_stride;                </span><br><span class="line"><span class="comment">// 如果要存 S 和 P，还需要额外开辟空间</span></span><br></pre></td></tr></table></figure></div>

<p><strong>人家的写法（生命周期错开的变量直接复用）</strong>：<br>由于 $S$ 矩阵在计算出 $P$ 之后就不再被需要，直接让 $P$ 覆盖 $S$ 的内存空间。通过指针复用，节省了一半的中间矩阵 SMEM 占用。</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> __shared__ <span class="type">char</span> shared_mem[];</span><br><span class="line"><span class="type">char</span>* ptr = shared_mem;  </span><br><span class="line"></span><br><span class="line"><span class="comment">// 复用 S 和 P 矩阵的内存空间</span></span><br><span class="line"><span class="type">double</span>* SP = <span class="built_in">reinterpret_cast</span>&lt;<span class="type">double</span>*&gt;(ptr); <span class="comment">// double SP[Br][Bc]</span></span><br><span class="line">ptr += Br * Bc * <span class="built_in">sizeof</span>(<span class="type">double</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义访问宏，统一接口</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SP_AT(y, x) SP[y * Bc + x]</span></span><br></pre></td></tr></table></figure></div>

<h3 id="8-5-关键步骤的精度保护-Double-Precision"><a href="#8-5-关键步骤的精度保护-Double-Precision" class="headerlink" title="8.5 关键步骤的精度保护 (Double Precision)"></a>8.5 关键步骤的精度保护 (Double Precision)</h3><p>在 Flash Attention 的 Online Softmax 计算中，涉及到指数运算 <code>exp(S - m)</code> 和累加求和。如果使用 <code>float</code> 甚至 <code>half</code>，在序列较长或数值差异较大时，极易发生精度溢出或下溢（Underflow）。</p>
<p><strong>我的写法（全程单精度）</strong>：</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 全程使用 float 进行计算</span></span><br><span class="line"><span class="type">const</span> <span class="type">float</span> scale = <span class="number">1.0f</span> / <span class="built_in">sqrtf</span>(<span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(head_dim));</span><br><span class="line"><span class="type">float</span> score = <span class="number">0.0f</span>;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="type">float</span> p = (score == -INFINITY) ? <span class="number">0.0f</span> : <span class="built_in">expf</span>(score - m_local);</span><br></pre></td></tr></table></figure></div>

<p><strong>人家的写法（关键路径双精度护航）</strong>：<br>在计算 $S$ 矩阵、$P$ 矩阵以及缩放因子 <code>scale_factor</code> 时，强制提升至 <code>double</code> 精度进行中间计算，最后再向下转换为目标类型。这在不显著增加计算时间的前提下，极大地保护了数值稳定性。</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 预计算常量，保留精度，采用 double</span></span><br><span class="line"><span class="type">const</span> <span class="type">double</span> scale_factor = <span class="number">1.0</span> / <span class="built_in">sqrt</span>(<span class="built_in">double</span>(head_dim));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 中间变量 SP 采用 double</span></span><br><span class="line"><span class="type">double</span>* SP = <span class="built_in">reinterpret_cast</span>&lt;<span class="type">double</span>*&gt;(ptr); </span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="comment">// 计算 S = Q @ K.T 时，累加器使用 double 保护数值稳定性</span></span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></table></figure></div>

<h3 id="8-6-Shared-Memory-访存模式优化：K-矩阵转置消除-Bank-Conflict"><a href="#8-6-Shared-Memory-访存模式优化：K-矩阵转置消除-Bank-Conflict" class="headerlink" title="8.6 Shared Memory 访存模式优化：K 矩阵转置消除 Bank Conflict"></a>8.6 Shared Memory 访存模式优化：K 矩阵转置消除 Bank Conflict</h3><p>在计算 $S &#x3D; Q \times K^T$ 时，需要从 Shared Memory 中读取 Q 和 K 的数据。</p>
<p><strong>我的写法（依赖 Padding 缓解冲突）</strong>：</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// K 矩阵按原布局加载：s_K[Bc][head_dim]</span></span><br><span class="line"><span class="type">float</span>* s_K = s_Q + Br * smem_stride; </span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算点积时，同一 Warp 内的线程（tx 不同）访问 s_K 的同一列</span></span><br><span class="line"><span class="comment">// 导致跨步访问（Strided Access），只能依赖 smem_stride = head_dim + 4 来缓解 Bank Conflict</span></span><br><span class="line">score += s_Q[ty * smem_stride + d] * s_K[tx * smem_stride + d];</span><br></pre></td></tr></table></figure></div>

<p><strong>人家的写法（加载时转置，实现连续访存）</strong>：<br>在将 K 矩阵从 Global Memory 加载到 Shared Memory 时，直接将其转置存储为 <code>K_T_sm[head_dim][Bc]</code>。这样在计算点积时，同一 Warp 内的线程（<code>tx</code> 不同）访问的是内存中连续的地址，从根本上消除了 Bank Conflict，达到了极致的访存效率。</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// K 矩阵在 Shared Memory 中直接转置存储：K_T_sm[head_dim][Bc]</span></span><br><span class="line"><span class="type">float</span>* K_T_sm = <span class="built_in">reinterpret_cast</span>&lt;<span class="type">float</span>*&gt;(ptr);  </span><br><span class="line"><span class="meta">#<span class="keyword">define</span> K_T_sm_AT(y, x) K_T_sm[y * Bc + x]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ... 加载时进行转置 ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算点积时，同一 Warp 内的线程（tx 不同）访问连续地址 K_T_sm_AT(d, tx)</span></span><br><span class="line"><span class="comment">// 完美实现 Coalesced Memory Access，零 Bank Conflict</span></span><br></pre></td></tr></table></figure></div>

<h3 id="8-7-跨平台兼容与软硬件协同：软件模拟双精度-FP32x2"><a href="#8-7-跨平台兼容与软硬件协同：软件模拟双精度-FP32x2" class="headerlink" title="8.7 跨平台兼容与软硬件协同：软件模拟双精度 (FP32x2)"></a>8.7 跨平台兼容与软硬件协同：软件模拟双精度 (FP32x2)</h3><p>在某些国产 GPU 平台（如 Iluvatar）或消费级显卡上，硬件原生的双精度（FP64）计算单元可能非常少，导致使用 <code>double</code> 会造成严重的性能瓶颈。</p>
<p><strong>我的写法（无视硬件差异）</strong>：<br>没有考虑不同硬件平台的特性，直接使用标准的 <code>float</code> 或 <code>double</code>，在 FP64 性能孱弱的显卡上会遭遇断崖式掉速。</p>
<p><strong>人家的写法（软件模拟双精度）</strong>：<br>针对特定平台（<code>#ifdef PLATFORM_ILUVATAR</code>），实现了一个 <code>myDouble</code> 类。利用两个 <code>float</code>（<code>_hi</code> 和 <code>_lo</code>）以及 FMA 指令（<code>__fmaf_rn</code>）在软件层面模拟双精度计算。这在保证 Online Softmax 数值稳定性的同时，完全避开了硬件 FP64 性能孱弱的瓶颈，是极其硬核的极致压榨。</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">ifdef</span> PLATFORM_ILUVATAR</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">myDouble</span>&#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">float</span> _hi; <span class="comment">// 高位</span></span><br><span class="line">    <span class="type">float</span> _lo; <span class="comment">// 低位（残差）</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 利用 FMA 指令捕捉乘法残差，实现软件层面的双精度</span></span><br><span class="line">    __host__ __device__</span><br><span class="line">    myDouble <span class="keyword">operator</span>*(<span class="type">const</span> <span class="type">float</span> op) <span class="type">const</span> &#123;</span><br><span class="line">        <span class="type">float</span> p_hi = _hi * op;</span><br><span class="line">        <span class="type">float</span> p_lo = __fmaf_rn(_hi, op, -p_hi); <span class="comment">// 捕捉 hi 乘法的剩余误差</span></span><br><span class="line">        p_lo += (_lo * op);                    <span class="comment">// 累加 lo 部分的乘积</span></span><br><span class="line">        <span class="type">float</span> final_hi = p_hi + p_lo;</span><br><span class="line">        <span class="type">float</span> final_lo = p_lo - (final_hi - p_hi);</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">myDouble</span>(final_hi, final_lo);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br></pre></td></tr></table></figure></div>

<h3 id="8-8-常量计算的查表法-LUT-优化"><a href="#8-8-常量计算的查表法-LUT-优化" class="headerlink" title="8.8 常量计算的查表法 (LUT) 优化"></a>8.8 常量计算的查表法 (LUT) 优化</h3><p>在计算 Attention Score 的缩放因子时，需要用到 <code>1.0 / sqrt(head_dim)</code>。</p>
<p><strong>我的写法（运行时计算）</strong>：<br>在 Kernel 启动前或 Kernel 内部，调用浮点开方和除法指令进行计算。</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">float</span> scale = <span class="number">1.0f</span> / <span class="built_in">sqrtf</span>(<span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(head_dim));</span><br></pre></td></tr></table></figure></div>

<p><strong>人家的写法（编译期&#x2F;设备端查表）</strong>：<br>针对常见的 <code>head_dim</code>（如 32, 64），直接硬编码预计算好的高精度双浮点结果（<code>mylut</code>），用查表法（Lookup Table）替代了昂贵的浮点开方和除法指令。</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__device__ myDouble <span class="title">mylut</span><span class="params">(<span class="type">int</span> head_dim)</span></span>&#123;</span><br><span class="line">    <span class="keyword">switch</span>(head_dim)&#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">32</span>:<span class="keyword">return</span> <span class="built_in">myDouble</span>(<span class="number">0.176776695f</span>, <span class="number">2.96636886e-10</span>f); <span class="comment">// 1/sqrt(32)</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">64</span>:<span class="keyword">return</span> <span class="built_in">myDouble</span>(<span class="number">0.125f</span>, <span class="number">0.0f</span>);                  <span class="comment">// 1/sqrt(64)</span></span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">        <span class="keyword">default</span>:<span class="keyword">return</span> <span class="built_in">myDouble</span>(<span class="number">0.0f</span>, <span class="number">0.0f</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Kernel 中直接查表获取高精度 scale_factor</span></span><br><span class="line"><span class="type">const</span> myDouble scale_factor = <span class="built_in">mylut</span>(head_dim);</span><br></pre></td></tr></table></figure></div>

<h3 id="8-9-国产平台适配：C-标准与编译器特性的妥协"><a href="#8-9-国产平台适配：C-标准与编译器特性的妥协" class="headerlink" title="8.9 国产平台适配：C++ 标准与编译器特性的妥协"></a>8.9 国产平台适配：C++ 标准与编译器特性的妥协</h3><p>在将代码移植到不同的国产 GPU 平台（如 Moore 摩尔线程）时，编译器的支持程度往往参差不齐。</p>
<p><strong>我的写法（过度依赖现代 C++ 特性）</strong>：<br>在实现 <code>myexp</code> 函数时，使用了 C++17 的 <code>if constexpr</code> 来进行编译期类型分支判断。这在 NVCC 或较新的编译器上运行良好，但在某些仅支持 C++11 的国产平台编译器（如 Moore 平台的 <code>musa</code> 编译器）上会直接导致编译失败。</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ T <span class="title">myexp</span><span class="params">(T x)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 致命错误：Moore 平台编译器不支持 C++17 的 if constexpr</span></span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span><span class="params">(std::is_same&lt;T, __half&gt;::value)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><strong>人家的写法（回归基础，利用模板特化）</strong>：<br>为了保证最大的跨平台兼容性，放弃了 <code>if constexpr</code>，转而使用最基础的 C++98&#x2F;11 模板特化（Template Specialization）或函数重载来实现不同类型的分支逻辑。</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 兼容所有平台的写法：使用模板特化或重载</span></span><br><span class="line"><span class="function">__device__ __forceinline__ <span class="type">float</span> <span class="title">myexp</span><span class="params">(<span class="type">float</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">expf</span>(x);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ __forceinline__ <span class="type">double</span> <span class="title">myexp</span><span class="params">(<span class="type">double</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">exp</span>(x);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ __forceinline__ half <span class="title">myexp</span><span class="params">(half x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> __float2half(<span class="built_in">expf</span>(__half2float(x)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="8-10-国产平台适配：从-CUDA-到-MUSA-Moore-与-MACA-沐曦-的迁移指南"><a href="#8-10-国产平台适配：从-CUDA-到-MUSA-Moore-与-MACA-沐曦-的迁移指南" class="headerlink" title="8.10 国产平台适配：从 CUDA 到 MUSA (Moore) 与 MACA (沐曦) 的迁移指南"></a>8.10 国产平台适配：从 CUDA 到 MUSA (Moore) 与 MACA (沐曦) 的迁移指南</h3><p>在将 CUDA 代码迁移到国产 GPU 平台（如摩尔线程的 MUSA 和沐曦的 MACA）时，除了上述提到的 C++ 标准兼容性问题，最核心的工作是 API 的替换与硬件特性的适配。</p>
<h4 id="1-API-命名空间的无缝替换"><a href="#1-API-命名空间的无缝替换" class="headerlink" title="1. API 命名空间的无缝替换"></a>1. API 命名空间的无缝替换</h4><p>国产平台通常提供了与 CUDA 高度兼容的 API 接口，迁移的第一步是进行全局的命名空间替换。</p>
<p>**CUDA 到 MUSA (摩尔线程 Moore)**：</p>
<ul>
<li>头文件：<code>#include &lt;cuda_fp16.h&gt;</code> $\rightarrow$ <code>#include &lt;musa_fp16.h&gt;</code></li>
<li>内存管理：<code>cudaMalloc</code> $\rightarrow$ <code>musaMalloc</code>，<code>cudaFree</code> $\rightarrow$ <code>musaFree</code></li>
<li>数据拷贝：<code>cudaMemcpy</code> $\rightarrow$ <code>musaMemcpy</code>，<code>cudaMemcpyHostToDevice</code> $\rightarrow$ <code>musaMemcpyHostToDevice</code></li>
<li>错误检查：<code>cudaGetLastError</code> $\rightarrow$ <code>musaGetLastError</code></li>
<li>设备同步：<code>cudaDeviceSynchronize</code> $\rightarrow$ <code>musaDeviceSynchronize</code></li>
</ul>
<p>**CUDA 到 MACA (沐曦)**：</p>
<ul>
<li>头文件：<code>#include &lt;cuda_fp16.h&gt;</code> $\rightarrow$ <code>#include &lt;common/maca_fp16.h&gt;</code></li>
<li>内存管理：<code>cudaMalloc</code> $\rightarrow$ <code>mcMalloc</code>，<code>cudaFree</code> $\rightarrow$ <code>mcFree</code></li>
<li>数据拷贝：<code>cudaMemcpy</code> $\rightarrow$ <code>mcMemcpy</code>，<code>cudaMemcpyHostToDevice</code> $\rightarrow$ <code>mcMemcpyHostToDevice</code></li>
<li>错误检查：<code>cudaGetLastError</code> $\rightarrow$ <code>mcGetLastError</code></li>
<li>设备同步：<code>cudaDeviceSynchronize</code> $\rightarrow$ <code>mcDeviceSynchronize</code></li>
</ul>
<h4 id="2-硬件架构参数的微调"><a href="#2-硬件架构参数的微调" class="headerlink" title="2. 硬件架构参数的微调"></a>2. 硬件架构参数的微调</h4><p>不同平台的 SM（Streaming Multiprocessor）架构和资源限制不同，需要针对性地调整 Kernel 的启动参数。</p>
<ul>
<li><p><strong>Warp Size 差异</strong>：</p>
<ul>
<li>NVIDIA (CUDA) 和 Moore (MUSA) 的 Warp Size 通常为 <strong>32</strong>。</li>
<li>沐曦 (MACA) 的 Warp Size 可能为 <strong>64</strong>（如 <code>mcDeviceProp_t.warpSize</code> 所示）。</li>
<li><strong>适配建议</strong>：在进行 Warp 级规约（如 <code>warp_reduce_sum</code>）时，循环的初始 <code>offset</code> 需要根据平台的实际 Warp Size 进行调整（如从 16 改为 32）。</li>
</ul>
</li>
<li><p><strong>Shared Memory 限制</strong>：</p>
<ul>
<li>NVIDIA RTX 5090：每 Block 约 48KB。</li>
<li>Moore：每 Block 可达 192KB。</li>
<li>沐曦：每 Block 约 64KB。</li>
<li><strong>适配建议</strong>：在设计 Tiling 策略（如 <code>Br</code> 和 <code>Bc</code> 的大小）时，需确保分配的 <code>smem_size</code> 不超过目标平台的硬件上限。</li>
</ul>
</li>
</ul>
<h4 id="3-异步操作的支持度"><a href="#3-异步操作的支持度" class="headerlink" title="3. 异步操作的支持度"></a>3. 异步操作的支持度</h4><p>在某些国产平台的早期驱动或特定型号上，异步 API（如 <code>cudaMallocAsync</code>、<code>cudaFreeAsync</code>）可能未被完全支持或存在性能问题。</p>
<ul>
<li><strong>适配建议</strong>：在迁移初期，建议先回退到同步 API（如 <code>mcMalloc</code>、<code>musaMalloc</code>），确保功能正确性后，再逐步尝试引入 Stream 和 Async API 进行性能调优。</li>
</ul>
<h3 id="8-11-总结"><a href="#8-11-总结" class="headerlink" title="8.11 总结"></a>8.11 总结</h3><p>通过对比分析这份高性能代码，我们可以得出编写极致 CUDA 算子的几个核心方法论：</p>
<ol>
<li><strong>Host 端能做的绝不交给 Device 端</strong>（如提取对角线）。</li>
<li><strong>API 调用的开销不容忽视</strong>（合并 Malloc，使用 Async 和 Stream）。</li>
<li><strong>寄存器 &gt; Shared Memory &gt; Global Memory</strong>（数据一旦加载到 SMEM，就要尽可能榨干其复用价值，如 S&#x2F;P 矩阵复用）。</li>
<li><strong>指令级优化</strong>（<code>#pragma unroll</code> 消除循环开销，位运算替代乘除法）。</li>
<li><strong>在性能与精度之间寻找平衡</strong>（关键路径使用 <code>double</code> 护航）。</li>
<li><strong>Shared Memory 布局的艺术</strong>（通过转置消除 Bank Conflict，远比单纯加 Padding 高效）。</li>
<li><strong>软硬件协同的极致压榨</strong>（在 FP64 孱弱的平台上，用 FP32x2 软件模拟双精度）。</li>
<li><strong>空间换时间</strong>（用查表法 LUT 替代昂贵的数学指令）。</li>
<li><strong>跨平台适配的克制与灵活</strong>（在国产平台上，尽量使用保守的 C++11 标准；熟练掌握 API 替换规则；并根据目标硬件的 Warp Size 和 SMEM 上限动态调整 Kernel 参数）。</li>
</ol>

		</div>

		
		<div class="post-copyright-info w-full my-8 px-2 sm:px-6 md:px-8">
			<div class="article-copyright-info-container">
    <ul>
        <li><strong>Title:</strong> 算子实例</li>
        <li><strong>Author:</strong> Ikko</li>
        <li><strong>Created at
                :</strong> 2026-02-05 13:56:54</li>
        
            <li>
                <strong>Updated at
                    :</strong> 2026-02-22 20:26:10
            </li>
        
        <li>
            <strong>Link:</strong> http://ikko-debug.github.io/2026/02/05/suanzi/
        </li>
        <li>
            <strong>
                License:
            </strong>
            

            
                This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0">CC BY-NC-SA 4.0</a>.
            
        </li>
    </ul>
</div>

		</div>
		

		

		

		
		<div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8">
			
			
			<div class="article-next border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="next" rel="next" href="/2026/01/26/bianyi/">
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item truncate max-w-48">AI编译器</span>
						<span class="post-nav-item">Next posts</span>
					</span>
					<span class="right arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-right"></i>
					</span>
				</a>
			</div>
			
		</div>
		


		
		<div class="comment-container px-2 sm:px-6 md:px-8 pb-8">
			<div class="comments-container mt-10 w-full ">
    <div id="comment-anchor" class="w-full h-2.5"></div>
    <div class="comment-area-title w-full my-1.5 md:my-2.5 text-xl md:text-3xl font-bold">
        Comments
    </div>
    

        
            
    <div id="gitalk-container"></div>
    <script data-swup-reload-script
            src="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js"></script>
    <script data-swup-reload-script>

        function loadGitalk() {
            let __gitalk__pathname = decodeURI(location.pathname);
            const __gitalk__pathnameLength = __gitalk__pathname.length;
            const __gitalk__pathnameMaxLength = 50;
            if (__gitalk__pathnameLength > __gitalk__pathnameMaxLength) {
                __gitalk__pathname = __gitalk__pathname.substring(0, __gitalk__pathnameMaxLength - 3) + '...';
            }

            try {
                Gitalk && new Gitalk({
                    clientID: '55ccec7ba873a504625f',
                    clientSecret: '5a44aa297b4124ddb3e9a8bb19842a6c066273e6',
                    repo: 'gittalk',
                    owner: 'ikko-debug',
                    admin: ['ikko-debug'],
                    id: __gitalk__pathname,
                    language: 'en',
                    proxy: 'https://github.com/login/oauth/access_token'
                }).render('gitalk-container');

            } catch (e) {
                window.Gitalk = null;
            }
        }

        if ('true') {
            const loadGitalkTimeout = setTimeout(() => {
                loadGitalk();
                clearTimeout(loadGitalkTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadGitalk);
        }
    </script>



        
    
</div>

		</div>
		
	</div>

	
	<div class="toc-content-container">
		<div class="post-toc-wrap">
	<div class="post-toc">
		<div class="toc-title">On this page</div>
		<div class="page-title">算子实例</div>
		<ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%B4%E6%96%87%E4%BB%B6%E5%92%8C%E5%AE%8F%E5%AE%9A%E4%B9%89"><span class="nav-text">头文件和宏定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Warp-%E7%BA%A7%E5%BD%92%E7%BA%A6%E6%93%8D%E4%BD%9C"><span class="nav-text">1. Warp 级归约操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Warp-Reduce-Sum"><span class="nav-text">Warp Reduce Sum</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Warp-Reduce-Max"><span class="nav-text">Warp Reduce Max</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Block-%E7%BA%A7%E5%BD%92%E7%BA%A6%E6%93%8D%E4%BD%9C"><span class="nav-text">2. Block 级归约操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E7%9F%A9%E9%98%B5%E8%BF%B9%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="nav-text">3. 矩阵迹的计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%B9%E7%9A%84%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-text">迹的核函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Host-%E7%AB%AF%E8%BF%B9%E7%9A%84%E8%AE%A1%E7%AE%97%E5%87%BD%E6%95%B0"><span class="nav-text">Host 端迹的计算函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="nav-text">4. 注意力机制 - 类型转换</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#float-%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="nav-text">float 类型转换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="nav-text">反向类型转换</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E6%9C%B4%E7%B4%A0%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%AE%9E%E7%8E%B0"><span class="nav-text">5. 朴素注意力实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Flash-Attention-V1-%E4%BC%98%E5%8C%96%E7%89%88%E6%9C%AC"><span class="nav-text">6. Flash Attention V1 - 优化版本</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%94%B9%E8%BF%9B%E8%AF%B4%E6%98%8E"><span class="nav-text">核心改进说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%90%8E%E7%9A%84%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-text">优化后的核函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Host-%E7%AB%AF-Flash-Attention-%E5%87%BD%E6%95%B0"><span class="nav-text">Host 端 Flash Attention 函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97"><span class="nav-text">更新日志</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Flash-Attention-%E4%BC%98%E5%8C%96%E5%8D%87%E7%BA%A7"><span class="nav-text">Flash Attention 优化升级</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-%E6%98%BE%E5%BC%8F%E6%A8%A1%E6%9D%BF%E5%AE%9E%E4%BE%8B%E5%8C%96"><span class="nav-text">7. 显式模板实例化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-%E6%9E%81%E8%87%B4%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%9A%E4%B8%8E%E7%AC%AC%E4%B8%80%E5%AF%B9%E6%AF%94"><span class="nav-text">8. 极致性能优化：与第一对比</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E4%B8%8E%E9%87%8A%E6%94%BE%E7%9A%84%E6%9E%81%E8%87%B4%E4%BC%98%E5%8C%96%EF%BC%9A%E5%90%88%E5%B9%B6%E4%B8%8E%E5%BC%82%E6%AD%A5"><span class="nav-text">8.1 内存分配与释放的极致优化：合并与异步</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-%E7%AE%97%E6%B3%95%E5%B1%82%E9%9D%A2%E7%9A%84-I-x2F-O-%E4%BC%98%E5%8C%96%EF%BC%9A%E4%BB%85%E5%8A%A0%E8%BD%BD%E6%9C%89%E6%95%88%E6%95%B0%E6%8D%AE"><span class="nav-text">8.2 算法层面的 I&#x2F;O 优化：仅加载有效数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-%E5%BE%AA%E7%8E%AF%E5%B1%95%E5%BC%80%E4%B8%8E%E5%88%86%E6%94%AF%E9%A2%84%E6%B5%8B%E4%BC%98%E5%8C%96"><span class="nav-text">8.3 循环展开与分支预测优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-Shared-Memory-%E8%B5%84%E6%BA%90%E5%A4%8D%E7%94%A8"><span class="nav-text">8.4 Shared Memory 资源复用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-5-%E5%85%B3%E9%94%AE%E6%AD%A5%E9%AA%A4%E7%9A%84%E7%B2%BE%E5%BA%A6%E4%BF%9D%E6%8A%A4-Double-Precision"><span class="nav-text">8.5 关键步骤的精度保护 (Double Precision)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-6-Shared-Memory-%E8%AE%BF%E5%AD%98%E6%A8%A1%E5%BC%8F%E4%BC%98%E5%8C%96%EF%BC%9AK-%E7%9F%A9%E9%98%B5%E8%BD%AC%E7%BD%AE%E6%B6%88%E9%99%A4-Bank-Conflict"><span class="nav-text">8.6 Shared Memory 访存模式优化：K 矩阵转置消除 Bank Conflict</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-7-%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%85%BC%E5%AE%B9%E4%B8%8E%E8%BD%AF%E7%A1%AC%E4%BB%B6%E5%8D%8F%E5%90%8C%EF%BC%9A%E8%BD%AF%E4%BB%B6%E6%A8%A1%E6%8B%9F%E5%8F%8C%E7%B2%BE%E5%BA%A6-FP32x2"><span class="nav-text">8.7 跨平台兼容与软硬件协同：软件模拟双精度 (FP32x2)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-8-%E5%B8%B8%E9%87%8F%E8%AE%A1%E7%AE%97%E7%9A%84%E6%9F%A5%E8%A1%A8%E6%B3%95-LUT-%E4%BC%98%E5%8C%96"><span class="nav-text">8.8 常量计算的查表法 (LUT) 优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-9-%E5%9B%BD%E4%BA%A7%E5%B9%B3%E5%8F%B0%E9%80%82%E9%85%8D%EF%BC%9AC-%E6%A0%87%E5%87%86%E4%B8%8E%E7%BC%96%E8%AF%91%E5%99%A8%E7%89%B9%E6%80%A7%E7%9A%84%E5%A6%A5%E5%8D%8F"><span class="nav-text">8.9 国产平台适配：C++ 标准与编译器特性的妥协</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-10-%E5%9B%BD%E4%BA%A7%E5%B9%B3%E5%8F%B0%E9%80%82%E9%85%8D%EF%BC%9A%E4%BB%8E-CUDA-%E5%88%B0-MUSA-Moore-%E4%B8%8E-MACA-%E6%B2%90%E6%9B%A6-%E7%9A%84%E8%BF%81%E7%A7%BB%E6%8C%87%E5%8D%97"><span class="nav-text">8.10 国产平台适配：从 CUDA 到 MUSA (Moore) 与 MACA (沐曦) 的迁移指南</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-11-%E6%80%BB%E7%BB%93"><span class="nav-text">8.11 总结</span></a></li></ol></li></ol>

	</div>
</div>
	</div>
	
</div>
			</div>

			
		</div>

		<div class="main-content-footer">
			<footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2022</span>
              -
            
            2026&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Ikko</a>
            
                
                <p class="post-count space-x-0.5">
                    <span>
                        34 posts in total
                    </span>
                    
                        <span>
                            79.9k words in total
                        </span>
                    
                </p>
            
        </div>
        
            <script data-swup-reload-script src="https://cn.vercount.one/js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">VISITOR COUNT</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">TOTAL PAGE VIEWS</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a></span>
            <span class="text-sm lg:block">THEME&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.8.5</a></span>
        </div>
        
        
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
		</div>
	</div>

	
	<div class="post-tools">
		<div class="post-tools-container">
	<ul class="article-tools-list">
		<!-- TOC aside toggle -->
		
		<li class="right-bottom-tools page-aside-toggle">
			<i class="fa-regular fa-outdent"></i>
		</li>
		

		<!-- go comment -->
		
		<li class="go-comment">
			<i class="fa-regular fa-comments"></i>
		</li>
		
	</ul>
</div>
	</div>
	

	<div class="right-side-tools-container">
		<div class="side-tools-container">
	<ul class="hidden-tools-list">
		<li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-plus"></i>
		</li>

		<li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-minus"></i>
		</li>

		<li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
			<i class="fa-regular fa-moon"></i>
		</li>

		<!-- rss -->
		

		

		<li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
			<i class="fa-regular fa-arrow-down"></i>
		</li>
	</ul>

	<ul class="visible-tools-list">
		<li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
			<i class="fa-regular fa-cog "></i>
		</li>
		
		<li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
			<i class="arrow-up fas fa-arrow-up"></i>
			<span class="percent"></span>
		</li>
		
		
	</ul>
</div>
	</div>

	<div class="image-viewer-container">
	<img src="">
</div>

	
	<div class="search-pop-overlay">
	<div class="popup search-popup">
		<div class="search-header">
			<span class="search-input-field-pre">
				<i class="fa-solid fa-keyboard"></i>
			</span>
			<div class="search-input-container">
				<input autocomplete="off" autocorrect="off" autocapitalize="off" placeholder="Search..." spellcheck="false" type="search" class="search-input">
			</div>
			<span class="popup-btn-close">
				<i class="fa-solid fa-times"></i>
			</span>
		</div>
		<div id="search-result">
			<div id="no-result">
				<i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i>
			</div>
		</div>
	</div>
</div>
	

</main>


<script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/Swup.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupSlideTheme.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupScriptsPlugin.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupProgressPlugin.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupScrollPlugin.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupPreloadPlugin.min.js" ></script>
<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
            new SwupPreloadPlugin(),
        ],
        containers: ["#swup"],
    });
</script>




	<script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/imageViewer.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/utils.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/main.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/navbarShrink.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/scrollTopBottom.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/lightDarkSwitch.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/categoryList.js" ></script>


    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/localSearch.js" ></script>



    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/codeBlock.js" ></script>



    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/lazyload.js" ></script>



    <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/runtime.js" ></script>
    <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/odometer.min.js" ></script>
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/assets/odometer-theme-minimal.css">



  <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/Typed.min.js" ></script>
  <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/plugins/typed.js" ></script>





    <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/minimasonry.min.js" ></script>
    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/plugins/masonry.js" ></script>



    <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/anime.min.js" ></script>




    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/tocToggle.js" data-swup-reload-script></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/toc.js" data-swup-reload-script></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/plugins/tabs.js" data-swup-reload-script></script>


<script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/moment-with-locales.min.js" data-swup-reload-script></script>
<script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/essays.js" data-swup-reload-script></script>




    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/bookmarkNav.js" ></script>

	
</body>

</html>