<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Ikko">
    
    <!-- Completely eliminate flash of wrong theme -->
    <script>
        (function() {
            const THEME_KEY = "REDEFINE-THEME-STATUS";
            const DARK = "dark", LIGHT = "light";
            
            // Get preferred theme
            function getTheme() {
                try {
                    const saved = localStorage.getItem(THEME_KEY);
                    if (saved) {
                        const { isDark } = JSON.parse(saved);
                        return isDark ? DARK : LIGHT;
                    }
                } catch (e) {}
                
                return matchMedia("(prefers-color-scheme: dark)").matches ? DARK : LIGHT;
            }
            
            // Apply theme to document
            function applyTheme(theme) {
                const isDark = theme === DARK;
                const root = document.documentElement;
                
                // Set data attribute for CSS variables
                root.setAttribute("data-theme", theme);
                
                // Set classes for compatibility
                root.classList.add(theme);
                root.classList.remove(isDark ? LIGHT : DARK);
                root.style.colorScheme = theme;
            }
            
            // Initial application
            const theme = getTheme();
            applyTheme(theme);
            
            // Listen for system preference changes
            matchMedia("(prefers-color-scheme: dark)").addEventListener("change", ({ matches }) => {
                // Only update if using system preference (no localStorage entry)
                if (!localStorage.getItem(THEME_KEY)) {
                    applyTheme(matches ? DARK : LIGHT);
                }
            });
            
            // Set body classes once DOM is ready
            if (document.readyState !== "loading") {
                document.body.classList.add(theme + "-mode");
            } else {
                document.addEventListener("DOMContentLoaded", () => {
                    document.body.classList.add(theme + "-mode");
                    document.body.classList.remove((theme === DARK ? LIGHT : DARK) + "-mode");
                });
            }
        })();
    </script>
    
    <!-- Critical CSS to prevent flash -->
    <style>
        :root[data-theme="dark"] {
            --background-color: #202124;
            --background-color-transparent: rgba(32, 33, 36, 0.6);
            --second-background-color: #2d2e32;
            --third-background-color: #34353a;
            --third-background-color-transparent: rgba(32, 33, 36, 0.6);
            --primary-color: #0066CC;
            --first-text-color: #ffffff;
            --second-text-color: #eeeeee;
            --third-text-color: #bebec6;
            --fourth-text-color: #999999;
            --default-text-color: #bebec6;
            --invert-text-color: #373D3F;
            --border-color: rgba(255, 255, 255, 0.08);
            --selection-color: #0066CC;
            --shadow-color-1: rgba(255, 255, 255, 0.08);
            --shadow-color-2: rgba(255, 255, 255, 0.05);
        }
        
        :root[data-theme="light"] {
            --background-color: #fff;
            --background-color-transparent: rgba(255, 255, 255, 0.6);
            --second-background-color: #f8f8f8;
            --third-background-color: #f2f2f2;
            --third-background-color-transparent: rgba(241, 241, 241, 0.6);
            --primary-color: #0066CC;
            --first-text-color: #16171a;
            --second-text-color: #2f3037;
            --third-text-color: #5e5e5e;
            --fourth-text-color: #eeeeee;
            --default-text-color: #373D3F;
            --invert-text-color: #bebec6;
            --border-color: rgba(0, 0, 0, 0.08);
            --selection-color: #0066CC;
            --shadow-color-1: rgba(0, 0, 0, 0.08);
            --shadow-color-2: rgba(0, 0, 0, 0.05);
        }
        
        body {
            background-color: var(--background-color);
            color: var(--default-text-color);
        }
        
        /* Apply body classes as soon as DOM is ready */
        :root[data-theme="dark"] body {
            background-color: var(--background-color);
            color: var(--default-text-color);
        }
    </style>
    
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    
        
        
        
            <link rel="preconnect" href="https://registry.npmmirror.com" crossorigin>
        
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://ikko-debug.github.io/2025/01/06/transformer/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
    
    
        
        <meta name="description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:type" content="article">
<meta property="og:title" content="transformer">
<meta property="og:url" content="http://ikko-debug.github.io/2025/01/06/transformer/index.html">
<meta property="og:site_name" content="IKKO">
<meta property="og:description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://ikko-debug.github.io/images/redefine-og.webp">
<meta property="article:published_time" content="2025-01-06T06:54:29.000Z">
<meta property="article:modified_time" content="2025-12-17T12:43:12.341Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://ikko-debug.github.io/images/redefine-og.webp">
    
    
        <!-- Google tag (gtag.js) -->
        <script src="https://www.googletagmanager.com/gtag/js?id=G-P5152X2PGJ"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-P5152X2PGJ');
        </script>
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/idea.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/idea.svg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/idea.svg">
    <!--- Page Info-->
    
    <title>
        
            transformer | IKKO
        
    </title>

    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fonts/Chillax/chillax.css">

    <!--- Inject Part-->
    

    
<link rel="stylesheet" href="/css/style.css">


    
        <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/css/build/tailwind.css">
    

    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fonts/GeistMono/geist-mono.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fonts/Geist/geist.css">
    <!--- Font Part-->
    
    
    
    
    
    

    <script id="hexo-configurations">
    window.config = {"hostname":"ikko-debug.github.io","root":"/","language":"en","path":"search.xml"};
    window.theme = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true,"delete_mask":false,"title_alignment":"left","headings_top_spacing":{"h1":"3.2rem","h2":"2.4rem","h3":"1.9rem","h4":"1.6rem","h5":"1.4rem","h6":"1.3rem"}},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":true,"list":[""]},"code_block":{"copy":true,"style":"mac","highlight_theme":{"light":"github","dark":"vs2015"},"font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":{"enable":true,"default":"cc_by_nc_sa"},"lazyload":true,"pangu_js":false,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null,"default_mode":"dark"},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null},"title":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"website_counter":{"url":"https://cn.vercount.one/js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"preloader":{"enable":false,"custom_message":null},"open_graph":{"enable":true,"image":"/images/redefine-og.webp","description":"Hexo Theme Redefine, Redefine Your Hexo Journey."},"google_analytics":{"enable":true,"id":"G-P5152X2PGJ"}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/light.jpg","dark":"/images/dark.jpg"},"title":"若有恒，何必三更眠五更起 最无益，只怕一日曝十日寒","subtitle":{"text":[],"hitokoto":{"enable":false,"show_author":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"1.5rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":false,"style":"default","links":{"github":"https://github.com/ikko-debug","instagram":null,"zhihu":null,"twitter":null,"email":null},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null,"lrc":null}]},"mermaid":{"enable":false,"version":"11.4.1"}},"version":"2.8.5","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"width":{"home":"1200px","pages":"1000px"},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"FriendLinks":{"icon":"fa-solid fa-link","submenus":{"yebao":"https://danmoliuhen.github.io/","wu-22":"https://wu-22.github.io/","jjb":"https://byjiangjb.github.io/"}},"album":{"icon":"fa-solid fa-image","path":"/masonry/"},"Bookmarks":{"icon":"fa-solid fa-bookmark","path":"/bookmarks/"}},"search":{"enable":true,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":"never","show_on_mobile":true,"links":null},"article_date_format":"auto","excerpt_length":200,"categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2022/8/17 11:45:14"};
    window.lang_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    window.data = {"masonry":true};
  </script>
    
    <!--- Fontawesome Part-->
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/fontawesome.min.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/brands.min.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/solid.min.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/regular.min.css">
    
    
    
    
<meta name="generator" content="Hexo 6.3.0"></head>



<body>
	<div class="progress-bar-container">
	

	
	<span class="pjax-progress-bar"></span>
	<!--        <span class="swup-progress-icon">-->
	<!--            <i class="fa-solid fa-circle-notch fa-spin"></i>-->
	<!--        </span>-->
	
</div>

<main class="page-container" id="swup">

	

	<div class="main-content-container flex flex-col justify-between min-h-dvh">
		<div class="main-content-header">
			<header class="navbar-container px-6 md:px-12">
    <div class="navbar-content transition-navbar ">
        <div class="left">
            
                <a class="logo-image h-8 w-8 sm:w-10 sm:h-10 mr-3" href="/">
                    <img src="/images/idea.svg" class="w-full h-full rounded-xs">
                </a>
            
            <a class="logo-title" href="/">
                
                IKKO
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/"
                                        >
                                    <i class="fa-regular fa-house fa-fw"></i>
                                    HOME
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/archives"
                                        >
                                    <i class="fa-regular fa-archive fa-fw"></i>
                                    ARCHIVES
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown"
                                   href="#"
                                        onClick=&#34;return false;&#34;>
                                    <i class="fa-solid fa-link fa-fw"></i>
                                    FRIENDLINKS
                                    <i class="fa-solid fa-chevron-down fa-fw"></i>
                                </a>

                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                        
                                            <li>
                                                <a target="_blank" rel="noopener" href="https://danmoliuhen.github.io/">
                                                    YEBAO
                                                </a>
                                            </li>
                                        
                                            <li>
                                                <a target="_blank" rel="noopener" href="https://wu-22.github.io/">
                                                    WU-22
                                                </a>
                                            </li>
                                        
                                            <li>
                                                <a target="_blank" rel="noopener" href="https://byjiangjb.github.io/">
                                                    JJB
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/masonry/"
                                        >
                                    <i class="fa-solid fa-image fa-fw"></i>
                                    ALBUM
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/bookmarks/"
                                        >
                                    <i class="fa-solid fa-bookmark fa-fw"></i>
                                    BOOKMARKS
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                    
                        <li class="navbar-item search search-popup-trigger">
                            <i class="fa-solid fa-magnifying-glass"></i>
                        </li>
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i>
                    </div>
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile sheet -->
    <div class="navbar-drawer h-dvh w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between">
        <ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start">
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/"
                        >
                            <span>
                                HOME
                            </span>
                            
                                <i class="fa-regular fa-house fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/archives"
                        >
                            <span>
                                ARCHIVES
                            </span>
                            
                                <i class="fa-regular fa-archive fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item-sub text-base my-1.5 flex flex-col w-full">
                        
                        <div class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary cursor-pointer text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                             navbar-data-toggle="submenu-FriendLinks"
                        >
                            <span>
                                FRIENDLINKS
                            </span>
                            
                                <i class="fa-solid fa-chevron-right fa-sm fa-fw transition-all"></i>
                            
                        </div>
                        

                        
                            <div class="flex-col items-start px-2 py-2 hidden" data-target="submenu-FriendLinks">
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           target="_blank" rel="noopener" href="https://danmoliuhen.github.io/">YEBAO</a>
                                    </div>
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           target="_blank" rel="noopener" href="https://wu-22.github.io/">WU-22</a>
                                    </div>
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           target="_blank" rel="noopener" href="https://byjiangjb.github.io/">JJB</a>
                                    </div>
                                
                            </div>
                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/masonry/"
                        >
                            <span>
                                ALBUM
                            </span>
                            
                                <i class="fa-solid fa-image fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/bookmarks/"
                        >
                            <span>
                                BOOKMARKS
                            </span>
                            
                                <i class="fa-solid fa-bookmark fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            

            
            
        </ul>

        <div class="statistics flex justify-around my-2.5">
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/tags">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">7</div>
        <div class="label text-third-text-color text-sm">Tags</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/categories">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">0</div>
        <div class="label text-third-text-color text-sm">Categories</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/archives">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">32</div>
        <div class="label text-third-text-color text-sm">Posts</div>
    </a>
</div>
    </div>

    <div class="window-mask"></div>

</header>


		</div>

		<div class="main-content-body transition-fade-up">
			

			<div class="main-content">
				<div class="post-page-container flex relative justify-between box-border w-full h-full">
	<div class="article-content-container">

		<div class="article-title relative w-full">
			
			<div class="w-full flex items-center pt-6 justify-start">
				<h1 class="article-title-regular text-second-text-color tracking-tight text-4xl md:text-6xl font-semibold px-2 sm:px-6 md:px-8 py-3">transformer</h1>
			</div>
			
		</div>

		
		<div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8">
			<div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]">
				<img src="/images/56D9D2F8A1A04F1D59D48CAD0011E450.jpg">
			</div>
			<div class="info flex flex-col justify-between">
				<div class="author flex items-center">
					<span class="name text-default-text-color text-lg font-semibold">Ikko</span>
					
					<span class="author-label ml-1.5 text-xs px-2 py-0.5 rounded-small text-third-text-color border border-shadow-color-1">Lv4</span>
					
				</div>
				<div class="meta-info">
					<div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2025-01-06 14:54:29</span>
        <span class="mobile">2025-01-06 14:54:29</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2025-12-17 20:43:12</span>
            <span class="mobile">2025-12-17 20:43:12</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fa-regular fa-typewriter"></i>&nbsp;<span>10.3k Words</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fa-regular fa-clock"></i>&nbsp;<span>64 Mins</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

				</div>
			</div>
		</div>
		

		


		<div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8">
			<p>简单改了下bart的代码测下时间，放这备份一下</p>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="comment"># Copyright 2021 The Fairseq Authors and The HuggingFace Inc. team. All rights reserved.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment"># you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment"># You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;PyTorch BART model.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, <span class="type">Optional</span>, <span class="type">Tuple</span>, <span class="type">Union</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.checkpoint</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> BCEWithLogitsLoss, CrossEntropyLoss, MSELoss</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> ...activations <span class="keyword">import</span> ACT2FN</span><br><span class="line"><span class="keyword">from</span> ...generation <span class="keyword">import</span> GenerationMixin</span><br><span class="line"><span class="keyword">from</span> ...modeling_attn_mask_utils <span class="keyword">import</span> (</span><br><span class="line">    _prepare_4d_attention_mask,</span><br><span class="line">    _prepare_4d_attention_mask_for_sdpa,</span><br><span class="line">    _prepare_4d_causal_attention_mask,</span><br><span class="line">    _prepare_4d_causal_attention_mask_for_sdpa,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> ...modeling_outputs <span class="keyword">import</span> (</span><br><span class="line">    BaseModelOutput,</span><br><span class="line">    BaseModelOutputWithPastAndCrossAttentions,</span><br><span class="line">    CausalLMOutputWithCrossAttentions,</span><br><span class="line">    Seq2SeqLMOutput,</span><br><span class="line">    Seq2SeqModelOutput,</span><br><span class="line">    Seq2SeqQuestionAnsweringModelOutput,</span><br><span class="line">    Seq2SeqSequenceClassifierOutput,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> ...modeling_utils <span class="keyword">import</span> PreTrainedModel</span><br><span class="line"><span class="keyword">from</span> ...utils <span class="keyword">import</span> (</span><br><span class="line">    add_code_sample_docstrings,</span><br><span class="line">    add_end_docstrings,</span><br><span class="line">    add_start_docstrings,</span><br><span class="line">    add_start_docstrings_to_model_forward,</span><br><span class="line">    is_flash_attn_2_available,</span><br><span class="line">    is_flash_attn_greater_or_equal_2_10,</span><br><span class="line">    logging,</span><br><span class="line">    replace_return_docstrings,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> .configuration_bart <span class="keyword">import</span> BartConfig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> is_flash_attn_2_available():</span><br><span class="line">    <span class="keyword">from</span> ...modeling_flash_attention_utils <span class="keyword">import</span> _flash_attention_forward</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">logger = logging.get_logger(__name__)</span><br><span class="line"><span class="comment"># 配置日志记录</span></span><br><span class="line"><span class="keyword">import</span> logging <span class="keyword">as</span> loggingg</span><br><span class="line">loggingg.basicConfig(</span><br><span class="line">    filename=<span class="string">&quot;bart_timing.log&quot;</span>,</span><br><span class="line">    level=logging.INFO,</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">&quot;%(asctime)s - %(message)s&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">_CHECKPOINT_FOR_DOC = <span class="string">&quot;facebook/bart-base&quot;</span></span><br><span class="line">_CONFIG_FOR_DOC = <span class="string">&quot;BartConfig&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Base model docstring</span></span><br><span class="line">_EXPECTED_OUTPUT_SHAPE = [<span class="number">1</span>, <span class="number">8</span>, <span class="number">768</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># SequenceClassification docstring</span></span><br><span class="line">_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION = <span class="string">&quot;valhalla/bart-large-sst2&quot;</span></span><br><span class="line">_SEQ_CLASS_EXPECTED_LOSS = <span class="number">0.0</span></span><br><span class="line">_SEQ_CLASS_EXPECTED_OUTPUT = <span class="string">&quot;&#x27;POSITIVE&#x27;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># QuestionAsnwering docstring</span></span><br><span class="line">_CHECKPOINT_FOR_QA = <span class="string">&quot;valhalla/bart-large-finetuned-squadv1&quot;</span></span><br><span class="line">_QA_EXPECTED_LOSS = <span class="number">0.59</span></span><br><span class="line">_QA_EXPECTED_OUTPUT = <span class="string">&quot;&#x27; nice puppet&#x27;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shift_tokens_right</span>(<span class="params">input_ids: torch.Tensor, pad_token_id: <span class="built_in">int</span>, decoder_start_token_id: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Shift input ids one token to the right.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    shifted_input_ids = input_ids.new_zeros(input_ids.shape)</span><br><span class="line">    shifted_input_ids[:, <span class="number">1</span>:] = input_ids[:, :-<span class="number">1</span>].clone()</span><br><span class="line">    shifted_input_ids[:, <span class="number">0</span>] = decoder_start_token_id</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> pad_token_id <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;self.model.config.pad_token_id has to be defined.&quot;</span>)</span><br><span class="line">    <span class="comment"># replace possible -100 values in labels by `pad_token_id`</span></span><br><span class="line">    shifted_input_ids.masked_fill_(shifted_input_ids == -<span class="number">100</span>, pad_token_id)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> shifted_input_ids</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BartLearnedPositionalEmbedding</span>(nn.Embedding):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This module learns positional embeddings up to a fixed maximum size.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_embeddings: <span class="built_in">int</span>, embedding_dim: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="comment"># Bart is set up so that if padding_idx is specified then offset the embedding ids by 2</span></span><br><span class="line">        <span class="comment"># and adjust num_embeddings appropriately. Other models don&#x27;t have this hack</span></span><br><span class="line">        self.offset = <span class="number">2</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(num_embeddings + self.offset, embedding_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids: torch.Tensor, past_key_values_length: <span class="built_in">int</span> = <span class="number">0</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;`input_ids&#x27; shape is expected to be [bsz x seqlen].&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        bsz, seq_len = input_ids.shape[:<span class="number">2</span>]</span><br><span class="line">        positions = torch.arange(</span><br><span class="line">            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device</span><br><span class="line">        ).expand(bsz, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().forward(positions + self.offset)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BartScaledWordEmbedding</span>(nn.Embedding):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This module overrides nn.Embeddings&#x27; forward by multiplying with embeddings scale.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_embeddings: <span class="built_in">int</span>, embedding_dim: <span class="built_in">int</span>, padding_idx: <span class="built_in">int</span>, embed_scale: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="number">1.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(num_embeddings, embedding_dim, padding_idx)</span><br><span class="line">        self.embed_scale = embed_scale</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids: torch.Tensor</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().forward(input_ids) * self.embed_scale</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BartAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multi-headed attention from &#x27;Attention Is All You Need&#x27; paper&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        embed_dim: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        num_heads: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        dropout: <span class="built_in">float</span> = <span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        is_decoder: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        bias: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        is_causal: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        config: <span class="type">Optional</span>[BartConfig] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.head_dim = embed_dim // num_heads</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (self.head_dim * num_heads) != self.embed_dim:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;embed_dim must be divisible by num_heads (got `embed_dim`: <span class="subst">&#123;self.embed_dim&#125;</span>&quot;</span></span><br><span class="line">                <span class="string">f&quot; and `num_heads`: <span class="subst">&#123;num_heads&#125;</span>).&quot;</span></span><br><span class="line">            )</span><br><span class="line">        self.scaling = self.head_dim**-<span class="number">0.5</span></span><br><span class="line">        self.is_decoder = is_decoder</span><br><span class="line">        self.is_causal = is_causal</span><br><span class="line"></span><br><span class="line">        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)</span><br><span class="line">        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)</span><br><span class="line">        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)</span><br><span class="line">        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_shape</span>(<span class="params">self, tensor: torch.Tensor, seq_len: <span class="built_in">int</span>, bsz: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="keyword">return</span> tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        hidden_states: torch.Tensor,</span></span><br><span class="line"><span class="params">        key_value_states: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        past_key_value: <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.Tensor]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        attention_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        layer_head_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        output_attentions: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, <span class="type">Optional</span>[torch.Tensor], <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.Tensor]]]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Input shape: Batch x Time x Channel&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># if key_value_states are provided this layer is used as a cross-attention layer</span></span><br><span class="line">        <span class="comment"># for the decoder</span></span><br><span class="line">        is_cross_attention = key_value_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        bsz, tgt_len, _ = hidden_states.size()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get query proj</span></span><br><span class="line">        query_states = self.q_proj(hidden_states) * self.scaling</span><br><span class="line">        <span class="comment"># get key, value proj</span></span><br><span class="line">        <span class="comment"># `past_key_value[0].shape[2] == key_value_states.shape[1]`</span></span><br><span class="line">        <span class="comment"># is checking that the `sequence_length` of the `past_key_value` is the same as</span></span><br><span class="line">        <span class="comment"># the provided `key_value_states` to support prefix tuning</span></span><br><span class="line">        <span class="keyword">if</span> (</span><br><span class="line">            is_cross_attention</span><br><span class="line">            <span class="keyword">and</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">and</span> past_key_value[<span class="number">0</span>].shape[<span class="number">2</span>] == key_value_states.shape[<span class="number">1</span>]</span><br><span class="line">        ):</span><br><span class="line">            <span class="comment"># reuse k,v, cross_attentions</span></span><br><span class="line">            key_states = past_key_value[<span class="number">0</span>]</span><br><span class="line">            value_states = past_key_value[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">elif</span> is_cross_attention:</span><br><span class="line">            <span class="comment"># cross_attentions</span></span><br><span class="line">            key_states = self._shape(self.k_proj(key_value_states), -<span class="number">1</span>, bsz)</span><br><span class="line">            value_states = self._shape(self.v_proj(key_value_states), -<span class="number">1</span>, bsz)</span><br><span class="line">        <span class="keyword">elif</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># reuse k, v, self_attention</span></span><br><span class="line">            key_states = self._shape(self.k_proj(hidden_states), -<span class="number">1</span>, bsz)</span><br><span class="line">            value_states = self._shape(self.v_proj(hidden_states), -<span class="number">1</span>, bsz)</span><br><span class="line">            key_states = torch.cat([past_key_value[<span class="number">0</span>], key_states], dim=<span class="number">2</span>)</span><br><span class="line">            value_states = torch.cat([past_key_value[<span class="number">1</span>], value_states], dim=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># self_attention</span></span><br><span class="line">            key_states = self._shape(self.k_proj(hidden_states), -<span class="number">1</span>, bsz)</span><br><span class="line">            value_states = self._shape(self.v_proj(hidden_states), -<span class="number">1</span>, bsz)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder:</span><br><span class="line">            <span class="comment"># if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.</span></span><br><span class="line">            <span class="comment"># Further calls to cross_attention layer can then reuse all cross-attention</span></span><br><span class="line">            <span class="comment"># key/value_states (first &quot;if&quot; case)</span></span><br><span class="line">            <span class="comment"># if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of</span></span><br><span class="line">            <span class="comment"># all previous decoder key/value_states. Further calls to uni-directional self-attention</span></span><br><span class="line">            <span class="comment"># can concat previous decoder key/value_states to current projected key/value_states (third &quot;elif&quot; case)</span></span><br><span class="line">            <span class="comment"># if encoder bi-directional self-attention `past_key_value` is always `None`</span></span><br><span class="line">            past_key_value = (key_states, value_states)</span><br><span class="line"></span><br><span class="line">        proj_shape = (bsz * self.num_heads, -<span class="number">1</span>, self.head_dim)</span><br><span class="line">        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)</span><br><span class="line">        key_states = key_states.reshape(*proj_shape)</span><br><span class="line">        value_states = value_states.reshape(*proj_shape)</span><br><span class="line"></span><br><span class="line">        src_len = key_states.size(<span class="number">1</span>)</span><br><span class="line">        attn_weights = torch.bmm(query_states, key_states.transpose(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;Attention weights should be of size <span class="subst">&#123;(bsz * self.num_heads, tgt_len, src_len)&#125;</span>, but is&quot;</span></span><br><span class="line">                <span class="string">f&quot; <span class="subst">&#123;attn_weights.size()&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> attention_mask.size() != (bsz, <span class="number">1</span>, tgt_len, src_len):</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">f&quot;Attention mask should be of size <span class="subst">&#123;(bsz, <span class="number">1</span>, tgt_len, src_len)&#125;</span>, but is <span class="subst">&#123;attention_mask.size()&#125;</span>&quot;</span></span><br><span class="line">                )</span><br><span class="line">            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask</span><br><span class="line">            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)</span><br><span class="line"></span><br><span class="line">        attn_weights = nn.functional.softmax(attn_weights, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> layer_head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> layer_head_mask.size() != (self.num_heads,):</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">f&quot;Head mask for a single layer should be of size <span class="subst">&#123;(self.num_heads,)&#125;</span>, but is&quot;</span></span><br><span class="line">                    <span class="string">f&quot; <span class="subst">&#123;layer_head_mask.size()&#125;</span>&quot;</span></span><br><span class="line">                )</span><br><span class="line">            attn_weights = layer_head_mask.view(<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)</span><br><span class="line">            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> output_attentions:</span><br><span class="line">            <span class="comment"># this operation is a bit awkward, but it&#x27;s required to</span></span><br><span class="line">            <span class="comment"># make sure that attn_weights keeps its gradient.</span></span><br><span class="line">            <span class="comment"># In order to do so, attn_weights have to be reshaped</span></span><br><span class="line">            <span class="comment"># twice and have to be reused in the following</span></span><br><span class="line">            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)</span><br><span class="line">            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn_weights_reshaped = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)</span><br><span class="line"></span><br><span class="line">        attn_output = torch.bmm(attn_probs, value_states)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;`attn_output` should be of size <span class="subst">&#123;(bsz * self.num_heads, tgt_len, self.head_dim)&#125;</span>, but is&quot;</span></span><br><span class="line">                <span class="string">f&quot; <span class="subst">&#123;attn_output.size()&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)</span><br><span class="line">        attn_output = attn_output.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be</span></span><br><span class="line">        <span class="comment"># partitioned across GPUs when using tensor-parallelism.</span></span><br><span class="line">        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)</span><br><span class="line"></span><br><span class="line">        attn_output = self.out_proj(attn_output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> attn_output, attn_weights_reshaped, past_key_value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BartFlashAttention2</span>(<span class="title class_ inherited__">BartAttention</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Bart flash attention module. This module inherits from `BartAttention` as the weights of the module stays</span></span><br><span class="line"><span class="string">    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of</span></span><br><span class="line"><span class="string">    flash attention and deal with padding tokens in case the input contains any of them.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Should be removed once Flash Attention for RoCm is bumped to 2.1.</span></span><br><span class="line">        <span class="comment"># flash_attn&lt;2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn&gt;=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.</span></span><br><span class="line">        <span class="comment"># Beware that with flash_attn&lt;2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).</span></span><br><span class="line">        self._flash_attn_uses_top_left_mask = <span class="keyword">not</span> is_flash_attn_greater_or_equal_2_10()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_reshape</span>(<span class="params">self, tensor: torch.Tensor, seq_len: <span class="built_in">int</span>, bsz: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="keyword">return</span> tensor.view(bsz, seq_len, self.num_heads, self.head_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        hidden_states: torch.Tensor,</span></span><br><span class="line"><span class="params">        key_value_states: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        past_key_value: <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.Tensor]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        attention_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        layer_head_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        output_attentions: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, <span class="type">Optional</span>[torch.Tensor], <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.Tensor]]]:</span><br><span class="line">        <span class="comment"># BartFlashAttention2 attention does not support output_attentions</span></span><br><span class="line">        <span class="keyword">if</span> output_attentions:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;BartFlashAttention2 attention does not support output_attentions&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if key_value_states are provided this layer is used as a cross-attention layer</span></span><br><span class="line">        <span class="comment"># for the decoder</span></span><br><span class="line">        is_cross_attention = key_value_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        bsz, q_len, _ = hidden_states.size()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get query proj</span></span><br><span class="line">        query_states = self._reshape(self.q_proj(hidden_states), -<span class="number">1</span>, bsz)</span><br><span class="line">        <span class="comment"># get key, value proj</span></span><br><span class="line">        <span class="comment"># `past_key_value[0].shape[2] == key_value_states.shape[1]`</span></span><br><span class="line">        <span class="comment"># is checking that the `sequence_length` of the `past_key_value` is the same as</span></span><br><span class="line">        <span class="comment"># the provided `key_value_states` to support prefix tuning</span></span><br><span class="line">        <span class="keyword">if</span> (</span><br><span class="line">            is_cross_attention</span><br><span class="line">            <span class="keyword">and</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">and</span> past_key_value[<span class="number">0</span>].shape[<span class="number">2</span>] == key_value_states.shape[<span class="number">1</span>]</span><br><span class="line">        ):</span><br><span class="line">            <span class="comment"># reuse k,v, cross_attentions</span></span><br><span class="line">            key_states = past_key_value[<span class="number">0</span>].transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            value_states = past_key_value[<span class="number">1</span>].transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">elif</span> is_cross_attention:</span><br><span class="line">            <span class="comment"># cross_attentions</span></span><br><span class="line">            key_states = self._reshape(self.k_proj(key_value_states), -<span class="number">1</span>, bsz)</span><br><span class="line">            value_states = self._reshape(self.v_proj(key_value_states), -<span class="number">1</span>, bsz)</span><br><span class="line">        <span class="keyword">elif</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># reuse k, v, self_attention</span></span><br><span class="line">            key_states = self._reshape(self.k_proj(hidden_states), -<span class="number">1</span>, bsz)</span><br><span class="line">            value_states = self._reshape(self.v_proj(hidden_states), -<span class="number">1</span>, bsz)</span><br><span class="line">            key_states = torch.cat([past_key_value[<span class="number">0</span>].transpose(<span class="number">1</span>, <span class="number">2</span>), key_states], dim=<span class="number">1</span>)</span><br><span class="line">            value_states = torch.cat([past_key_value[<span class="number">1</span>].transpose(<span class="number">1</span>, <span class="number">2</span>), value_states], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># self_attention</span></span><br><span class="line">            key_states = self._reshape(self.k_proj(hidden_states), -<span class="number">1</span>, bsz)</span><br><span class="line">            value_states = self._reshape(self.v_proj(hidden_states), -<span class="number">1</span>, bsz)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder:</span><br><span class="line">            <span class="comment"># if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.</span></span><br><span class="line">            <span class="comment"># Further calls to cross_attention layer can then reuse all cross-attention</span></span><br><span class="line">            <span class="comment"># key/value_states (first &quot;if&quot; case)</span></span><br><span class="line">            <span class="comment"># if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of</span></span><br><span class="line">            <span class="comment"># all previous decoder key/value_states. Further calls to uni-directional self-attention</span></span><br><span class="line">            <span class="comment"># can concat previous decoder key/value_states to current projected key/value_states (third &quot;elif&quot; case)</span></span><br><span class="line">            <span class="comment"># if encoder bi-directional self-attention `past_key_value` is always `None`</span></span><br><span class="line">            past_key_value = (key_states.transpose(<span class="number">1</span>, <span class="number">2</span>), value_states.transpose(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        kv_seq_len = key_states.shape[-<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            kv_seq_len += past_key_value[<span class="number">0</span>].shape[-<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># In PEFT, usually we cast the layer norms in float32 for training stability reasons</span></span><br><span class="line">        <span class="comment"># therefore the input hidden states gets silently casted in float32. Hence, we need</span></span><br><span class="line">        <span class="comment"># cast them back in the correct dtype just to be sure everything works as expected.</span></span><br><span class="line">        <span class="comment"># This might slowdown training &amp; inference so it is recommended to not cast the LayerNorms</span></span><br><span class="line">        <span class="comment"># in fp32. (LlamaRMSNorm handles it correctly)</span></span><br><span class="line"></span><br><span class="line">        input_dtype = query_states.dtype</span><br><span class="line">        <span class="keyword">if</span> input_dtype == torch.float32:</span><br><span class="line">            <span class="keyword">if</span> torch.is_autocast_enabled():</span><br><span class="line">                target_dtype = torch.get_autocast_gpu_dtype()</span><br><span class="line">            <span class="comment"># Handle the case where the model is quantized</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">hasattr</span>(self.config, <span class="string">&quot;_pre_quantization_dtype&quot;</span>):</span><br><span class="line">                target_dtype = self.config._pre_quantization_dtype</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                target_dtype = self.q_proj.weight.dtype</span><br><span class="line"></span><br><span class="line">            logger.warning_once(</span><br><span class="line">                <span class="string">f&quot;The input hidden states seems to be silently casted in float32, this might be related to&quot;</span></span><br><span class="line">                <span class="string">f&quot; the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in&quot;</span></span><br><span class="line">                <span class="string">f&quot; <span class="subst">&#123;target_dtype&#125;</span>.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            query_states = query_states.to(target_dtype)</span><br><span class="line">            key_states = key_states.to(target_dtype)</span><br><span class="line">            value_states = value_states.to(target_dtype)</span><br><span class="line"></span><br><span class="line">        attn_output = _flash_attention_forward(</span><br><span class="line">            query_states,</span><br><span class="line">            key_states,</span><br><span class="line">            value_states,</span><br><span class="line">            attention_mask,</span><br><span class="line">            q_len,</span><br><span class="line">            dropout=self.dropout <span class="keyword">if</span> self.training <span class="keyword">else</span> <span class="number">0.0</span>,</span><br><span class="line">            is_causal=self.is_causal,</span><br><span class="line">            use_top_left_mask=self._flash_attn_uses_top_left_mask,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        attn_output = attn_output.reshape(bsz, q_len, -<span class="number">1</span>)</span><br><span class="line">        attn_output = self.out_proj(attn_output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> output_attentions:</span><br><span class="line">            attn_weights = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> attn_output, attn_weights, past_key_value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BartSdpaAttention</span>(<span class="title class_ inherited__">BartAttention</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        hidden_states: torch.Tensor,</span></span><br><span class="line"><span class="params">        key_value_states: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        past_key_value: <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.Tensor]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        attention_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        layer_head_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        output_attentions: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, <span class="type">Optional</span>[torch.Tensor], <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.Tensor]]]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Input shape: Batch x Time x Channel&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> output_attentions <span class="keyword">or</span> layer_head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span> Improve this warning with e.g. `model.config._attn_implementation = &quot;manual&quot;` once this is implemented.</span></span><br><span class="line">            logger.warning_once(</span><br><span class="line">                <span class="string">&quot;BartModel is using BartSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention&quot;</span></span><br><span class="line">                <span class="string">&#x27; implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=&quot;eager&quot;` when loading the model.&#x27;</span></span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">super</span>().forward(</span><br><span class="line">                hidden_states,</span><br><span class="line">                key_value_states=key_value_states,</span><br><span class="line">                past_key_value=past_key_value,</span><br><span class="line">                attention_mask=attention_mask,</span><br><span class="line">                layer_head_mask=layer_head_mask,</span><br><span class="line">                output_attentions=output_attentions,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if key_value_states are provided this layer is used as a cross-attention layer</span></span><br><span class="line">        <span class="comment"># for the decoder</span></span><br><span class="line">        is_cross_attention = key_value_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        bsz, tgt_len, _ = hidden_states.size()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get query proj</span></span><br><span class="line">        query_states = self.q_proj(hidden_states)</span><br><span class="line">        <span class="comment"># get key, value proj</span></span><br><span class="line">        <span class="comment"># `past_key_value[0].shape[2] == key_value_states.shape[1]`</span></span><br><span class="line">        <span class="comment"># is checking that the `sequence_length` of the `past_key_value` is the same as</span></span><br><span class="line">        <span class="comment"># the provided `key_value_states` to support prefix tuning</span></span><br><span class="line">        <span class="keyword">if</span> (</span><br><span class="line">            is_cross_attention</span><br><span class="line">            <span class="keyword">and</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">and</span> past_key_value[<span class="number">0</span>].shape[<span class="number">2</span>] == key_value_states.shape[<span class="number">1</span>]</span><br><span class="line">        ):</span><br><span class="line">            <span class="comment"># reuse k,v, cross_attentions</span></span><br><span class="line">            key_states = past_key_value[<span class="number">0</span>]</span><br><span class="line">            value_states = past_key_value[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">elif</span> is_cross_attention:</span><br><span class="line">            <span class="comment"># cross_attentions</span></span><br><span class="line">            key_states = self._shape(self.k_proj(key_value_states), -<span class="number">1</span>, bsz)</span><br><span class="line">            value_states = self._shape(self.v_proj(key_value_states), -<span class="number">1</span>, bsz)</span><br><span class="line">        <span class="keyword">elif</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># reuse k, v, self_attention</span></span><br><span class="line">            key_states = self._shape(self.k_proj(hidden_states), -<span class="number">1</span>, bsz)</span><br><span class="line">            value_states = self._shape(self.v_proj(hidden_states), -<span class="number">1</span>, bsz)</span><br><span class="line">            key_states = torch.cat([past_key_value[<span class="number">0</span>], key_states], dim=<span class="number">2</span>)</span><br><span class="line">            value_states = torch.cat([past_key_value[<span class="number">1</span>], value_states], dim=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># self_attention</span></span><br><span class="line">            key_states = self._shape(self.k_proj(hidden_states), -<span class="number">1</span>, bsz)</span><br><span class="line">            value_states = self._shape(self.v_proj(hidden_states), -<span class="number">1</span>, bsz)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder:</span><br><span class="line">            <span class="comment"># if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.</span></span><br><span class="line">            <span class="comment"># Further calls to cross_attention layer can then reuse all cross-attention</span></span><br><span class="line">            <span class="comment"># key/value_states (first &quot;if&quot; case)</span></span><br><span class="line">            <span class="comment"># if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of</span></span><br><span class="line">            <span class="comment"># all previous decoder key/value_states. Further calls to uni-directional self-attention</span></span><br><span class="line">            <span class="comment"># can concat previous decoder key/value_states to current projected key/value_states (third &quot;elif&quot; case)</span></span><br><span class="line">            <span class="comment"># if encoder bi-directional self-attention `past_key_value` is always `None`</span></span><br><span class="line">            past_key_value = (key_states, value_states)</span><br><span class="line"></span><br><span class="line">        query_states = self._shape(query_states, tgt_len, bsz)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># We dispatch to SDPA&#x27;s Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment</span></span><br><span class="line">        <span class="comment"># in SDPA to support both torch.compile&#x27;s dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.</span></span><br><span class="line">        <span class="comment"># The tgt_len &gt; 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.</span></span><br><span class="line">        is_causal = <span class="literal">True</span> <span class="keyword">if</span> self.is_causal <span class="keyword">and</span> attention_mask <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> tgt_len &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,</span></span><br><span class="line">        <span class="comment"># but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577</span></span><br><span class="line">        attn_output = torch.nn.functional.scaled_dot_product_attention(</span><br><span class="line">            query_states,</span><br><span class="line">            key_states,</span><br><span class="line">            value_states,</span><br><span class="line">            attn_mask=attention_mask,</span><br><span class="line">            dropout_p=self.dropout <span class="keyword">if</span> self.training <span class="keyword">else</span> <span class="number">0.0</span>,</span><br><span class="line">            is_causal=is_causal,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;`attn_output` should be of size <span class="subst">&#123;(bsz, self.num_heads, tgt_len, self.head_dim)&#125;</span>, but is&quot;</span></span><br><span class="line">                <span class="string">f&quot; <span class="subst">&#123;attn_output.size()&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        attn_output = attn_output.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be</span></span><br><span class="line">        <span class="comment"># partitioned across GPUs when using tensor-parallelism.</span></span><br><span class="line">        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)</span><br><span class="line"></span><br><span class="line">        attn_output = self.out_proj(attn_output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> attn_output, <span class="literal">None</span>, past_key_value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BART_ATTENTION_CLASSES = &#123;</span><br><span class="line">    <span class="string">&quot;eager&quot;</span>: BartAttention,</span><br><span class="line">    <span class="string">&quot;sdpa&quot;</span>: BartSdpaAttention,</span><br><span class="line">    <span class="string">&quot;flash_attention_2&quot;</span>: BartFlashAttention2,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BartEncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config: BartConfig</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embed_dim = config.d_model</span><br><span class="line"></span><br><span class="line">        self.self_attn = BART_ATTENTION_CLASSES[config._attn_implementation](</span><br><span class="line">            embed_dim=self.embed_dim,</span><br><span class="line">            num_heads=config.encoder_attention_heads,</span><br><span class="line">            dropout=config.attention_dropout,</span><br><span class="line">            config=config,</span><br><span class="line">        )</span><br><span class="line">        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)</span><br><span class="line">        self.dropout = config.dropout</span><br><span class="line">        self.activation_fn = ACT2FN[config.activation_function]</span><br><span class="line">        self.activation_dropout = config.activation_dropout</span><br><span class="line">        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)</span><br><span class="line">        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)</span><br><span class="line">        self.final_layer_norm = nn.LayerNorm(self.embed_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        hidden_states: torch.FloatTensor,</span></span><br><span class="line"><span class="params">        attention_mask: torch.FloatTensor,</span></span><br><span class="line"><span class="params">        layer_head_mask: torch.FloatTensor,</span></span><br><span class="line"><span class="params">        output_attentions: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Tuple</span>[torch.FloatTensor, <span class="type">Optional</span>[torch.FloatTensor]]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`</span></span><br><span class="line"><span class="string">            attention_mask (`torch.FloatTensor`): attention mask of size</span></span><br><span class="line"><span class="string">                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.</span></span><br><span class="line"><span class="string">            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size</span></span><br><span class="line"><span class="string">                `(encoder_attention_heads,)`.</span></span><br><span class="line"><span class="string">            output_attentions (`bool`, *optional*):</span></span><br><span class="line"><span class="string">                Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span></span><br><span class="line"><span class="string">                returned tensors for more detail.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Attention 处理</span></span><br><span class="line">        start_time1 = time.time()</span><br><span class="line">        residual = hidden_states</span><br><span class="line">        hidden_states, attn_weights, _ = self.self_attn(</span><br><span class="line">            hidden_states=hidden_states,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            layer_head_mask=layer_head_mask,</span><br><span class="line">            output_attentions=output_attentions,</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)</span><br><span class="line">        attention_time = time.time() - start_time1</span><br><span class="line">        loggingg.info(<span class="string">f&quot;Attention time: <span class="subst">&#123;attention_time:<span class="number">.4</span>f&#125;</span>s&quot;</span>)</span><br><span class="line"></span><br><span class="line">        start_time2 = time.time()</span><br><span class="line">        hidden_states = residual + hidden_states</span><br><span class="line">        hidden_states = self.self_attn_layer_norm(hidden_states)</span><br><span class="line">        addnorm_time = time.time() - start_time2</span><br><span class="line">        loggingg.info(<span class="string">f&quot;Add&amp;Norm time: <span class="subst">&#123;addnorm_time:<span class="number">.4</span>f&#125;</span>s&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前馈网络处理</span></span><br><span class="line">        start_time3 = time.time()</span><br><span class="line">        residual = hidden_states</span><br><span class="line">        hidden_states = self.activation_fn(self.fc1(hidden_states))</span><br><span class="line">        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)</span><br><span class="line">        hidden_states = self.fc2(hidden_states)</span><br><span class="line">        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)</span><br><span class="line">        feedforward_time = time.time() - start_time3</span><br><span class="line">        loggingg.info(<span class="string">f&quot;Feedforward time: <span class="subst">&#123;feedforward_time:<span class="number">.4</span>f&#125;</span>s&quot;</span>)</span><br><span class="line">        start_time4 = time.time()</span><br><span class="line">        hidden_states = residual + hidden_states</span><br><span class="line">        hidden_states = self.final_layer_norm(hidden_states)</span><br><span class="line">        addnorm_time = time.time() - start_time4</span><br><span class="line">        loggingg.info(<span class="string">f&quot;Add&amp;Norm time: <span class="subst">&#123;addnorm_time:<span class="number">.4</span>f&#125;</span>s&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 返回结果</span></span><br><span class="line">        outputs = (hidden_states,)</span><br><span class="line">        <span class="keyword">if</span> output_attentions:</span><br><span class="line">            outputs += (attn_weights,)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BartDecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config: BartConfig</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embed_dim = config.d_model</span><br><span class="line"></span><br><span class="line">        self.self_attn = BART_ATTENTION_CLASSES[config._attn_implementation](</span><br><span class="line">            embed_dim=self.embed_dim,</span><br><span class="line">            num_heads=config.decoder_attention_heads,</span><br><span class="line">            dropout=config.attention_dropout,</span><br><span class="line">            is_decoder=<span class="literal">True</span>,</span><br><span class="line">            is_causal=<span class="literal">True</span>,</span><br><span class="line">            config=config,</span><br><span class="line">        )</span><br><span class="line">        self.dropout = config.dropout</span><br><span class="line">        self.activation_fn = ACT2FN[config.activation_function]</span><br><span class="line">        self.activation_dropout = config.activation_dropout</span><br><span class="line"></span><br><span class="line">        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)</span><br><span class="line">        self.encoder_attn = BART_ATTENTION_CLASSES[config._attn_implementation](</span><br><span class="line">            self.embed_dim,</span><br><span class="line">            config.decoder_attention_heads,</span><br><span class="line">            dropout=config.attention_dropout,</span><br><span class="line">            is_decoder=<span class="literal">True</span>,</span><br><span class="line">            config=config,</span><br><span class="line">        )</span><br><span class="line">        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)</span><br><span class="line">        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)</span><br><span class="line">        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)</span><br><span class="line">        self.final_layer_norm = nn.LayerNorm(self.embed_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        </span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        hidden_states: torch.Tensor,</span></span><br><span class="line"><span class="params">        attention_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        encoder_hidden_states: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        encoder_attention_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        layer_head_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        cross_attn_layer_head_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        past_key_value: <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.Tensor]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        output_attentions: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        use_cache: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Tuple</span>[torch.FloatTensor, <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.FloatTensor, torch.FloatTensor]]]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`</span></span><br><span class="line"><span class="string">            attention_mask (`torch.FloatTensor`): attention mask of size</span></span><br><span class="line"><span class="string">                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.</span></span><br><span class="line"><span class="string">            encoder_hidden_states (`torch.FloatTensor`):</span></span><br><span class="line"><span class="string">                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`</span></span><br><span class="line"><span class="string">            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size</span></span><br><span class="line"><span class="string">                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.</span></span><br><span class="line"><span class="string">            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size</span></span><br><span class="line"><span class="string">                `(encoder_attention_heads,)`.</span></span><br><span class="line"><span class="string">            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of</span></span><br><span class="line"><span class="string">                size `(decoder_attention_heads,)`.</span></span><br><span class="line"><span class="string">            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states</span></span><br><span class="line"><span class="string">            output_attentions (`bool`, *optional*):</span></span><br><span class="line"><span class="string">                Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span></span><br><span class="line"><span class="string">                returned tensors for more detail.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># attention处理</span></span><br><span class="line">        start_time1 = time.time()</span><br><span class="line">        residual = hidden_states</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Self Attention</span></span><br><span class="line">        <span class="comment"># decoder uni-directional self-attention cached key/values tuple is at positions 1,2</span></span><br><span class="line">        self_attn_past_key_value = past_key_value[:<span class="number">2</span>] <span class="keyword">if</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        <span class="comment"># add present self-attn cache to positions 1,2 of present_key_value tuple</span></span><br><span class="line">        hidden_states, self_attn_weights, present_key_value = self.self_attn(</span><br><span class="line">            hidden_states=hidden_states,</span><br><span class="line">            past_key_value=self_attn_past_key_value,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            layer_head_mask=layer_head_mask,</span><br><span class="line">            output_attentions=output_attentions,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)</span><br><span class="line">        attention_time = time.time() - start_time1</span><br><span class="line">        loggingg.info(<span class="string">f&quot;Attention time: <span class="subst">&#123;attention_time:<span class="number">.4</span>f&#125;</span>s&quot;</span>)</span><br><span class="line"></span><br><span class="line">        start_time2 = time.time()</span><br><span class="line">        hidden_states = residual + hidden_states</span><br><span class="line">        hidden_states = self.self_attn_layer_norm(hidden_states)</span><br><span class="line"></span><br><span class="line">        addnorm_time = time.time() - start_time2</span><br><span class="line">        loggingg.info(<span class="string">f&quot;Add&amp;Norm time: <span class="subst">&#123;addnorm_time:<span class="number">.4</span>f&#125;</span>s&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cross-Attention Block</span></span><br><span class="line">        cross_attn_present_key_value = <span class="literal">None</span></span><br><span class="line">        cross_attn_weights = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            residual = hidden_states</span><br><span class="line"></span><br><span class="line">            <span class="comment"># cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple</span></span><br><span class="line">            cross_attn_past_key_value = past_key_value[-<span class="number">2</span>:] <span class="keyword">if</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(</span><br><span class="line">                hidden_states=hidden_states,</span><br><span class="line">                key_value_states=encoder_hidden_states,</span><br><span class="line">                attention_mask=encoder_attention_mask,</span><br><span class="line">                layer_head_mask=cross_attn_layer_head_mask,</span><br><span class="line">                past_key_value=cross_attn_past_key_value,</span><br><span class="line">                output_attentions=output_attentions,</span><br><span class="line">            )</span><br><span class="line">            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)</span><br><span class="line">            hidden_states = residual + hidden_states</span><br><span class="line">            hidden_states = self.encoder_attn_layer_norm(hidden_states)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># add cross-attn to positions 3,4 of present_key_value tuple</span></span><br><span class="line">            present_key_value = present_key_value + cross_attn_present_key_value</span><br><span class="line">        start_time3 = time.time()</span><br><span class="line">        <span class="comment"># Fully Connected</span></span><br><span class="line">        residual = hidden_states</span><br><span class="line">        hidden_states = self.activation_fn(self.fc1(hidden_states))</span><br><span class="line">        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)</span><br><span class="line">        hidden_states = self.fc2(hidden_states)</span><br><span class="line">        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)</span><br><span class="line">        feedforward_time = time.time() - start_time3</span><br><span class="line">        loggingg.info(<span class="string">f&quot;Feedforward time: <span class="subst">&#123;feedforward_time:<span class="number">.4</span>f&#125;</span>s&quot;</span>)</span><br><span class="line"></span><br><span class="line">        start_time4 = time.time()</span><br><span class="line">        hidden_states = residual + hidden_states</span><br><span class="line">        hidden_states = self.final_layer_norm(hidden_states)</span><br><span class="line">        addnorm_time = time.time() - start_time4</span><br><span class="line">        loggingg.info(<span class="string">f&quot;Add&amp;Norm time: <span class="subst">&#123;addnorm_time:<span class="number">.4</span>f&#125;</span>s&quot;</span>)</span><br><span class="line"></span><br><span class="line">        outputs = (hidden_states,)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> output_attentions:</span><br><span class="line">            outputs += (self_attn_weights, cross_attn_weights)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_cache:</span><br><span class="line">            outputs += (present_key_value,)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BartClassificationHead</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Head for sentence-level classification tasks.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        input_dim: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        inner_dim: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        num_classes: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        pooler_dropout: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(input_dim, inner_dim)</span><br><span class="line">        self.dropout = nn.Dropout(p=pooler_dropout)</span><br><span class="line">        self.out_proj = nn.Linear(inner_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = torch.tanh(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.out_proj(hidden_states)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BartPreTrainedModel</span>(<span class="title class_ inherited__">PreTrainedModel</span>):</span><br><span class="line">    config_class = BartConfig</span><br><span class="line">    base_model_prefix = <span class="string">&quot;model&quot;</span></span><br><span class="line">    supports_gradient_checkpointing = <span class="literal">True</span></span><br><span class="line">    _keys_to_ignore_on_load_unexpected = [<span class="string">&quot;encoder.version&quot;</span>, <span class="string">&quot;decoder.version&quot;</span>]</span><br><span class="line">    _no_split_modules = [<span class="string">r&quot;BartEncoderLayer&quot;</span>, <span class="string">r&quot;BartDecoderLayer&quot;</span>]</span><br><span class="line">    _skip_keys_device_placement = <span class="string">&quot;past_key_values&quot;</span></span><br><span class="line">    _supports_flash_attn_2 = <span class="literal">True</span></span><br><span class="line">    _supports_sdpa = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self, module</span>):</span><br><span class="line">        std = self.config.init_std</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, nn.Linear):</span><br><span class="line">            module.weight.data.normal_(mean=<span class="number">0.0</span>, std=std)</span><br><span class="line">            <span class="keyword">if</span> module.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                module.bias.data.zero_()</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(module, nn.Embedding):</span><br><span class="line">            module.weight.data.normal_(mean=<span class="number">0.0</span>, std=std)</span><br><span class="line">            <span class="keyword">if</span> module.padding_idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                module.weight.data[module.padding_idx].zero_()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dummy_inputs</span>(<span class="params">self</span>):</span><br><span class="line">        pad_token = self.config.pad_token_id</span><br><span class="line">        input_ids = torch.tensor([[<span class="number">0</span>, <span class="number">6</span>, <span class="number">10</span>, <span class="number">4</span>, <span class="number">2</span>], [<span class="number">0</span>, <span class="number">8</span>, <span class="number">12</span>, <span class="number">2</span>, pad_token]], device=self.device)</span><br><span class="line">        dummy_inputs = &#123;</span><br><span class="line">            <span class="string">&quot;attention_mask&quot;</span>: input_ids.ne(pad_token),</span><br><span class="line">            <span class="string">&quot;input_ids&quot;</span>: input_ids,</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dummy_inputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PretrainedBartModel</span>(<span class="title class_ inherited__">BartPreTrainedModel</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init_subclass__</span>(<span class="params">self</span>):</span><br><span class="line">        warnings.warn(</span><br><span class="line">            <span class="string">&quot;The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.&quot;</span>,</span><br><span class="line">            FutureWarning,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BartPretrainedModel</span>(<span class="title class_ inherited__">BartPreTrainedModel</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init_subclass__</span>(<span class="params">self</span>):</span><br><span class="line">        warnings.warn(</span><br><span class="line">            <span class="string">&quot;The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.&quot;</span>,</span><br><span class="line">            FutureWarning,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BART_START_DOCSTRING = <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the</span></span><br><span class="line"><span class="string">    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads</span></span><br><span class="line"><span class="string">    etc.)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.</span></span><br><span class="line"><span class="string">    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage</span></span><br><span class="line"><span class="string">    and behavior.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        config ([`BartConfig`]):</span></span><br><span class="line"><span class="string">            Model configuration class with all the parameters of the model. Initializing with a config file does not</span></span><br><span class="line"><span class="string">            load the weights associated with the model, only the configuration. Check out the</span></span><br><span class="line"><span class="string">            [`~PreTrainedModel.from_pretrained`] method to load the model weights.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">BART_GENERATION_EXAMPLE = <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Summarization example:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ```python</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; from transformers import AutoTokenizer, BartForConditionalGeneration</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; model = BartForConditionalGeneration.from_pretrained(&quot;facebook/bart-large-cnn&quot;)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/bart-large-cnn&quot;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; ARTICLE_TO_SUMMARIZE = (</span></span><br><span class="line"><span class="string">    ...     &quot;PG&amp;E stated it scheduled the blackouts in response to forecasts for high winds &quot;</span></span><br><span class="line"><span class="string">    ...     &quot;amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were &quot;</span></span><br><span class="line"><span class="string">    ...     &quot;scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.&quot;</span></span><br><span class="line"><span class="string">    ... )</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=&quot;pt&quot;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; # Generate Summary</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; summary_ids = model.generate(inputs[&quot;input_ids&quot;], num_beams=2, min_length=0, max_length=20)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]</span></span><br><span class="line"><span class="string">    &#x27;PG&amp;E scheduled the blackouts in response to forecasts for high winds amid dry conditions&#x27;</span></span><br></pre></td></tr></table></figure></div>

<pre><code>Mask filling example:

<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, BartForConditionalGeneration</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;facebook/bart-base&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>model = BartForConditionalGeneration.from_pretrained(<span class="string">&quot;facebook/bart-base&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>TXT = <span class="string">&quot;My friends are &lt;mask&gt; but they eat too many carbs.&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input_ids = tokenizer([TXT], return_tensors=<span class="string">&quot;pt&quot;</span>)[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>logits = model(input_ids).logits</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>masked_index = (input_ids[<span class="number">0</span>] == tokenizer.mask_token_id).nonzero().item()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>probs = logits[<span class="number">0</span>, masked_index].softmax(dim=<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>values, predictions = probs.topk(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer.decode(predictions).split()</span><br><span class="line">[<span class="string">&#x27;not&#x27;</span>, <span class="string">&#x27;good&#x27;</span>, <span class="string">&#x27;healthy&#x27;</span>, <span class="string">&#x27;great&#x27;</span>, <span class="string">&#x27;very&#x27;</span>]</span><br></pre></td></tr></table></figure></div>
</code></pre>
<p>“””</p>
<p>BART_INPUTS_DOCSTRING &#x3D; r”””<br>    Args:<br>        input_ids (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>):<br>            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide<br>            it.</p>
<pre><code>        Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
        [`PreTrainedTokenizer.__call__`] for details.

        [What are input IDs?](../glossary#input-ids)
    attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
        Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

        - 1 for tokens that are **not masked**,
        - 0 for tokens that are **masked**.

        [What are attention masks?](../glossary#attention-mask)
    decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
        Indices of decoder input sequence tokens in the vocabulary.

        Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
        [`PreTrainedTokenizer.__call__`] for details.

        [What are decoder input IDs?](../glossary#decoder-input-ids)

        Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`
        is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).

        For translation and summarization training, `decoder_input_ids` should be provided. If no
        `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
        for denoising pre-training following the paper.
    decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
        Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
        be used by default.

        If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]
        and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
        information on the default strategy.
    head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
        Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:

        - 1 indicates the head is **not masked**,
        - 0 indicates the head is **masked**.

    decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
        Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:

        - 1 indicates the head is **not masked**,
        - 0 indicates the head is **masked**.

    cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
        Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,
        1]`:

        - 1 indicates the head is **not masked**,
        - 0 indicates the head is **masked**.

    encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):
        Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
        `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
        hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
        `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

        Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
        blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

        If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
        don&#39;t have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
        `decoder_input_ids` of shape `(batch_size, sequence_length)`.
    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
        Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
        This is useful if you want more control over how to convert `input_ids` indices into associated vectors
        than the model&#39;s internal embedding lookup matrix.
    decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):
        Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded
        representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be
        input (see `past_key_values`). This is useful if you want more control over how to convert
        `decoder_input_ids` indices into associated vectors than the model&#39;s internal embedding lookup matrix.

        If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value
        of `inputs_embeds`.
    use_cache (`bool`, *optional*):
        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
        `past_key_values`).
    output_attentions (`bool`, *optional*):
        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
        tensors for more detail.
    output_hidden_states (`bool`, *optional*):
        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
        more detail.
    return_dict (`bool`, *optional*):
        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
</code></pre>
<p>“””</p>
<p>class BartEncoder(BartPreTrainedModel):<br>    “””<br>    Transformer encoder consisting of <em>config.encoder_layers</em> self attention layers. Each layer is a<br>    [<code>BartEncoderLayer</code>].</p>
<pre><code>Args:
    config: BartConfig
    embed_tokens (nn.Embedding): output embedding
&quot;&quot;&quot;

def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):
    super().__init__(config)

    self.dropout = config.dropout
    self.layerdrop = config.encoder_layerdrop

    embed_dim = config.d_model
    self.padding_idx = config.pad_token_id
    self.max_source_positions = config.max_position_embeddings
    embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0

    self.embed_tokens = BartScaledWordEmbedding(
        config.vocab_size, embed_dim, self.padding_idx, embed_scale=embed_scale
    )

    if embed_tokens is not None:
        self.embed_tokens.weight = embed_tokens.weight

    self.embed_positions = BartLearnedPositionalEmbedding(
        config.max_position_embeddings,
        embed_dim,
    )
    self.layers = nn.ModuleList([BartEncoderLayer(config) for _ in range(config.encoder_layers)])
    self._use_flash_attention_2 = config._attn_implementation == &quot;flash_attention_2&quot;
    self._use_sdpa = config._attn_implementation == &quot;sdpa&quot;
    self.layernorm_embedding = nn.LayerNorm(embed_dim)

    self.gradient_checkpointing = False
    # Initialize weights and apply final processing
    self.post_init()

def get_input_embeddings(self):
    return self.embed_tokens

def set_input_embeddings(self, value):
    self.embed_tokens = value

def forward(
    self,
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    head_mask: Optional[torch.Tensor] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
) -&gt; Union[Tuple, BaseModelOutput]:
    r&quot;&quot;&quot;
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
            provide it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)
        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
            Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.

        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
            This is useful if you want more control over how to convert `input_ids` indices into associated vectors
            than the model&#39;s internal embedding lookup matrix.
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under
            returned tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
            for more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
    &quot;&quot;&quot;
    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
    output_hidden_states = (
        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
    )
    return_dict = return_dict if return_dict is not None else self.config.use_return_dict

    # retrieve input_ids and inputs_embeds
    if input_ids is not None and inputs_embeds is not None:
        raise ValueError(&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;)
    elif input_ids is not None:
        input = input_ids
        input_ids = input_ids.view(-1, input_ids.shape[-1])
    elif inputs_embeds is not None:
        input = inputs_embeds[:, :, -1]
    else:
        raise ValueError(&quot;You have to specify either input_ids or inputs_embeds&quot;)
    start_time = time.time()
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)

    embed_pos = self.embed_positions(input)
    embed_pos = embed_pos.to(inputs_embeds.device)
    embeding_time = time.time() - start_time
    loggingg.info(f&quot;Embedding time: &#123;embeding_time:.4f&#125;s&quot;)

    hidden_states = inputs_embeds + embed_pos
    hidden_states = self.layernorm_embedding(hidden_states)
    hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)

    # expand attention_mask
    if attention_mask is not None:
        if self._use_flash_attention_2:
            attention_mask = attention_mask if 0 in attention_mask else None
        elif self._use_sdpa and head_mask is None and not output_attentions:
            # output_attentions=True &amp; head_mask can not be supported when using SDPA, fall back to
            # the manual implementation that requires a 4D causal mask in all cases.
            # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]
            attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)
        else:
            # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]
            attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)

    encoder_states = () if output_hidden_states else None
    all_attentions = () if output_attentions else None

    # check if head_mask has a correct number of layers specified if desired
    if head_mask is not None:
        if head_mask.size()[0] != (len(self.layers)):
            raise ValueError(
                f&quot;The head_mask should be specified for &#123;len(self.layers)&#125; layers, but it is for&quot;
                f&quot; &#123;head_mask.size()[0]&#125;.&quot;
            )

    for idx, encoder_layer in enumerate(self.layers):
        if output_hidden_states:
            encoder_states = encoder_states + (hidden_states,)
        # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
        to_drop = False
        if self.training:
            dropout_probability = torch.rand([])
            if dropout_probability &lt; self.layerdrop:  # skip the layer
                to_drop = True

        if to_drop:
            layer_outputs = (None, None)
        else:
            if self.gradient_checkpointing and self.training:
                layer_outputs = self._gradient_checkpointing_func(
                    encoder_layer.__call__,
                    hidden_states,
                    attention_mask,
                    (head_mask[idx] if head_mask is not None else None),
                    output_attentions,
                )
            else:
                layer_outputs = encoder_layer(
                    hidden_states,
                    attention_mask,
                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),
                    output_attentions=output_attentions,
                )

            hidden_states = layer_outputs[0]

        if output_attentions:
            all_attentions = all_attentions + (layer_outputs[1],)

    if output_hidden_states:
        encoder_states = encoder_states + (hidden_states,)

    if not return_dict:
        return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)
    return BaseModelOutput(
        last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions
    )
</code></pre>
<p>class BartDecoder(BartPreTrainedModel):<br>    “””<br>    Transformer decoder consisting of <em>config.decoder_layers</em> layers. Each layer is a [<code>BartDecoderLayer</code>]</p>
<pre><code>Args:
    config: BartConfig
    embed_tokens (nn.Embedding): output embedding
&quot;&quot;&quot;

def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):
    super().__init__(config)
    self.dropout = config.dropout
    self.layerdrop = config.decoder_layerdrop
    self.padding_idx = config.pad_token_id
    self.max_target_positions = config.max_position_embeddings
    embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0

    self.embed_tokens = BartScaledWordEmbedding(
        config.vocab_size, config.d_model, self.padding_idx, embed_scale=embed_scale
    )

    if embed_tokens is not None:
        self.embed_tokens.weight = embed_tokens.weight

    self.embed_positions = BartLearnedPositionalEmbedding(
        config.max_position_embeddings,
        config.d_model,
    )
    self.layers = nn.ModuleList([BartDecoderLayer(config) for _ in range(config.decoder_layers)])
    self._use_flash_attention_2 = config._attn_implementation == &quot;flash_attention_2&quot;
    self._use_sdpa = config._attn_implementation == &quot;sdpa&quot;

    self.layernorm_embedding = nn.LayerNorm(config.d_model)

    self.gradient_checkpointing = False
    # Initialize weights and apply final processing
    self.post_init()

def get_input_embeddings(self):
    return self.embed_tokens

def set_input_embeddings(self, value):
    self.embed_tokens = value

def forward(
    self,
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    encoder_hidden_states: Optional[torch.FloatTensor] = None,
    encoder_attention_mask: Optional[torch.LongTensor] = None,
    head_mask: Optional[torch.Tensor] = None,
    cross_attn_head_mask: Optional[torch.Tensor] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
) -&gt; Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:
    r&quot;&quot;&quot;
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
            provide it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)
        encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):
            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
            of the decoder.
        encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):
            Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values
            selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)
        head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
            Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.

        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
            Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing
            cross-attention on hidden heads. Mask values selected in `[0, 1]`:

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.

        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
            shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
            cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those
            that don&#39;t have their past key value states given to this model) of shape `(batch_size, 1)` instead of
            all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
            This is useful if you want more control over how to convert `input_ids` indices into associated vectors
            than the model&#39;s internal embedding lookup matrix.
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under
            returned tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
            for more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
    &quot;&quot;&quot;
    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
    output_hidden_states = (
        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
    )
    use_cache = use_cache if use_cache is not None else self.config.use_cache
    return_dict = return_dict if return_dict is not None else self.config.use_return_dict

    # retrieve input_ids and inputs_embeds
    if input_ids is not None and inputs_embeds is not None:
        raise ValueError(&quot;You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time&quot;)
    elif input_ids is not None:
        input = input_ids
        input_shape = input.shape
        input_ids = input_ids.view(-1, input_shape[-1])
    elif inputs_embeds is not None:
        input_shape = inputs_embeds.size()[:-1]
        input = inputs_embeds[:, :, -1]
    else:
        raise ValueError(&quot;You have to specify either decoder_input_ids or decoder_inputs_embeds&quot;)

    # past_key_values_length
    past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0

    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input)

    if self._use_flash_attention_2:
        # 2d mask is passed through the layers
        attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
    elif self._use_sdpa and not output_attentions and cross_attn_head_mask is None:
        # output_attentions=True &amp; cross_attn_head_mask can not be supported when using SDPA, and we fall back on
        # the manual implementation that requires a 4D causal mask in all cases.
        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
            attention_mask,
            input_shape,
            inputs_embeds,
            past_key_values_length,
        )
    else:
        # 4d mask is passed through the layers
        attention_mask = _prepare_4d_causal_attention_mask(
            attention_mask, input_shape, inputs_embeds, past_key_values_length
        )

    # expand encoder attention mask
    if encoder_hidden_states is not None and encoder_attention_mask is not None:
        if self._use_flash_attention_2:
            encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None
        elif self._use_sdpa and cross_attn_head_mask is None and not output_attentions:
            # output_attentions=True &amp; cross_attn_head_mask can not be supported when using SDPA, and we fall back on
            # the manual implementation that requires a 4D causal mask in all cases.
            # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]
            encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(
                encoder_attention_mask,
                inputs_embeds.dtype,
                tgt_len=input_shape[-1],
            )
        else:
            # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]
            encoder_attention_mask = _prepare_4d_attention_mask(
                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]
            )

    # embed positions
    positions = self.embed_positions(input, past_key_values_length)
    positions = positions.to(inputs_embeds.device)

    hidden_states = inputs_embeds + positions
    hidden_states = self.layernorm_embedding(hidden_states)

    hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)

    if self.gradient_checkpointing and self.training:
        if use_cache:
            logger.warning_once(
                &quot;`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...&quot;
            )
            use_cache = False

    # decoder layers
    all_hidden_states = () if output_hidden_states else None
    all_self_attns = () if output_attentions else None
    all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None
    next_decoder_cache = () if use_cache else None

    # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired
    for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [&quot;head_mask&quot;, &quot;cross_attn_head_mask&quot;]):
        if attn_mask is not None:
            if attn_mask.size()[0] != (len(self.layers)):
                raise ValueError(
                    f&quot;The `&#123;mask_name&#125;` should be specified for &#123;len(self.layers)&#125; layers, but it is for&quot;
                    f&quot; &#123;head_mask.size()[0]&#125;.&quot;
                )

    for idx, decoder_layer in enumerate(self.layers):
        # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
        if output_hidden_states:
            all_hidden_states += (hidden_states,)
        if self.training:
            dropout_probability = torch.rand([])
            if dropout_probability &lt; self.layerdrop:
                continue

        past_key_value = past_key_values[idx] if past_key_values is not None else None

        if self.gradient_checkpointing and self.training:
            layer_outputs = self._gradient_checkpointing_func(
                decoder_layer.__call__,
                hidden_states,
                attention_mask,
                encoder_hidden_states,
                encoder_attention_mask,
                head_mask[idx] if head_mask is not None else None,
                cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,
                None,
                output_attentions,
                use_cache,
            )
        else:
            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=attention_mask,
                encoder_hidden_states=encoder_hidden_states,
                encoder_attention_mask=encoder_attention_mask,
                layer_head_mask=(head_mask[idx] if head_mask is not None else None),
                cross_attn_layer_head_mask=(
                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None
                ),
                past_key_value=past_key_value,
                output_attentions=output_attentions,
                use_cache=use_cache,
            )
        hidden_states = layer_outputs[0]

        if use_cache:
            next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)

        if output_attentions:
            all_self_attns += (layer_outputs[1],)

            if encoder_hidden_states is not None:
                all_cross_attentions += (layer_outputs[2],)

    # add hidden states from the last decoder layer
    if output_hidden_states:
        all_hidden_states += (hidden_states,)

    next_cache = next_decoder_cache if use_cache else None
    if not return_dict:
        return tuple(
            v
            for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]
            if v is not None
        )
    return BaseModelOutputWithPastAndCrossAttentions(
        last_hidden_state=hidden_states,
        past_key_values=next_cache,
        hidden_states=all_hidden_states,
        attentions=all_self_attns,
        cross_attentions=all_cross_attentions,
    )
</code></pre>
<p>@add_start_docstrings(<br>    “The bare BART Model outputting raw hidden-states without any specific head on top.”,<br>    BART_START_DOCSTRING,<br>)<br>class BartModel(BartPreTrainedModel):<br>    _tied_weights_keys &#x3D; [“encoder.embed_tokens.weight”, “decoder.embed_tokens.weight”]</p>
<pre><code>def __init__(self, config: BartConfig):
    super().__init__(config)

    padding_idx, vocab_size = config.pad_token_id, config.vocab_size
    embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0
    self.shared = BartScaledWordEmbedding(vocab_size, config.d_model, padding_idx, embed_scale=embed_scale)

    self.encoder = BartEncoder(config, self.shared)
    self.decoder = BartDecoder(config, self.shared)

    # Initialize weights and apply final processing
    self.post_init()

def _tie_weights(self):
    if self.config.tie_word_embeddings:
        self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)
        self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)

def get_input_embeddings(self):
    return self.shared

def set_input_embeddings(self, value):
    self.shared = value
    self.encoder.embed_tokens = self.shared
    self.decoder.embed_tokens = self.shared

def get_encoder(self):
    return self.encoder

def get_decoder(self):
    return self.decoder

@add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)
@add_code_sample_docstrings(
    checkpoint=_CHECKPOINT_FOR_DOC,
    output_type=Seq2SeqModelOutput,
    config_class=_CONFIG_FOR_DOC,
    expected_output=_EXPECTED_OUTPUT_SHAPE,
)
def forward(
    self,
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    decoder_input_ids: Optional[torch.LongTensor] = None,
    decoder_attention_mask: Optional[torch.LongTensor] = None,
    head_mask: Optional[torch.Tensor] = None,
    decoder_head_mask: Optional[torch.Tensor] = None,
    cross_attn_head_mask: Optional[torch.Tensor] = None,
    encoder_outputs: Optional[List[torch.FloatTensor]] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
) -&gt; Union[Tuple, Seq2SeqModelOutput]:
    # different to other models, Bart automatically creates decoder_input_ids from
    # input_ids if no decoder_input_ids are provided
    if decoder_input_ids is None and decoder_inputs_embeds is None:
        if input_ids is None:
            raise ValueError(
                &quot;If no `decoder_input_ids` or `decoder_inputs_embeds` are &quot;
                &quot;passed, `input_ids` cannot be `None`. Please pass either &quot;
                &quot;`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.&quot;
            )

        decoder_input_ids = shift_tokens_right(
            input_ids, self.config.pad_token_id, self.config.decoder_start_token_id
        )

    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
    output_hidden_states = (
        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
    )
    use_cache = use_cache if use_cache is not None else self.config.use_cache
    return_dict = return_dict if return_dict is not None else self.config.use_return_dict

    if encoder_outputs is None:
        encoder_outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
    # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True
    elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):
        encoder_outputs = BaseModelOutput(
            last_hidden_state=encoder_outputs[0],
            hidden_states=encoder_outputs[1] if len(encoder_outputs) &gt; 1 else None,
            attentions=encoder_outputs[2] if len(encoder_outputs) &gt; 2 else None,
        )

    # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)
    decoder_outputs = self.decoder(
        input_ids=decoder_input_ids,
        attention_mask=decoder_attention_mask,
        encoder_hidden_states=encoder_outputs[0],
        encoder_attention_mask=attention_mask,
        head_mask=decoder_head_mask,
        cross_attn_head_mask=cross_attn_head_mask,
        past_key_values=past_key_values,
        inputs_embeds=decoder_inputs_embeds,
        use_cache=use_cache,
        output_attentions=output_attentions,
        output_hidden_states=output_hidden_states,
        return_dict=return_dict,
    )

    if not return_dict:
        return decoder_outputs + encoder_outputs

    return Seq2SeqModelOutput(
        last_hidden_state=decoder_outputs.last_hidden_state,
        past_key_values=decoder_outputs.past_key_values,
        decoder_hidden_states=decoder_outputs.hidden_states,
        decoder_attentions=decoder_outputs.attentions,
        cross_attentions=decoder_outputs.cross_attentions,
        encoder_last_hidden_state=encoder_outputs.last_hidden_state,
        encoder_hidden_states=encoder_outputs.hidden_states,
        encoder_attentions=encoder_outputs.attentions,
    )
</code></pre>
<p>@add_start_docstrings(<br>    “The BART Model with a language modeling head. Can be used for summarization.”, BART_START_DOCSTRING<br>)<br>class BartForConditionalGeneration(BartPreTrainedModel, GenerationMixin):<br>    base_model_prefix &#x3D; “model”<br>    _tied_weights_keys &#x3D; [“encoder.embed_tokens.weight”, “decoder.embed_tokens.weight”, “lm_head.weight”]<br>    _keys_to_ignore_on_load_missing &#x3D; [“final_logits_bias”]</p>
<pre><code>def __init__(self, config: BartConfig):
    super().__init__(config)
    self.model = BartModel(config)
    self.register_buffer(&quot;final_logits_bias&quot;, torch.zeros((1, self.model.shared.num_embeddings)))
    self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)

    # Initialize weights and apply final processing
    self.post_init()

def get_encoder(self):
    return self.model.get_encoder()

def get_decoder(self):
    return self.model.get_decoder()

def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -&gt; nn.Embedding:
    new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)
    self._resize_final_logits_bias(new_embeddings.weight.shape[0])
    return new_embeddings

def _resize_final_logits_bias(self, new_num_tokens: int) -&gt; None:
    old_num_tokens = self.final_logits_bias.shape[-1]
    if new_num_tokens &lt;= old_num_tokens:
        new_bias = self.final_logits_bias[:, :new_num_tokens]
    else:
        extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)
        new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)
    self.register_buffer(&quot;final_logits_bias&quot;, new_bias)

def get_output_embeddings(self):
    return self.lm_head

def set_output_embeddings(self, new_embeddings):
    self.lm_head = new_embeddings

@add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)
@replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)
@add_end_docstrings(BART_GENERATION_EXAMPLE)
def forward(
    self,
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    decoder_input_ids: Optional[torch.LongTensor] = None,
    decoder_attention_mask: Optional[torch.LongTensor] = None,
    head_mask: Optional[torch.Tensor] = None,
    decoder_head_mask: Optional[torch.Tensor] = None,
    cross_attn_head_mask: Optional[torch.Tensor] = None,
    encoder_outputs: Optional[List[torch.FloatTensor]] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
    labels: Optional[torch.LongTensor] = None,
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
) -&gt; Union[Tuple, Seq2SeqLMOutput]:
    r&quot;&quot;&quot;
    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
        config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
        (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

    Returns:
    &quot;&quot;&quot;
    return_dict = return_dict if return_dict is not None else self.config.use_return_dict

    if labels is not None:
        if use_cache:
            logger.warning(&quot;The `use_cache` argument is changed to `False` since `labels` is provided.&quot;)
        use_cache = False
        if decoder_input_ids is None and decoder_inputs_embeds is None:
            decoder_input_ids = shift_tokens_right(
                labels, self.config.pad_token_id, self.config.decoder_start_token_id
            )

    outputs = self.model(
        input_ids,
        attention_mask=attention_mask,
        decoder_input_ids=decoder_input_ids,
        encoder_outputs=encoder_outputs,
        decoder_attention_mask=decoder_attention_mask,
        head_mask=head_mask,
        decoder_head_mask=decoder_head_mask,
        cross_attn_head_mask=cross_attn_head_mask,
        past_key_values=past_key_values,
        inputs_embeds=inputs_embeds,
        decoder_inputs_embeds=decoder_inputs_embeds,
        use_cache=use_cache,
        output_attentions=output_attentions,
        output_hidden_states=output_hidden_states,
        return_dict=return_dict,
    )

    lm_logits = self.lm_head(outputs[0])
    lm_logits = lm_logits + self.final_logits_bias.to(lm_logits.device)

    masked_lm_loss = None
    if labels is not None:
        labels = labels.to(lm_logits.device)
        loss_fct = CrossEntropyLoss()
        masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))

    if not return_dict:
        output = (lm_logits,) + outputs[1:]
        return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output

    return Seq2SeqLMOutput(
        loss=masked_lm_loss,
        logits=lm_logits,
        past_key_values=outputs.past_key_values,
        decoder_hidden_states=outputs.decoder_hidden_states,
        decoder_attentions=outputs.decoder_attentions,
        cross_attentions=outputs.cross_attentions,
        encoder_last_hidden_state=outputs.encoder_last_hidden_state,
        encoder_hidden_states=outputs.encoder_hidden_states,
        encoder_attentions=outputs.encoder_attentions,
    )

def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):
    return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)

@staticmethod
def _reorder_cache(past_key_values, beam_idx):
    reordered_past = ()
    for layer_past in past_key_values:
        # cached cross_attention states don&#39;t have to be reordered -&gt; they are always the same
        reordered_past += (
            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])
            + layer_past[2:],
        )
    return reordered_past
</code></pre>
<p>@add_start_docstrings(<br>    “””<br>    Bart model with a sequence classification&#x2F;head on top (a linear layer on top of the pooled output) e.g. for GLUE<br>    tasks.<br>    “””,<br>    BART_START_DOCSTRING,<br>)<br>class BartForSequenceClassification(BartPreTrainedModel):<br>    _tied_weights_keys &#x3D; [“encoder.embed_tokens.weight”, “decoder.embed_tokens.weight”]</p>
<pre><code>def __init__(self, config: BartConfig, **kwargs):
    super().__init__(config, **kwargs)
    self.model = BartModel(config)
    self.classification_head = BartClassificationHead(
        config.d_model,
        config.d_model,
        config.num_labels,
        config.classifier_dropout,
    )

    # Initialize weights and apply final processing
    self.post_init()

@add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)
@add_code_sample_docstrings(
    checkpoint=_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,
    output_type=Seq2SeqSequenceClassifierOutput,
    config_class=_CONFIG_FOR_DOC,
    expected_output=_SEQ_CLASS_EXPECTED_OUTPUT,
    expected_loss=_SEQ_CLASS_EXPECTED_LOSS,
)
def forward(
    self,
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    decoder_input_ids: Optional[torch.LongTensor] = None,
    decoder_attention_mask: Optional[torch.LongTensor] = None,
    head_mask: Optional[torch.Tensor] = None,
    decoder_head_mask: Optional[torch.Tensor] = None,
    cross_attn_head_mask: Optional[torch.Tensor] = None,
    encoder_outputs: Optional[List[torch.FloatTensor]] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
    labels: Optional[torch.LongTensor] = None,
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
) -&gt; Union[Tuple, Seq2SeqSequenceClassifierOutput]:
    r&quot;&quot;&quot;
    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
        config.num_labels - 1]`. If `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).
    &quot;&quot;&quot;
    return_dict = return_dict if return_dict is not None else self.config.use_return_dict
    if labels is not None:
        use_cache = False

    if input_ids is None and inputs_embeds is not None:
        raise NotImplementedError(
            f&quot;Passing input embeddings is currently not supported for &#123;self.__class__.__name__&#125;&quot;
        )

    outputs = self.model(
        input_ids,
        attention_mask=attention_mask,
        decoder_input_ids=decoder_input_ids,
        decoder_attention_mask=decoder_attention_mask,
        head_mask=head_mask,
        decoder_head_mask=decoder_head_mask,
        cross_attn_head_mask=cross_attn_head_mask,
        encoder_outputs=encoder_outputs,
        inputs_embeds=inputs_embeds,
        decoder_inputs_embeds=decoder_inputs_embeds,
        use_cache=use_cache,
        output_attentions=output_attentions,
        output_hidden_states=output_hidden_states,
        return_dict=return_dict,
    )
    hidden_states = outputs[0]  # last hidden state

    eos_mask = input_ids.eq(self.config.eos_token_id).to(hidden_states.device)

    if len(torch.unique_consecutive(eos_mask.sum(1))) &gt; 1:
        raise ValueError(&quot;All examples must have the same number of &lt;eos&gt; tokens.&quot;)
    sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[
        :, -1, :
    ]
    logits = self.classification_head(sentence_representation)

    loss = None
    if labels is not None:
        labels = labels.to(logits.device)
        if self.config.problem_type is None:
            if self.config.num_labels == 1:
                self.config.problem_type = &quot;regression&quot;
            elif self.config.num_labels &gt; 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                self.config.problem_type = &quot;single_label_classification&quot;
            else:
                self.config.problem_type = &quot;multi_label_classification&quot;

        if self.config.problem_type == &quot;regression&quot;:
            loss_fct = MSELoss()
            if self.config.num_labels == 1:
                loss = loss_fct(logits.squeeze(), labels.squeeze())
            else:
                loss = loss_fct(logits, labels)
        elif self.config.problem_type == &quot;single_label_classification&quot;:
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))
        elif self.config.problem_type == &quot;multi_label_classification&quot;:
            loss_fct = BCEWithLogitsLoss()
            loss = loss_fct(logits, labels)
    if not return_dict:
        output = (logits,) + outputs[1:]
        return ((loss,) + output) if loss is not None else output

    return Seq2SeqSequenceClassifierOutput(
        loss=loss,
        logits=logits,
        past_key_values=outputs.past_key_values,
        decoder_hidden_states=outputs.decoder_hidden_states,
        decoder_attentions=outputs.decoder_attentions,
        cross_attentions=outputs.cross_attentions,
        encoder_last_hidden_state=outputs.encoder_last_hidden_state,
        encoder_hidden_states=outputs.encoder_hidden_states,
        encoder_attentions=outputs.encoder_attentions,
    )
</code></pre>
<p>@add_start_docstrings(<br>    “””<br>    BART Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear<br>    layer on top of the hidden-states output to compute <code>span start logits</code> and <code>span end logits</code>).<br>    “””,<br>    BART_START_DOCSTRING,<br>)<br>class BartForQuestionAnswering(BartPreTrainedModel):<br>    _tied_weights_keys &#x3D; [“encoder.embed_tokens.weight”, “decoder.embed_tokens.weight”]</p>
<pre><code>def __init__(self, config):
    super().__init__(config)

    config.num_labels = 2
    self.num_labels = config.num_labels

    self.model = BartModel(config)
    self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)

    # Initialize weights and apply final processing
    self.post_init()

@add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)
@add_code_sample_docstrings(
    checkpoint=_CHECKPOINT_FOR_QA,
    output_type=Seq2SeqQuestionAnsweringModelOutput,
    config_class=_CONFIG_FOR_DOC,
    expected_loss=_QA_EXPECTED_LOSS,
    expected_output=_QA_EXPECTED_OUTPUT,
)
def forward(
    self,
    input_ids: torch.Tensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    decoder_input_ids: Optional[torch.LongTensor] = None,
    decoder_attention_mask: Optional[torch.LongTensor] = None,
    head_mask: Optional[torch.Tensor] = None,
    decoder_head_mask: Optional[torch.Tensor] = None,
    cross_attn_head_mask: Optional[torch.Tensor] = None,
    encoder_outputs: Optional[List[torch.FloatTensor]] = None,
    start_positions: Optional[torch.LongTensor] = None,
    end_positions: Optional[torch.LongTensor] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
) -&gt; Union[Tuple, Seq2SeqQuestionAnsweringModelOutput]:
    r&quot;&quot;&quot;
    start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
        Labels for position (index) of the start of the labelled span for computing the token classification loss.
        Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence
        are not taken into account for computing the loss.
    end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
        Labels for position (index) of the end of the labelled span for computing the token classification loss.
        Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence
        are not taken into account for computing the loss.
    &quot;&quot;&quot;
    return_dict = return_dict if return_dict is not None else self.config.use_return_dict
    if start_positions is not None and end_positions is not None:
        use_cache = False

    outputs = self.model(
        input_ids,
        attention_mask=attention_mask,
        decoder_input_ids=decoder_input_ids,
        decoder_attention_mask=decoder_attention_mask,
        head_mask=head_mask,
        decoder_head_mask=decoder_head_mask,
        cross_attn_head_mask=cross_attn_head_mask,
        encoder_outputs=encoder_outputs,
        inputs_embeds=inputs_embeds,
        decoder_inputs_embeds=decoder_inputs_embeds,
        use_cache=use_cache,
        output_attentions=output_attentions,
        output_hidden_states=output_hidden_states,
        return_dict=return_dict,
    )

    sequence_output = outputs[0]

    logits = self.qa_outputs(sequence_output)
    start_logits, end_logits = logits.split(1, dim=-1)
    start_logits = start_logits.squeeze(-1).contiguous()
    end_logits = end_logits.squeeze(-1).contiguous()

    total_loss = None
    if start_positions is not None and end_positions is not None:
        # If we are on multi-GPU, split add a dimension
        if len(start_positions.size()) &gt; 1:
            start_positions = start_positions.squeeze(-1)
        if len(end_positions.size()) &gt; 1:
            end_positions = end_positions.squeeze(-1)
        # sometimes the start/end positions are outside our model inputs, we ignore these terms
        ignored_index = start_logits.size(1)
        start_positions = start_positions.clamp(0, ignored_index)
        end_positions = end_positions.clamp(0, ignored_index)

        loss_fct = CrossEntropyLoss(ignore_index=ignored_index)
        start_loss = loss_fct(start_logits, start_positions)
        end_loss = loss_fct(end_logits, end_positions)
        total_loss = (start_loss + end_loss) / 2

    if not return_dict:
        output = (
            start_logits,
            end_logits,
        ) + outputs[1:]
        return ((total_loss,) + output) if total_loss is not None else output

    return Seq2SeqQuestionAnsweringModelOutput(
        loss=total_loss,
        start_logits=start_logits,
        end_logits=end_logits,
        past_key_values=outputs.past_key_values,
        decoder_hidden_states=outputs.decoder_hidden_states,
        decoder_attentions=outputs.decoder_attentions,
        cross_attentions=outputs.cross_attentions,
        encoder_last_hidden_state=outputs.encoder_last_hidden_state,
        encoder_hidden_states=outputs.encoder_hidden_states,
        encoder_attentions=outputs.encoder_attentions,
    )
</code></pre>
<p>class BartDecoderWrapper(BartPreTrainedModel):<br>    “””<br>    This wrapper class is a helper class to correctly load pretrained checkpoints when the causal language model is<br>    used in combination with the [<code>EncoderDecoderModel</code>] framework.<br>    “””</p>
<pre><code>def __init__(self, config):
    super().__init__(config)
    self.decoder = BartDecoder(config)

def forward(self, *args, **kwargs):
    return self.decoder(*args, **kwargs)
</code></pre>
<p>@add_start_docstrings(<br>    “””<br>    BART decoder with a language modeling head on top (linear layer with weights tied to the input embeddings).<br>    “””,<br>    BART_START_DOCSTRING,<br>)<br>class BartForCausalLM(BartPreTrainedModel, GenerationMixin):<br>    _tied_weights_keys &#x3D; [“lm_head.weight”]</p>
<pre><code>def __init__(self, config):
    config = copy.deepcopy(config)
    config.is_decoder = True
    config.is_encoder_decoder = False
    super().__init__(config)
    self.model = BartDecoderWrapper(config)

    self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

    # Initialize weights and apply final processing
    self.post_init()

def get_input_embeddings(self):
    return self.model.decoder.embed_tokens

def set_input_embeddings(self, value):
    self.model.decoder.embed_tokens = value

def get_output_embeddings(self):
    return self.lm_head

def set_output_embeddings(self, new_embeddings):
    self.lm_head = new_embeddings

def set_decoder(self, decoder):
    self.model.decoder = decoder

def get_decoder(self):
    return self.model.decoder

@replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)
def forward(
    self,
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    encoder_hidden_states: Optional[torch.FloatTensor] = None,
    encoder_attention_mask: Optional[torch.FloatTensor] = None,
    head_mask: Optional[torch.Tensor] = None,
    cross_attn_head_mask: Optional[torch.Tensor] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    labels: Optional[torch.LongTensor] = None,
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
) -&gt; Union[Tuple, CausalLMOutputWithCrossAttentions]:
    r&quot;&quot;&quot;
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
            provide it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)
        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
            if the model is configured as a decoder.
        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used
            in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:
        head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
            Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.

        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.

        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
            shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional
            tensors are only required when the model is used as a decoder in a Sequence to Sequence model.

            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
            cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those
            that don&#39;t have their past key value states given to this model) of shape `(batch_size, 1)` instead of
            all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
            (see `past_key_values`).

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under
            returned tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
            for more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.

    Returns:

    Example:

    <div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">        &gt;&gt;&gt; <span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, BartForCausalLM</span><br><span class="line"></span><br><span class="line">        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;facebook/bart-base&quot;</span>)</span><br><span class="line">        &gt;&gt;&gt; model = BartForCausalLM.from_pretrained(<span class="string">&quot;facebook/bart-base&quot;</span>, add_cross_attention=<span class="literal">False</span>)</span><br><span class="line">        &gt;&gt;&gt; <span class="keyword">assert</span> model.config.is_decoder, <span class="string">f&quot;<span class="subst">&#123;model.__class__&#125;</span> has to be configured as a decoder.&quot;</span></span><br><span class="line">        &gt;&gt;&gt; inputs = tokenizer(<span class="string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">        &gt;&gt;&gt; outputs = model(**inputs)</span><br><span class="line"></span><br><span class="line">        &gt;&gt;&gt; logits = outputs.logits</span><br><span class="line">        &gt;&gt;&gt; expected_shape = [<span class="number">1</span>, inputs.input_ids.shape[-<span class="number">1</span>], model.config.vocab_size]</span><br><span class="line">        &gt;&gt;&gt; <span class="built_in">list</span>(logits.shape) == expected_shape</span><br><span class="line">        <span class="literal">True</span></span><br><span class="line">        ```<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions</span></span><br><span class="line"><span class="string">        output_hidden_states = (</span></span><br><span class="line"><span class="string">            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states</span></span><br><span class="line"><span class="string">        )</span></span><br><span class="line"><span class="string">        return_dict = return_dict if return_dict is not None else self.config.use_return_dict</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)</span></span><br><span class="line"><span class="string">        outputs = self.model.decoder(</span></span><br><span class="line"><span class="string">            input_ids=input_ids,</span></span><br><span class="line"><span class="string">            attention_mask=attention_mask,</span></span><br><span class="line"><span class="string">            encoder_hidden_states=encoder_hidden_states,</span></span><br><span class="line"><span class="string">            encoder_attention_mask=encoder_attention_mask,</span></span><br><span class="line"><span class="string">            head_mask=head_mask,</span></span><br><span class="line"><span class="string">            cross_attn_head_mask=cross_attn_head_mask,</span></span><br><span class="line"><span class="string">            past_key_values=past_key_values,</span></span><br><span class="line"><span class="string">            inputs_embeds=inputs_embeds,</span></span><br><span class="line"><span class="string">            use_cache=use_cache,</span></span><br><span class="line"><span class="string">            output_attentions=output_attentions,</span></span><br><span class="line"><span class="string">            output_hidden_states=output_hidden_states,</span></span><br><span class="line"><span class="string">            return_dict=return_dict,</span></span><br><span class="line"><span class="string">        )</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        logits = self.lm_head(outputs[0])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        loss = None</span></span><br><span class="line"><span class="string">        if labels is not None:</span></span><br><span class="line"><span class="string">            labels = labels.to(logits.device)</span></span><br><span class="line"><span class="string">            loss_fct = CrossEntropyLoss()</span></span><br><span class="line"><span class="string">            loss = loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        if not return_dict:</span></span><br><span class="line"><span class="string">            output = (logits,) + outputs[1:]</span></span><br><span class="line"><span class="string">            return (loss,) + output if loss is not None else output</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        return CausalLMOutputWithCrossAttentions(</span></span><br><span class="line"><span class="string">            loss=loss,</span></span><br><span class="line"><span class="string">            logits=logits,</span></span><br><span class="line"><span class="string">            past_key_values=outputs.past_key_values,</span></span><br><span class="line"><span class="string">            hidden_states=outputs.hidden_states,</span></span><br><span class="line"><span class="string">            attentions=outputs.attentions,</span></span><br><span class="line"><span class="string">            cross_attentions=outputs.cross_attentions,</span></span><br><span class="line"><span class="string">        )</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    @staticmethod</span></span><br><span class="line"><span class="string">    def _reorder_cache(past_key_values, beam_idx):</span></span><br><span class="line"><span class="string">        reordered_past = ()</span></span><br><span class="line"><span class="string">        for layer_past in past_key_values:</span></span><br><span class="line"><span class="string">            reordered_past += (</span></span><br><span class="line"><span class="string">                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),</span></span><br><span class="line"><span class="string">            )</span></span><br><span class="line"><span class="string">        return reordered_past</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">__all__ = [</span></span><br><span class="line"><span class="string">    &quot;BartForCausalLM&quot;,</span></span><br><span class="line"><span class="string">    &quot;BartForConditionalGeneration&quot;,</span></span><br><span class="line"><span class="string">    &quot;BartForQuestionAnswering&quot;,</span></span><br><span class="line"><span class="string">    &quot;BartForSequenceClassification&quot;,</span></span><br><span class="line"><span class="string">    &quot;BartModel&quot;,</span></span><br><span class="line"><span class="string">    &quot;BartPreTrainedModel&quot;,</span></span><br><span class="line"><span class="string">    &quot;BartPretrainedModel&quot;,</span></span><br><span class="line"><span class="string">    &quot;PretrainedBartModel&quot;,</span></span><br><span class="line"><span class="string">]</span></span><br></pre></td></tr></table></figure></div>
</code></pre>

		</div>

		
		<div class="post-copyright-info w-full my-8 px-2 sm:px-6 md:px-8">
			<div class="article-copyright-info-container">
    <ul>
        <li><strong>Title:</strong> transformer</li>
        <li><strong>Author:</strong> Ikko</li>
        <li><strong>Created at
                :</strong> 2025-01-06 14:54:29</li>
        
            <li>
                <strong>Updated at
                    :</strong> 2025-12-17 20:43:12
            </li>
        
        <li>
            <strong>Link:</strong> http://ikko-debug.github.io/2025/01/06/transformer/
        </li>
        <li>
            <strong>
                License:
            </strong>
            

            
                This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0">CC BY-NC-SA 4.0</a>.
            
        </li>
    </ul>
</div>

		</div>
		

		

		

		
		<div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8">
			
			<div class="article-prev border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="prev" rel="prev" href="/2025/01/15/tvm/">
					<span class="left arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-left"></i>
					</span>
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item truncate max-w-48">tvm</span>
						<span class="post-nav-item">Prev posts</span>
					</span>
				</a>
			</div>
			
			
			<div class="article-next border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="next" rel="next" href="/2023/09/28/%E4%BF%A1%E5%8F%B7%E4%B8%8E%E7%B3%BB%E7%BB%9F/">
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item truncate max-w-48">信号与系统</span>
						<span class="post-nav-item">Next posts</span>
					</span>
					<span class="right arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-right"></i>
					</span>
				</a>
			</div>
			
		</div>
		


		
		<div class="comment-container px-2 sm:px-6 md:px-8 pb-8">
			<div class="comments-container mt-10 w-full ">
    <div id="comment-anchor" class="w-full h-2.5"></div>
    <div class="comment-area-title w-full my-1.5 md:my-2.5 text-xl md:text-3xl font-bold">
        Comments
    </div>
    

        
            
    <div id="gitalk-container"></div>
    <script data-swup-reload-script
            src="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js"></script>
    <script data-swup-reload-script>

        function loadGitalk() {
            let __gitalk__pathname = decodeURI(location.pathname);
            const __gitalk__pathnameLength = __gitalk__pathname.length;
            const __gitalk__pathnameMaxLength = 50;
            if (__gitalk__pathnameLength > __gitalk__pathnameMaxLength) {
                __gitalk__pathname = __gitalk__pathname.substring(0, __gitalk__pathnameMaxLength - 3) + '...';
            }

            try {
                Gitalk && new Gitalk({
                    clientID: '55ccec7ba873a504625f',
                    clientSecret: '5a44aa297b4124ddb3e9a8bb19842a6c066273e6',
                    repo: 'gittalk',
                    owner: 'ikko-debug',
                    admin: ['ikko-debug'],
                    id: __gitalk__pathname,
                    language: 'en',
                    proxy: 'https://github.com/login/oauth/access_token'
                }).render('gitalk-container');

            } catch (e) {
                window.Gitalk = null;
            }
        }

        if ('true') {
            const loadGitalkTimeout = setTimeout(() => {
                loadGitalk();
                clearTimeout(loadGitalkTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadGitalk);
        }
    </script>



        
    
</div>

		</div>
		
	</div>

	
	<div class="toc-content-container">
		<div class="post-toc-wrap">
	<div class="post-toc">
		<div class="toc-title">On this page</div>
		<div class="page-title">transformer</div>
		

	</div>
</div>
	</div>
	
</div>
			</div>

			
		</div>

		<div class="main-content-footer">
			<footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2022</span>
              -
            
            2025&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Ikko</a>
            
                
                <p class="post-count space-x-0.5">
                    <span>
                        32 posts in total
                    </span>
                    
                        <span>
                            58.8k words in total
                        </span>
                    
                </p>
            
        </div>
        
            <script data-swup-reload-script src="https://cn.vercount.one/js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">VISITOR COUNT</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">TOTAL PAGE VIEWS</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a></span>
            <span class="text-sm lg:block">THEME&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.8.5</a></span>
        </div>
        
        
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
		</div>
	</div>

	
	<div class="post-tools">
		<div class="post-tools-container">
	<ul class="article-tools-list">
		<!-- TOC aside toggle -->
		
		<li class="right-bottom-tools page-aside-toggle">
			<i class="fa-regular fa-outdent"></i>
		</li>
		

		<!-- go comment -->
		
		<li class="go-comment">
			<i class="fa-regular fa-comments"></i>
		</li>
		
	</ul>
</div>
	</div>
	

	<div class="right-side-tools-container">
		<div class="side-tools-container">
	<ul class="hidden-tools-list">
		<li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-plus"></i>
		</li>

		<li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-minus"></i>
		</li>

		<li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
			<i class="fa-regular fa-moon"></i>
		</li>

		<!-- rss -->
		

		

		<li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
			<i class="fa-regular fa-arrow-down"></i>
		</li>
	</ul>

	<ul class="visible-tools-list">
		<li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
			<i class="fa-regular fa-cog "></i>
		</li>
		
		<li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
			<i class="arrow-up fas fa-arrow-up"></i>
			<span class="percent"></span>
		</li>
		
		
	</ul>
</div>
	</div>

	<div class="image-viewer-container">
	<img src="">
</div>

	
	<div class="search-pop-overlay">
	<div class="popup search-popup">
		<div class="search-header">
			<span class="search-input-field-pre">
				<i class="fa-solid fa-keyboard"></i>
			</span>
			<div class="search-input-container">
				<input autocomplete="off" autocorrect="off" autocapitalize="off" placeholder="Search..." spellcheck="false" type="search" class="search-input">
			</div>
			<span class="popup-btn-close">
				<i class="fa-solid fa-times"></i>
			</span>
		</div>
		<div id="search-result">
			<div id="no-result">
				<i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i>
			</div>
		</div>
	</div>
</div>
	

</main>


<script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/Swup.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupSlideTheme.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupScriptsPlugin.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupProgressPlugin.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupScrollPlugin.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupPreloadPlugin.min.js" ></script>
<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
            new SwupPreloadPlugin(),
        ],
        containers: ["#swup"],
    });
</script>




	<script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/imageViewer.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/utils.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/main.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/navbarShrink.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/scrollTopBottom.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/lightDarkSwitch.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/categoryList.js" ></script>


    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/localSearch.js" ></script>



    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/codeBlock.js" ></script>



    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/lazyload.js" ></script>



    <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/runtime.js" ></script>
    <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/odometer.min.js" ></script>
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/assets/odometer-theme-minimal.css">



  <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/Typed.min.js" ></script>
  <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/plugins/typed.js" ></script>





    <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/minimasonry.min.js" ></script>
    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/plugins/masonry.js" ></script>



    <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/anime.min.js" ></script>




    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/tocToggle.js" data-swup-reload-script></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/toc.js" data-swup-reload-script></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/plugins/tabs.js" data-swup-reload-script></script>


<script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/moment-with-locales.min.js" data-swup-reload-script></script>
<script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/essays.js" data-swup-reload-script></script>




    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/bookmarkNav.js" ></script>

	
</body>

</html>